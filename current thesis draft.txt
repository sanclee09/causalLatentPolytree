%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis Draft -- TU München
% Title  : LiNGAM Models on Minimal Latent Polytrees
% Author : <Your Name>
% Supervisor : Prof. Dr. Mathias Drton
% Mentor : Daniele Tramontano
% Date   : \today
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

%%%%%% Packages %%%%%%
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
    \bibliographystyle{plainnat}
\usepackage{lmodern}
\usepackage{geometry}
  \geometry{margin=3cm}
\usepackage{setspace}
  \onehalfspacing
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
  \hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning}
\usepackage[overload,ntheorem]{empheq}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{subfig}
\usepackage{comment}
\usepackage{booktabs} % commands to create good-looking tables
\usetikzlibrary{arrows.meta}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand{\indep}{\perp \!\!\! \perp}  % Independence symbol
\numberwithin{equation}{section}

%%%%%% Theorem Environments %%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}

\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\an}{an}
\DeclareMathOperator{\de}{de}
\DeclareMathOperator{\cum}{cum}
\DeclareMathOperator{\ttop}{top}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\thr}{thr}


%%%%%% Title %%%%%%
\title{LiNGAM Models on Minimal Latent Polytrees:\\A Cumulant--Based Discrepancy Approach}
\author{Sang Hyeon Lee\\
  \small Supervisor: Prof.~Dr.~Mathias Drton\\
  \small Mentor: Daniele Tramontano\\[1ex]
  \small Department of Mathematics, Technical University of Munich}
\date{\today}


\begin{document}

\maketitle

\begin{abstract}
This thesis studies linear, non-Gaussian, acyclic models (LiNGAM) whose underlying directed graph is a \emph{minimal latent polytree}. Building on moment identities from the LiNGAM literature, we propose a novel discrepancy matrix based on second and third cumulants of the observed variables. We adapt the algorithm proposed by Etesami et al. to the LiNGAM setting, enabling provably consistent recovery of both observed and latent portions of the polytree. We outline theoretical guarantees and provide an empirical evaluation on synthetic data sets.
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Understanding causality—the fundamental question of which events or variables influence others—has been central to scientific inquiry across disciplines for centuries. From the experimental sciences that seek to establish cause-and-effect relationships through controlled interventions, to the social sciences that must often rely on observational data to infer causal mechanisms, the challenge of distinguishing genuine causal relationships from mere statistical associations remains one of the most important methodological problems in empirical research.

A modern framework for causal inference has its roots in the pioneering work of \citet{pearl:causality:2009}, who formalized the distinction between statistical dependence and causal relationships through the lens of directed acyclic graphs (DAGs) and structural equation models. This framework recognizes that while correlation patterns can suggest possible causal structures, additional assumptions—whether experimental, distributional, or structural—are necessary to identify the direction and magnitude of causal effects from observational data alone.

In parallel, the field of graphical modeling has developed sophisticated methods for representing and learning dependency structures among random variables \citep{laurizen:graphical:1996, Edwards:intro:2000}. These approaches excel at capturing the conditional independence relationships that characterize the joint distribution of multivariate data, but they generally cannot distinguish between statistically equivalent models that encode different causal assumptions. This limitation, known as the Markov equivalence problem, represents a fundamental challenge for causal discovery from observational data.

Causal discovery from observational data emerges at the intersection of these two research traditions. The central challenge is to design methods that can recover not merely the statistical dependency structure, but the underlying causal mechanisms that generated the observed data. This requires exploiting additional constraints beyond those captured by conditional independence relationships—constraints that can break the symmetries inherent in purely associational models and reveal the directional nature of causal influences.

The \emph{Linear Non-Gaussian Acyclic Model} (LiNGAM) \citep{shimizu:hoyer:2006} represents a breakthrough in this endeavor, demonstrating that non-Gaussianity of disturbances suffices to orient edges in a directed acyclic graph that is fully observed. By departing from the Gaussian assumption that underlies many classical approaches, LiNGAM methods can achieve full causal identifiability from observational data alone, without requiring experimental interventions or temporal information. This result has profound implications for causal discovery in domains where controlled experiments are impractical or impossible, such as economics, where randomized interventions on national policy variables are infeasible, or epidemiology, where ethical constraints prohibit experimental manipulation of disease exposure.

However, many practical systems contain latent variables—unobserved confounders, mediators, or common causes—whose omission can severely distort causal inferences. The presence of latent variables introduces additional challenges beyond those encountered in fully observed settings, as these hidden factors can create spurious dependencies among observed variables and mask the true causal structure. Traditional approaches to latent variable modeling often require strong parametric assumptions or prior knowledge about the number and nature of unobserved factors.

This thesis investigates LiNGAMs whose causal structure forms a \emph{minimal latent polytree}, following the terminology of \citet{etesami:kiyavash:coleman:2016}. Polytree structures—directed acyclic graphs whose underlying undirected graph is a tree—represent an important middle ground between the restrictive assumptions of fully observed models and the computational intractability of general latent variable models. While polytrees impose structural constraints that may not hold in all applications, they capture many realistic scenarios while remaining amenable to efficient algorithmic solutions.

The theoretical foundation for our approach builds on the axiomatic framework of \citet{etesami:kiyavash:coleman:2016}, which develops a four-axiom \emph{discrepancy matrix} framework that suffices to learn a latent polytree structure. However, their work instantiates the discrepancy matrix via directed-information estimators specifically designed for time series data rather than single-time-slice observational studies. In this work, we bridge LiNGAM identifiability results with the Etesami et al. framework by constructing a cumulant-based discrepancy measure that satisfies the required axioms while remaining applicable to cross-sectional data.

\paragraph{Related work.} Our cumulant-based discrepancy approach is closely related to several recent developments in latent variable causal discovery. \citet{tramontano:monod:drton:2022} developed algorithms for learning linear non-Gaussian polytrees in the fully observed setting, providing the concentration inequalities for sample cumulants that underpin our finite-sample analysis. A complementary line of work uses \emph{rank constraints of high-order cumulant matrices} rather than discrepancy-based methods for latent polytree learning \citep{cai:etal:2024:rank}. This technique identifies latent structure by testing rank constraints on augmented cumulant matrices $\Psi^{(k)}_{Y;Z} = [C^{(k)}_{Y,Z} \mid C^{(k+1)}_{Y,Z}]$, which combine $k$-th and $(k+1)$-th order cumulants to enable rank testing even when the covariance matrix alone is rank-deficient. Their graphical implication results can distinguish between different structural patterns (atomic-fork, atomic-chain, atomic-Y) through rank conditions. While our discrepancy-based method requires specifying which nodes are latent (typically via out-degree criteria), the rank constraint approach can potentially discover latent nodes directly from observed data, offering complementary strategies for latent polytree identification.

\paragraph{Contributions.} The main contributions are:
\begin{enumerate}[label=(C\arabic*)]
  \item Definition of the \emph{Latent--LiNGAM Polytree Model} (Section~\ref{sec:model}), combining non--Gaussian noise with minimal latent structures.
  \item Construction of a \emph{cumulant discrepancy matrix} (Section~\ref{sec:discrepancy}) and proof that it satisfies the axioms of \citet{etesami:kiyavash:coleman:2016}.
  \item Adaptation of the Separation--Tree--Merger algorithm to the new discrepancy and consistency analysis (Section~\ref{sec:algorithm}).
  \item Empirical study evaluating orientation accuracy and sample complexity under varying latent proportions (Section~\ref{sec:experiments}).
\end{enumerate}

\paragraph{Organisation.} Section~\ref{sec:lingam_background} reviews the requisite graph--theoretic and statistical preliminaries.  The latent--LiNGAM model is formalised in Section~\ref{sec:model}.  Section~\ref{sec:discrepancy} introduces the cumulant discrepancy matrix and establishes its properties.  Section~\ref{sec:algorithm} outlines a learning algorithm and sketches consistency proofs.  Section~\ref{sec:experiments} presents numerical experiments, and Section~\ref{sec:conclusion} concludes with directions for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Non-Gaussian Structural Causal Models}
\label{sec:lingam_background}

A directed graph (digraph) is a pair $G = (V, E)$, where $V$ is the set of vertices and $E \subset V \times V$ is the set of directed edges. We let $V = [p] := \{1, \ldots, p\}$. An element $(i, j) \in E$ may also be denoted by $i \to j$. A digraph $G$ is acyclic (i.e., a DAG) if it does not contain any directed cycle: there is no sequence of vertices $i_0, \ldots, i_k$ with $i_j \to i_{j+1} \in E$ for $j = 0, \ldots, k - 1$ and $i_0 = i_k$.

A path in $G$ is a sequence of vertices $i_0, \ldots, i_k$ such that $i_j \to i_{j+1} \in E$ or $i_{j+1} \to i_j \in E$ for all $j$. It is directed if all the arrows point in the same direction. A \emph{polytree} is a DAG in which there is a unique path between any two vertices.

If $i \to j \in E$, then $i$ is a parent of $j$, and $j$ is a child of $i$. If $G$ contains a directed path from $i$ to $j$, then $i$ is an ancestor of $j$ and $j$ is a descendant of $i$. The sets of parents, children, ancestors, and descendants of $i \in V$ are denoted by $\pa(i)$, $\ch(i)$, $\an(i)$, $\de(i)$, respectively. The \emph{out-degree} of vertex $i$ is the number of children of $i$, i.e., $\text{out-deg}(i) = |\ch(i)|$.

\begin{definition}[Conditional independence]
\label{def:conditional_independence}
Two random variables $X_i$ and $X_j$ are \emph{conditionally independent} given a set of variables $X_C$ if
\[
P(X_i = x_i, X_j = x_j \mid X_C = x_C) = P(X_i = x_i \mid X_C = x_C) P(X_j = x_j \mid X_C = x_C)
\]
for all values $x_i, x_j, x_C$ such that $P(X_C = x_C) > 0$. We denote this relationship by $X_i \indep X_j \mid X_C$.

More generally, for disjoint subsets $A, B, C \subset [p]$, the random vectors $X_A$ and $X_B$ are conditionally independent given $X_C$, written $A \indep B \mid C$, if
\[
P(X_A = x_A, X_B = x_B \mid X_C = x_C) = P(X_A = x_A \mid X_C = x_C) P(X_B = x_B \mid X_C = x_C)
\]
for all values $x_A, x_B, x_C$ such that $P(X_C = x_C) > 0$.
\end{definition}


The joint distribution of $X$ satisfies the \emph{local Markov property} with respect to $G$ if
\[
\{i\} \indep [p] \setminus (\pa(i) \cup \de(i)) \mid \pa(i) \quad \forall i \in [p].
\]

% Replace the existing paragraph about Markov equivalence class with this expanded version:

The \emph{Markov equivalence class} of $G$ is the set of all DAGs that encode the same conditional independence relations, i.e., for which the set of distributions satisfying the local Markov property is the same. See \citet[Chap.~1]{handbook} for further details.

Two fundamental challenges arise when learning causal structure from observational data. First, multiple DAGs can be statistically indistinguishable because they encode the same set of conditional independence constraints. For instance, the two DAGs with two nodes and one edge (so, $1 \to 2$ and $1 \leftarrow 2$) are in the same Markov equivalence class, and cannot be distinguished empirically without imposing further assumptions on the model. This motivates the need for additional identifying assumptions, such as the non-Gaussianity constraints exploited in LiNGAM models. To illustrate, consider a two-node graph: under Gaussianity, the models $X_2 = \lambda X_1 + \varepsilon_2$ (causal direction $1 \to 2$) and $X_1 = \tilde{\lambda} X_2 + \tilde{\varepsilon}_1$ (direction $2 \to 1$) can be statistically indistinguishable. However, with non-Gaussian noise, third-order cumulants break this symmetry. For the direction $1 \to 2$, we have
\[
\frac{\Sigma_{1,2}}{\Sigma_{1,1}} = \frac{\mathcal{C}^{(3)}_{1,1,2}}{\mathcal{C}^{(3)}_{1,1,1}} = \lambda,
\]
while for the reverse direction $2 \to 1$, we obtain
\[
\frac{\Sigma_{1,2}}{\Sigma_{2,2}} = \frac{\mathcal{C}^{(3)}_{1,1,2}}{\mathcal{C}^{(3)}_{2,2,2}} = \tilde{\lambda}.
\]
Only one of these relationships holds for the true causal direction, enabling identifiability.

Methods for causal discovery thus aim to either infer the Markov equivalence class or infer the DAG itself in a model class that renders the graph identifiable. The former approach focuses on recovering the \emph{completed partially directed acyclic graph} (CPDAG), a mixed graph that encodes the causal information common to all members of a Markov equivalence class \citep{meek:1995}. The latter scenario, which is the focus of this work, postulates that the considered models exhibit special properties that permit identification of the full graph \citep{shimizu:hoyer:2006}.

The \emph{skeleton} of a DAG is the undirected graph obtained by replacing each directed edge by an undirected edge. Here, edges are denoted by $\{i,j\} \subseteq E$.


% Replace the existing structural equation paragraph with this expanded version:

\subsection{Structural Equations}
\label{subsec:equations}

A structural equation model hypothesizes that every random variable in $X$ is functionally related to its parent variables, i.e.,
\[
X_i = f_i(X_{\pa(i)}, \varepsilon_i), \quad i \in V,
\]
where the $\varepsilon_i$ are independent noise terms and the $f_i$ are measurable functions.

This framework provides a principled approach to modeling causation by encoding the fundamental insight that effects are generated by their causes \citep{Peters:Elements:2017}. The structural equations make explicit the \emph{causal mechanism} by which each variable is generated: $X_i$ is determined as a function of its direct causes $X_{\pa(i)}$ and an independent random disturbance $\varepsilon_i$. The independence of the noise terms $\varepsilon_i$ reflects the assumption that, conditional on the direct causes, there are no confounding variables affecting multiple outcomes simultaneously.

The key conceptual advantage of this formulation is that it distinguishes between \emph{seeing} and \emph{doing} \citep{pearl:causality:2009}. While observational distributions $P(X)$ capture statistical associations, the structural equations enable us to answer counterfactual questions of the form ``What would happen to $X_j$ if we set $X_i = x_i$?'' Such interventional reasoning is essential for causal inference and policy evaluation.

If the $f_i$ are linear, then we obtain a \emph{linear structural equation model} (LSEM). An LSEM can be written in matrix form as
\begin{equation}
\label{eq:lsem}
X = (I - \Lambda)^{-\top} \varepsilon,
\end{equation}
where $\Lambda = (\lambda_{ij})$ with $\lambda_{ij} \neq 0$ only if $i \to j \in E$.

The linearity assumption, while restrictive, offers significant computational and theoretical advantages. Linear models admit closed-form solutions for interventional distributions and enable the use of powerful algebraic tools for structure learning. Moreover, in many applications, linear approximations provide reasonable first-order descriptions of complex causal relationships.

An LSEM constrains the dependence structure on the coordinates of $X$, but not the mean. Hence, when working with the LSEM, we may assume without loss of generality that $\mathbb{E}[\varepsilon_i] = 0$, which implies $\mathbb{E}[X_i] = 0$ for all $i \in V$.

Let $\varepsilon^{(2)} = (\mathbb{E}[\varepsilon_i \varepsilon_j])_{ij}$ be the covariance matrix of $\varepsilon$, which is a diagonal matrix by independence, and write $\varepsilon^{(2)}_i := \mathbb{E}[\varepsilon_i^2] > 0$ for its $i$th diagonal entry. The covariance matrix of $X$ is then the positive definite matrix
\begin{equation}
\label{eq:Sigma}
\Sigma = (I - \Lambda)^{-\top} \varepsilon^{(2)} (I - \Lambda)^{-1}.
\end{equation}

\begin{proof}[Derivation of equation~\eqref{eq:Sigma}]
From the structural equation~\eqref{eq:lsem}, we have $X = (I - \Lambda)^{-\top} \varepsilon$. The covariance matrix of $X$ is therefore
\begin{align}
\Sigma &= \mathbb{E}[XX^{\top}] \quad \text{(since $\mathbb{E}[X] = 0$)} \\
&= \mathbb{E}\left[(I - \Lambda)^{-\top} \varepsilon \varepsilon^{\top} (I - \Lambda)^{-1}\right] \\
&= (I - \Lambda)^{-\top} \mathbb{E}[\varepsilon \varepsilon^{\top}] (I - \Lambda)^{-1} \\
&= (I - \Lambda)^{-\top} \varepsilon^{(2)} (I - \Lambda)^{-1},
\end{align}
where the third equality uses the linearity of expectation and the final equality follows from the definition of $\varepsilon^{(2)}$.
\end{proof}

This relationship between the structural parameters $(\Lambda, \varepsilon^{(2)})$ and the observed covariance matrix $\Sigma$ is fundamental to the identifiability analysis that follows \citep[see, e.g.,][]{tramontano:monod:drton:2022}. When the noise is Gaussian, equation~\eqref{eq:Sigma} captures all distributional information, leading to the Markov equivalence problem discussed above. The non-Gaussian setting, which we explore next, breaks this equivalence and enables full structural recovery.

\subsection{Cumulants in Gaussian and Non-Gaussian Models}
\label{subsec:non-gaussian}

Cumulants are alternative representations of moments of a distribution \citep{comon:jutten:handbook}. Here, we formalize the definition in higher order settings and discuss their implications under Gaussian and non-Gaussian errors.

\begin{definition}[Cumulant tensor]
\label{def:cumulant_tensor}
The $k$th cumulant tensor of a random vector $(X_1, \ldots, X_p)$ is the $k$-way tensor in $\mathbb{R}^{p \times \cdots \times p} \equiv (\mathbb{R}^p)^{\otimes k}$ whose entry in position $(i_1, \ldots, i_k)$ is the joint cumulant
\[
\cum(X_{i_1}, \ldots, X_{i_k}) := \sum_{(A_1, \ldots, A_L)} (-1)^{L-1} (L-1)! \mathbb{E}\left[\prod_{j \in A_1} X_j\right] \cdots \mathbb{E}\left[\prod_{j \in A_L} X_j\right],
\]
where the sum is taken over all partitions $(A_1, \ldots, A_L)$ of the multiset $\{i_1, \ldots, i_k\}$.
\end{definition}

In our context, the variables have mean 0, so
\begin{align}
\cum(X_i) &= \mathbb{E}[X_i] = 0, \\
\cum(X_{i_1}, X_{i_2}) &= \text{Cov}[X_{i_1}, X_{i_2}] = \mathbb{E}[X_{i_1} X_{i_2}].
\end{align}

More generally, the sum can be restricted to the partitions in which all blocks $A_i$ have at least two elements. In particular,
\begin{align}
\cum(X_{i_1}, X_{i_2}, X_{i_3}) &= \mathbb{E}[X_{i_1} X_{i_2} X_{i_3}], \\
\cum(X_{i_1}, X_{i_2}, X_{i_3}, X_{i_4}) &= \mathbb{E}[X_{i_1} X_{i_2} X_{i_3} X_{i_4}] - \mathbb{E}[X_{i_1} X_{i_2}] \mathbb{E}[X_{i_3} X_{i_4}] \\
&\quad - \mathbb{E}[X_{i_1} X_{i_3}] \mathbb{E}[X_{i_2} X_{i_4}] - \mathbb{E}[X_{i_1} X_{i_4}] \mathbb{E}[X_{i_2} X_{i_3}].
\end{align}

The following powerful result dictates a simple condition that characterizes the Gaussianity of $X$.

\begin{theorem}[Marcinkiewicz's theorem {\citealp{marcinkiewicz:1939}}]
\label{thm:marcinkiewicz}
If there exists $k$ such that $\cum(X_{i_1}, \ldots, X_{i_j}) = 0$ for all $j \geq k$, then $k = 3$ and $X$ has a multivariate Gaussian distribution.
\end{theorem}

This theorem establishes that non-Gaussian distributions necessarily have non-zero cumulants of order three or higher, making these higher-order moments essential for distinguishing between Gaussian and non-Gaussian models.


\begin{lemma}
\label{lem:indep_cum}
If the variables $ \varepsilon_1,\dots,\varepsilon_n$ are jointly independent, then $\cum(\varepsilon_{i_1},\dots,\varepsilon_{i_k})=0$ unless $i_1=\dots=i_k$.
\end{lemma}

\begin{definition}[Tucker product]
\label{def:tucker_product}
Let $\mathcal{T}$ be a $k$-way tensor in $\mathbb{R}^{n_1 \times \cdots \times n_k}$ and let $A^{(j)} \in \mathbb{R}^{m_j \times n_j}$ be matrices for $j = 1, \ldots, k$. The \emph{Tucker product} of $\mathcal{T}$ with the matrices $A^{(1)}, \ldots, A^{(k)}$ is the $k$-way tensor
\[
\mathcal{T} \bullet A^{(1)} \bullet \cdots \bullet A^{(k)} \in \mathbb{R}^{m_1 \times \cdots \times m_k}
\]
with entries
\[
\left(\mathcal{T} \bullet A^{(1)} \bullet \cdots \bullet A^{(k)}\right)_{i_1,\ldots,i_k} = \sum_{j_1=1}^{n_1} \cdots \sum_{j_k=1}^{n_k} \mathcal{T}_{j_1,\ldots,j_k} A^{(1)}_{i_1,j_1} \cdots A^{(k)}_{i_k,j_k}.
\]
When all matrices are identical, $A^{(1)} = \cdots = A^{(k)} = A$, we write this as $\mathcal{T} \bullet [A]_{j=1}^k$.
\end{definition}

\begin{lemma}
\label{lem:tucker}
Let the random vector $X$ follow the LSEM from \eqref{eq:lsem} with noise vector $\varepsilon$.  Let
$\mathcal{C}^{(k)}$ and $\varepsilon^{(k)}$ be the $k$th order cumulant tensors of $X$ and $\varepsilon$, respectively.  Then
\begin{align*}
    \mathcal{C}^{(k)}&= \varepsilon^{(k)}\bullet \big[(I-\Lambda)^{-1} \big]_{j=1}^k\\
    &=\varepsilon^{(k)}\bullet(I-\Lambda)^{-1}\bullet \dots \bullet (I-\Lambda)^{-1}
\end{align*}
is the Tucker product of $\varepsilon^{(k)}$ and $k$ copies of $(I-\Lambda)^{-1}$.
\end{lemma}
Notice here that $\mathcal{C}^{(k)}$ reduces to \eqref{eq:Sigma} when $k=2$.

See \citet{comon:jutten:handbook} and references therein for proofs of Theorem \ref{thm:marcinkiewicz} and Lemmas \ref{lem:indep_cum} and \ref{lem:tucker}.

The next definition introduces the cumulant model obtained from the LSEM \eqref{eq:lsem}.  %Recall that tensor $\varepsilon^{(k)}$ is diagonal if only entries in positions $(i,\dots,i)$, $i\in[p]$, are nonzero.

\begin{definition}
Let $G=(V,E)$ be a DAG, and let $K\geq2$ be an integer.  The $K$th cumulant model of $G$ is the set of $K$-way tensors
\begin{multline*}
    \mathcal{M}^{(K)}(G)=
    \{\varepsilon^{(K)}\bullet \big[(I-\Lambda)^{-1} \big]_{j=1}^K\;:
    \Lambda\in\mathbb{R}^E,\; \varepsilon^{(K)}\in(\mathbb{R}^{p})^K \ \text{diagonal}\}.
\end{multline*}
Here, $\mathbb{R}^E$ is the set of $p\times p$ matrices with support $E$.
Further, the cumulants up to order K defined by G are modeled by
\begin{equation}
    \mathcal{M}^{(\leq K)}(G)=\mathcal{M}^{(2)}(G)\times\dots\times\mathcal{M}^{(K)}(G).
\end{equation}
%to be the model of cumulants up to order $K$ defined by $G$.
\end{definition}

By Theorem~\ref{thm:marcinkiewicz}, all multivariate Gaussian vectors $X$ correspond to the zero element of $\mathcal{M}^{(K)}(G)$ for $k\geq3$.
% From now on, we assume the variables in $\varepsilon$ to be non-Gaussian.

When the errors in an LSEM are Gaussian, all distributional information is captured by the covariance matrix and equivalence issues arise that hinder identifiability of the full graph.  %However, the situation is different when the errors are non-Gaussian.
It then becomes necessary to consider non-Gaussian settings.  Relaxing the constraint of Gaussianity gives rise to the class of LiNGAMs where the underlying graph now becomes identifiable \citep{shimizu:hoyer:2006,shimizu:2011}.  We will exploit this property algorithmically and use the signal provided by higher cumulants; we do this by way of {\em treks}.

\begin{definition}[Multi-Trek]
A $k$-trek between vertices $i_1,\dots,i_k\in V$ of a DAG $G=(V,E)$ is a collection of directed paths $T=(P_1,\dots,P_k)$ in $G$ that share the same source and have $i_j$ as the sink of $P_j$ for all $j$. The common source node is the top of the trek $\ttop(T)$.  A trek is simple if the top node is the unique node on all the paths.
\end{definition}

We denote the set of $k$-treks between $i_1,\dots,i_k$ by $\mathcal{T}(i_1,\dots,i_k)$ and the set of simple treks by $\mathcal{S}(i_1,\dots,i_k)$. See Figure~\ref{fig:trek} for an example.
\begin{figure}[ht]
\centering
\begin{tikzpicture}[scale=0.5]
\begin{scope}[every node/.style={circle,thick,draw}]
    \node (1) at (0,0) {t};
    \node (2) at (-6,-4) {1};
    \node (3) at (-2,-4) {2};
    \node (4) at (2,-4) {3};
    \node (5) at (6,-4) {4};
\end{scope}

\begin{scope}[>={Stealth[black]},
              every edge/.style={draw=black,thick}]
    \path [->] (1) edge node{} (2);
    \path [->] (1) edge node{} (3);
    \path [->] (1) edge node{} (4);
    \path [->] (1) edge node{} (5);
\end{scope}
\end{tikzpicture}
\caption{Example of a 4-trek.}
\label{fig:trek}
\end{figure}

If $P$ is a directed path in the DAG $G=(V,E)$ and $\Lambda=(\lambda_{ij})\in\mathbb{R}^E$, then $\lambda^P=\prod_{(i,j)\in P}\lambda_{ij}$ is a path monomial.
%the path monomial given by the product of $\lambda_{ij}$ with $i\to j$ an edge in $P$.
For a $k$-trek $T=(P_1,\dots,P_k)$, set $\lambda^T:=\lambda^{P_1}\cdots\lambda^{P_k}$.

\begin{proposition}[Multi-Trek Rule]
\label{prop:multi:trek}
The $k$th order cumulant tensor $\mathcal{C}^{(k)}(G)$ of $X$ can be expressed as
\begin{equation}
\label{eq:trek}
    \mathcal{C}^{(k)}_{i_1,\dots,i_k}(G)=\sum\varepsilon^{(k)}_{\ttop(T)}\lambda^T,
\end{equation}
where the sum is over all the treks $T$ in $\mathcal{T}(i_1,\dots,i_k)$ and $\varepsilon^{(k)}_{\ttop(T)}$ denotes the $\ttop(T)$ diagonal entry of $\varepsilon^{(k)}$.
\end{proposition}

Proposition \ref{prop:multi:trek} follows from Lemma~\ref{lem:tucker} and expanding the entries of $(I-\Lambda)^{-1}$ into sums of path monomials as in the usual trek rule for covariances \citep{robeva:2021}.

\begin{corollary}[Simple Multi-Trek Rule]
\label{cor:simple-trek-rule}
The $k$th order cumulant tensor $\mathcal{C}^{(k)}(G)$ of $X$ can be expressed as
\begin{equation}
    \mathcal{C}^{(k)}_{i_1,\dots,i_k}(G)=\sum \mathcal{C}^{(k)}_{\ttop(S)}(G)\lambda^{S},
\end{equation}
where the sum is extended to all the simple treks $S$ in $\mathcal{S}(i_1,\dots,i_k)$.
%, and the new term $m^{(k)}_{i}$.
\end{corollary}

\begin{corollary}
\label{cor:simple_trek_2}
The $i$th diagonal entry of $\mathcal{C}^{(k)}$ is
\begin{equation*}
     \mathcal{C}^{(k)}_{i}(G)=\displaystyle\sum_{p_1,\dots,p_k\in \pa(i)}\lambda_{p_1, i}\cdots\lambda_{p_k,i}\mathcal{C}^{(k)}_{p_1,\dots,p_k}(G)+\varepsilon^{(k)}_i.
\end{equation*}
\end{corollary}

\subsection{Polytree Models}
\label{subsec:polytree-models}
For general graphs, the algebraic relations among the cumulants may be far more complicated than the bivariate case (as illustrated in the two-node example of \citet[Example A.1]{tramontano:monod:drton:2022}) and have not yet been fully characterized. However, there exists a generalization of rank-one constraints for polytrees, which we now discuss.

Since there is at most one directed path between any two nodes of a polytree $G$, there is at most one simple trek between any set of nodes $i_1,\dots,i_k$. The simple multi-trek rule then reduces to $C^{(k)}_{i_1,\dots,i_k}(G)=\lambda^{S}\mathcal{C}^{(k)}_{\ttop(S)}$ for a trek between nodes with $S$ being the unique simple trek; denote the top of the simple trek between $i_1,\dots,i_k$, if it exists by $\ttop(i_1,\dots,i_k)$. Also, $C^{(k)}_{i_1,\dots,i_k}(G)=0$ if there is no $k$-trek between the nodes.

For any two vertices $i\not=j$, let $c^{(i,j),k}_m$ denote the $k$th order cumulant $\mathcal{C}^{(k)}_{{i\dots i},{j\dots j}}(G)$, where the first $m$ indices are equal to $i$ and the remaining $k-m$ equal $j$.
% ---------- Section 4 : matrix definition (exact Prop. 2.10) ----------
% For i ≠ j observed and an integer K ≥ 3
\newcommand{\cumedge}[3]{c_{#1,#2#3}}  % usage: \cumedge{e}{k}{m}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Latent--LiNGAM Polytree Model}
\label{sec:model}

We now specialize the general LiNGAM framework to the polytree setting with latent variables. This restriction provides significant computational advantages while still capturing important classes of causal relationships.

\begin{definition}[Minimal latent polytree]\label{def:minimal_polytree}
Let $G=(V,E)$ be a DAG whose underlying undirected graph is a tree. Partition $V=O\cup L$ into observed and latent vertices. The pair $(G,O)$ is a \emph{minimal latent polytree} if every $\ell\in L$ has out-degree at least~$2$.
\end{definition}

The minimality condition ensures that no latent variable is redundant—removing any latent variable would disconnect the observed variables or change the conditional independence structure among them. This constraint is essential for identifiability, as latent nodes with out-degree 1 cannot be distinguished from direct edges between observed variables. To see why, consider a latent variable $\ell$ with parent $X_j$ and single child $X_i$. The path $X_j \to \ell \to X_i$ induces the same conditional independence relationships among observed variables as the direct edge $X_j \to X_i$, rendering the latent node $\ell$ unidentifiable from observational data alone. Even in the LiNGAM setting with non-Gaussian noise, this equivalence persists at the level of observed variables. Specifically, the model with latent variable
\begin{align*}
X_j &= \varepsilon_j, \\
\ell &= a_1 X_j + \varepsilon_\ell, \\
X_i &= a_2 \ell + \varepsilon_i
\end{align*}
is observationally equivalent to the direct edge model
\begin{align*}
X_j &= \varepsilon_j, \\
X_i &= a_2 a_1 X_j + (a_2 \varepsilon_\ell + \varepsilon_i),
\end{align*}
where the composite noise term $\tilde{\varepsilon}_i := a_2 \varepsilon_\ell + \varepsilon_i$ is non-Gaussian whenever either $\varepsilon_\ell$ or $\varepsilon_i$ is non-Gaussian. Thus, both models induce identical joint distributions over the observed variables $(X_j, X_i)$, making it impossible to detect the presence of $\ell$ from observed data. This fundamental limitation motivates restricting attention to latent structures where each latent variable has at least two children, ensuring structural identifiability. This fundamental limitation motivates the minimality constraint in latent polytree models \citep[cf.][]{etesami:kiyavash:coleman:2016}.

\begin{definition}[Latent--LiNGAM polytree model]\label{def:latent_lingam_model}
A random vector $X\in\mathbb R^{|V|}$ follows the latent--LiNGAM polytree model on $(G,O)$ if:
\begin{enumerate}[label=(\roman*)]
  \item The distribution of $X$ satisfies the structural equation~\eqref{eq:lsem} with coefficient matrix~$\Lambda$ compatible with $G$.
  \item The set $\varepsilon=(\varepsilon_i)_{i\in V}$ has independent, non--Gaussian entries with finite third moments.
  \item Only $(X_i)_{i\in O}$ are observed.
\end{enumerate}
\end{definition}

This model combines the identifiability advantages of non-Gaussian noise with the computational tractability of polytree structures. The restriction to polytrees ensures that there is a unique undirected path between any two nodes, which simplifies both the theoretical analysis and algorithmic development.

\subsection{Key Properties of the Model}

Under the latent-LiNGAM polytree assumptions, several important properties hold:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Path uniqueness}: The unique directed path between any $i,j\in O$ factors through their lowest common ancestor (LCA). This property will underpin our discrepancy construction.

\item \textbf{Moment identifiability}: The non-Gaussian noise assumption ensures that higher-order cumulants provide sufficient information to identify both the structure and parameters of the model, breaking the equivalence classes that arise under Gaussianity.

\item \textbf{Computational tractability}: The polytree constraint reduces the complexity of structure learning algorithms from exponential (in general DAGs) to polynomial time.
\end{enumerate}

The combination of these properties makes the latent-LiNGAM polytree model particularly well-suited for developing efficient structure learning algorithms based on cumulant information, as we will demonstrate in the subsequent sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cumulant--Based Discrepancy Matrix}
\label{sec:discrepancy}
%--------------------------------------------------------------------
%  Definition 7 from Etesami–Kiyavash–Coleman (Neural Computation 2016)
%--------------------------------------------------------------------
\begin{definition}[Discrepancy on a polytree {\citealp[Def.~7]{etesami:kiyavash:coleman:2016}}]
\label{def:discrepancy_paper1}
Given a polytree $\vec T=(V,\vec E)$ with root set $R$, every function
$\gamma:V\times V\to\mathbb R$ that satisfies the following four criteria
is called a \emph{discrepancy} on $\vec T$:
\begin{enumerate}[label=(\arabic*)]
  \item $\gamma(v_1,v_2)=0$ $\iff$ either $v_1$ is an ancestor of $v_2$ or $v_1=v_2$.
  \item If $\operatorname{LCA}(v_1,v_2)=\operatorname{LCA}(v_1,v_3)$, then $\gamma(v_1,v_2)=\gamma(v_1,v_3)$.
  \item If $\operatorname{LCA}(v_1,v_2)$ lies on the path from $\operatorname{LCA}(v_1,v_3)$ to $v_1$, then $\gamma(v_1,v_2) < \gamma(v_1,v_3)$.
  \item $\gamma(v_1,v_2)<0$ $\iff$ $v_1$ and $v_2$ have no common ancestor.
\end{enumerate}

The image of such functions can be presented by the discrepancy matrix:
\[
  \Gamma_V := \left[ \gamma(v_i, v_j) \right], \quad v_i, v_j \in V.
\]

Note that for a given tree, the discrepancy matrix is not unique. Any function that satisfies the conditions in Definition~\ref{def:discrepancy_paper1} is a valid discrepancy measure.

\end{definition}


\begin{definition}[Cumulant Discrepancy Measure]
\label{def:discrepancy}
Motivated by the rank conditions in \citet[Prop.~2.10]{tramontano:monod:drton:2022}, which show that certain cumulant matrices characterize edge orientations in fully observed polytrees, we define the following discrepancy measure between observed nodes. For any pair of observed nodes $i, j \in O$, we define the cumulant discrepancy measure $\gamma(i,j)$ as
\begin{equation}\label{eq:gamma}
\gamma(i,j) =
\begin{cases}
-1, & \text{if } \rho_{i,j} = 0, \\[0.5ex]
0, & \text{if } (\Sigma_{i,i} \mathcal{C}^{(3)}_{i,i,j} - \Sigma_{i,j} \mathcal{C}^{(3)}_{i,i,i}) = 0 \text{ or } i = j, \\[0.5ex]
\dfrac{\mathcal{C}^{(3)}_{i,j,j} \, \Sigma_{i,i}}{\mathcal{C}^{(3)}_{i,i,j} \, \Sigma_{i,j}}, & \text{otherwise}.
\end{cases}
\end{equation}
\noindent
where $\Sigma$ is the covariance matrix, and $\mathcal{C}^{(3)}$ is the third-order cumulant tensor.

\end{definition}

\begin{remark}
Throughout this work, we restrict our analysis to third-order cumulants ($k=3$). However, the extension to higher-order cumulants is straightforward: for any $k \geq 3$, one can replace $\mathcal{C}^{(3)}_{i,j,j}$ and $\mathcal{C}^{(3)}_{i,i,j}$ in \eqref{eq:gamma} with appropriate $k$th-order cumulants $\mathcal{C}^{(k)}$ following the same ratio structure. The choice of third-order moments provides a balance between identifiability power and estimation complexity in practice.
\end{remark}

\begin{lemma}[Wright's Formula {\citealp{wright:1960}}]
\label{lem:wright}
In a LiNGAM polytree model, the correlation $\rho_{i,j} = \text{Corr}[X_i,X_j]$ satisfies
\begin{equation}
  |\rho_{i,j}| =
  \begin{cases}
    \prod |\rho_e|, & \text{if } \mathcal{T}(i,j) \ne \emptyset, \\[0.5ex]
    0, & \text{otherwise},
  \end{cases}
\end{equation}
where the product is taken over the edges $e$ of the unique trek connecting $i$ and $j$, and $\rho_e$ denotes the correlation between the variables at the endpoints of $e$.
\end{lemma}

\begin{remark}
This result implies that $\rho_{i,j} = 0$ if and only if there exists no trek connecting $i$ and $j$ in the polytree. This is particularly useful in interpreting the case $\gamma(i,j) = -1$ in Definition~\ref{def:discrepancy}.
\end{remark}

\begin{proposition}[Axioms]\label{prop:axioms}
The map $\gamma:O\times O\to\mathbb R$ defined in Definition~\ref{def:discrepancy} satisfies the four axioms of Definition~\ref{def:discrepancy_paper1} on the latent--LiNGAM polytree model, provided the cumulants exist.
\end{proposition}

\begin{proof}
We verify that the cumulant discrepancy measure $\gamma: O \times O \to \mathbb{R}$ defined in Definition~\ref{def:discrepancy} satisfies the four axioms of Definition~\ref{def:discrepancy_paper1}, assuming all required cumulants exist.

\begin{enumerate}[label=(\arabic*)]
  \item
    From Definition~\ref{def:discrepancy}, $\gamma(i,i) = 0$. Assume $i \ne j$ and that $i$ is an ancestor of $j$. Then we know that there is a simple trek between $i$ and $j$. From the simple trek rule we know that $\Sigma_{i,j} = \lambda^{S}\Sigma_{i,i}$ and $\mathcal{C}^{(3)}_{i,i,j} = \lambda^{S}\mathcal{C}^{(3)}_{i}$, where $S$ is the simple trek between $i$ and $j$. Hence we have $(\Sigma_{i,i} \mathcal{C}^{(3)}_{i,i,j} - \Sigma_{i,j} \mathcal{C}^{(3)}_{i,i,i}) = 0$.

  \item Let $v_i$ be the lowest common ancestor of two observed variables $v_j$ and $v_k$. Suppose also that $v_i$ is the lowest common ancestor of $v_j$ and $v_\ell$. We will show that
  \[
  \gamma(j,k) = \gamma(j,\ell).
  \]
  By Definition~\ref{def:discrepancy}, for distinct observed vertices $u,v$ one has
    \[
      \gamma(u,v)
      =
      \frac{\mathcal{C}^{(3)}_{u,v,v}\,\Sigma_{u,i}}{\mathcal{C}^{(3)}_{u,u,v}\,\Sigma_{u,v}},
    \]
    where $\Sigma$ is the covariance matrix and $\mathcal{C}^{(3)}$ is the third-order cumulant tensor. We now express each entry in this ratio using the simple trek rule (Corollary~\ref{cor:simple-trek-rule}).

    \begin{enumerate}
      \item Because $v_i$ is the lowest common ancestor of $v_j$ and $v_k$, every simple trek from $v_j$ to $v_k$ factors through $v_i$. The simple trek rule implies
      \[
        \mathcal{C}^{(3)}_{j,j,k}
        =
        \bigl(\lambda^{P(i,j)}\bigr)^2\,\lambda^{P(i,k)}\,\mathcal{C}^{(3)}_{i},\qquad
        \Sigma_{j,k}
        =
        \lambda^{P(i,j)}\,\lambda^{P(i,k)}\,\Sigma_{i},
      \]
      where $\lambda^{P(a,b)}$ denotes the product of structural coefficients along the unique directed path $P(a,b)$ in the polytree, and $\mathcal{C}^{(3)}_{i}$, $\Sigma_i$ are the third-order cumulant and variance of $X_{v_i}$, respectively.

      \item Similarly,
      \[
        \mathcal{C}^{(3)}_{j,k,k}
        =
        \lambda^{P(i,j)}\,\bigl(\lambda^{P(i,k)}\bigr)^2\,\mathcal{C}^{(3)}_{i},\qquad
        \Sigma_{j,j}
        =
        \,\bigl(\lambda^{P(i,j)}\bigr)^2\,\Sigma_{i}.
      \]
    \end{enumerate}

    Substituting these identities into the definition of $\gamma$ gives
    \begin{align*}
      \gamma(j,k)
      &= \frac{\mathcal{C}^{(3)}_{j,k,k}}{\mathcal{C}^{(3)}_{j,j,k}}
         \cdot\frac{\Sigma_{j,j}}{\Sigma_{j,k}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(i,j)}\,(\lambda^{P(i,k)})^2\,\mathcal{C}^{(3)}_{i}\bigr]
        \cdot
        \bigl[\bigl(\lambda^{P(i,j)}\bigr)^2\,\Sigma_{i}\bigr]
      }{
        \bigl[\bigl(\lambda^{P(i,j)}\bigr)^2\,\lambda^{P(i,k)}\,\mathcal{C}^{(3)}_{i}\bigr]
        \cdot
        \bigl[\lambda^{P(i,j)}\,\lambda^{P(i,k)}\,\Sigma_{i}\bigr]
      } \\[1ex]
      &= 1.
    \end{align*}

    An identical computation replacing $v_k$ by $v_\ell$ yields
    \[
      \gamma(j,\ell)
      = 1.
    \]

    This completes the verification of Axiom~(2).

  \item Suppose $d:=\mathrm{LCA}(i,j)$ lies strictly below $c:=\mathrm{LCA}(i,k)$ on the unique path from \(c\) to \(i\).  Write \(\lambda^{P(u,v)}\) for the product of structural coefficients on the directed path \(P(u,v)\) in the polytree.  Using the simple–trek rule again, we obtain the following identities:
  \[
    \begin{aligned}
    \mathcal{C}^{(3)}_{i,j,j}&=\lambda^{P(d,i)}\,\bigl(\lambda^{P(d,j)}\bigr)^{2}\,\mathcal{C}^{(3)}_{d},&
    \mathcal{C}^{(3)}_{i,i,j}&=\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\lambda^{P(d,j)}\,\mathcal{C}^{(3)}_{d},\\[0.5ex]
    \mathcal{C}^{(3)}_{i,k,k}&=\lambda^{P(c,i)}\,\bigl(\lambda^{P(c,k)}\bigr)^{2}\,\mathcal{C}^{(3)}_{c},&
    \mathcal{C}^{(3)}_{i,i,k}&=\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\lambda^{P(c,k)}\,\mathcal{C}^{(3)}_{c},\\[0.5ex]
    \Sigma_{i,j}&=\lambda^{P(d,i)}\,\lambda^{P(d,j)}\,\Sigma_{d,d},&
    \Sigma_{i,k}&=\lambda^{P(c,i)}\,\lambda^{P(c,k)}\,\Sigma_{c,c}.&
    \end{aligned}
  \]
  Substituting into~\eqref{eq:gamma}, the path monomials and third–order cumulants cancel, and we find
  \begin{align*}
      \gamma(i,j)
      &= \frac{\mathcal{C}^{(3)}_{i,j,j}}{\mathcal{C}^{(3)}_{i,i,j}}
         \cdot\frac{\Sigma_{i,i}}{\Sigma_{i,j}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(d,i)}\,(\lambda^{P(d,j)})^2\,\mathcal{C}^{(3)}_{d}\bigr]
        \cdot
        \Sigma_{i,i}
      }{
        \bigl[\bigl(\lambda^{P(d,i)}\bigr)^2\,\lambda^{P(d,j)}\,\mathcal{C}^{(3)}_{d}\bigr]
        \cdot
        \bigl[\lambda^{P(d,i)}\,\lambda^{P(d,j)}\,\Sigma_{d,d}\bigr]
      } \\[1ex]
      &= \frac{\Sigma_{i,i}}{\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\Sigma_{d,d}},
    \end{align*}

  \begin{align*}
      \gamma(i,k)
      &= \frac{\mathcal{C}^{(3)}_{i,k,k}}{\mathcal{C}^{(3)}_{i,i,k}}
         \cdot\frac{\Sigma_{i,i}}{\Sigma_{i,k}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(c,i)}\,(\lambda^{P(c,k)})^2\,\mathcal{C}^{(3)}_{c}\bigr]
        \cdot
        \Sigma_{i,i}
      }{
        \bigl[\bigl(\lambda^{P(c,i)}\bigr)^2\,\lambda^{P(c,k)}\,\mathcal{C}^{(3)}_{c}\bigr]
        \cdot
        \bigl[\lambda^{P(c,i)}\,\lambda^{P(c,k)}\,\Sigma_{c,c}\bigr]
      } \\[1ex]
      &= \frac{\Sigma_{i,i}}{\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\Sigma_{c,c}}.
    \end{align*}

  Taking the ratio gives
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\Sigma_{c,c}}{\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\Sigma_{d,d}}.
  \]
  Because \(c\) is an ancestor of \(d\), the path from \(c\) to \(i\) factors through \(d\).  Hence
  \(\lambda^{P(c,i)}=\lambda_{c,d}\,\lambda^{P(d,i)}\) and
  \(\bigl(\lambda^{P(c,i)}\bigr)^{2}=\lambda_{c,d}^{2}\,\bigl(\lambda^{P(d,i)}\bigr)^{2}\).  Substituting yields
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\lambda_{c,d}^{2}\,\Sigma_{c,c}}{\Sigma_{d,d}}.
  \]
  Finally, using the Corollary~\ref{cor:simple_trek_2} and that in a polytree there can be at most one simple trek between $c$ and $d$, we have that
  \(\Sigma_{d,d}=\lambda_{c,d}^{2}\,\Sigma_{c,c}+\omega_{d}^{2}\), where \(\omega_{d}^{2}=\var(\varepsilon_{d})>0\) is the variance of the disturbance at \(d\).  Consequently,
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\lambda_{c,d}^{2}\,\Sigma_{c,c}}{\lambda_{c,d}^{2}\,\Sigma_{c,c}+\omega_{d}^{2}}
    < 1,
  \]
  proving that \(\gamma(i,j)<\gamma(i,k)\) whenever \(\mathrm{LCA}(i,j)\) is strictly below \(\mathrm{LCA}(i,k)\).

  \item
  If $i$ and $j$ have no common ancestor, then there exists no trek between them. Hence from Lemma~\ref{lem:wright} we have $\rho_{i,j} = 0$, and thus $\gamma(i,j) = -1 < 0$.
\end{enumerate}
\end{proof}

\subsection{Example: Discrepancy Matrix on a Four‐Node Polytree}

We illustrate the cumulant‐based discrepancy measure on a simple polytree with four observed nodes.  Let $V=\{v_1,v_2,v_3,v_4\}$ and consider the directed edges
\[
  v_1 \longrightarrow v_2,\qquad
  v_1 \longrightarrow v_3,\qquad
  v_3 \longrightarrow v_4,
\]
depicted in Figure~\ref{fig:example-polytree}.  In this example every vertex is observed; there are no latent variables.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[scale=1.1,node distance=1.5cm and 2cm]
  \node[circle,draw](v1) {$v_1$};
  \node[circle,draw,below left of=v1](v2) {$v_2$};
  \node[circle,draw,below right of=v1](v3) {$v_3$};
  \node[circle,draw,below right of=v3](v4) {$v_4$};
  \draw[->] (v1) -- (v2);
  \draw[->] (v1) -- (v3);
  \draw[->] (v3) -- (v4);
\end{tikzpicture}
\caption{A four‐node polytree with edges $v_1\to v_2$, $v_1\to v_3$ and $v_3\to v_4$.}
\label{fig:example-polytree}
\end{figure}

\paragraph{Structural parameters.}
Assign structural coefficients $\lambda_{1,2}=2$, $\lambda_{1,3}=3$ and $\lambda_{3,4}=4$.  Let the noise variables $\varepsilon_i$ be independent with variances $\sigma_i^2 = 1$ and third cumulants $\kappa_i = 1$ for $i=1,2,3,4$.  The structural equations are then
\[
  X_1 = \varepsilon_1,\quad
  X_2 = 2\,X_1 + \varepsilon_2,\quad
  X_3 = 3\,X_1 + \varepsilon_3,\quad
  X_4 = 4\,X_3 + \varepsilon_4 = 12\,X_1 + 4\,\varepsilon_3 + \varepsilon_4.
\]

\paragraph{Covariances and third cumulants.}
From these recursions one computes the covariance matrix
\[
\Sigma =
\begin{pmatrix}
 1 &  2 &  3 & 12\\
 2 &  5 &  6 & 24\\
 3 &  6 & 10 & 40\\
12 & 24 & 40 & 161
\end{pmatrix}.
\]
The third‐order cumulant tensor $\mathcal{C}^{(3)}$ has diagonal entries
\[
 \mathcal{C}^{(3)}_{1,1,1} = 1,\quad
 \mathcal{C}^{(3)}_{2,2,2} = 2^3 + 1 = 9,\quad
 \mathcal{C}^{(3)}_{3,3,3} = 3^3 + 1 = 28,\quad
 \mathcal{C}^{(3)}_{4,4,4} = (12)^3 + (4)^3 + 1 = 1793,
\]
using the simple trek rule, where for $\mathcal{C}^{(3)}_{4,4,4}$, it can also be shown differently with Corollary~\ref{cor:simple_trek_2} :
\[
 \mathcal{C}^{(3)}_{4,4,4} = \lambda_{3,4}^3\cdot\mathcal{C}^{(3)}_{3,3,3} + \kappa_4 = 4^3\cdot28+1=1793.
\]
Moreover, with a polytree, the only non–vanishing mixed entries are of the form
\[
\mathcal{C}^{(3)}_{i,j,k} \;=\; \lambda^{P(h,i)}\lambda^{P(h,j)}\lambda^{P(h,k)}\kappa_h,
\]
where $\lambda^{P(h,i)}$ denotes the path monomial from $h$ to $i$, with $h$ being the unique common ancestor of $i$,$j$ and $k$. For instance, $\mathcal{C}^{(3)}_{2,3,3}=2\cdot 3^2\cdot1=18$ and $\mathcal{C}^{(3)}_{2,3,4}=2\cdot3\cdot12\cdot1=72$.

\paragraph{Discrepancy matrix.}
Applying Definition~\ref{def:discrepancy} yields the following cumulant discrepancy matrix~$\Gamma=[\gamma(v_i,v_j)]$:
\[
\Gamma
=
\begin{pmatrix}
 0 & 0 & 0 & 0\\[0.2ex]
 \frac{5}{4} & 0 & \frac{5}{4} & \frac{5}{4}\\[0.4ex]
 \frac{10}{9} & \frac{10}{9} & 0 & 0\\[0.4ex]
 \frac{161}{144} & \frac{161}{144} & \frac{161}{160} & 0
\end{pmatrix}.
\]
Here, each non-zero entry was computed using the ratio in~\eqref{eq:gamma} or, equivalently, by using the simplified formula derived in the proof of Proposition~\ref{prop:axioms}.  For example,
\[
  \gamma(v_2,v_3)
  = \frac{\Sigma_{2,2}\,\mathcal{C}^{(3)}_{2,3,3}}
         {\mathcal{C}^{(3)}_{2,2,3}\,\Sigma_{2,3}}
  = \frac{5 \cdot 18}{12\cdot 6} = \frac{5}{4},
  \quad
  \gamma(v_4,v_3)
  = \frac{\Sigma_{4,4}}{\lambda_{3,4}^2\,\Sigma_{3,3}}
  = \frac{161}{4^2\cdot 10} = \frac{161}{160}\approx 1.006.
\]

\paragraph{Interpretation.}
The matrix $\Gamma$ respects all four axioms of Definition~\ref{def:discrepancy_paper1}.  Zero entries arise whenever the first argument is an ancestor of the second; equal values appear when pairs share the same lowest common ancestor, and nested ancestry leads to increasing values, e.g.\ $\gamma(v_4,v_3)=161/160<\gamma(v_4,v_2)=161/144$ since $\mathrm{LCA}(v_4,v_3)=v_3$ lies below $\mathrm{LCA}(v_4,v_2)=v_1$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recovery of Latent Trees}
\label{sec:algorithm}

\subsection{Theoretical Foundation}

Before presenting the algorithmic framework, we establish the key theoretical concepts from \citet{etesami:kiyavash:coleman:2016} that underpin our approach.

\begin{definition}[Learnable subset {\citealp[Def.~8]{etesami:kiyavash:coleman:2016}}]
\label{def:learnable_subset}
In a polytree $\vec{T} = (V, \vec{E})$, we call a subset $L \subset V$ \emph{learnable} if every node $v \in L$ has at least two outgoing arrows. We call $O := V \setminus L$ the set of observed nodes.
\end{definition}

\begin{remark}
From Definition~\ref{def:learnable_subset}, if $L$ is a learnable subset of a polytree, then all the leaves belong to $O = V \setminus L$. This ensures that latent variables with insufficient connectivity (out-degree less than 2) cannot be distinguished from observed variables based solely on the discrepancy patterns.
\end{remark}

The following theorem provides the theoretical guarantee for the recovery algorithm:

\begin{theorem}[Structure identifiability {\citealp[Thm.~4]{etesami:kiyavash:coleman:2016}}]
\label{thm:identifiability}
Let $\vec{T} = (V, \vec{E})$ be a polytree with root set $R$, and let $L \subseteq V$ be a learnable subset. Then the existence of a discrepancy matrix $\Gamma_O$ for $O = V \setminus L$ suffices for learning $\vec{T}$.
\end{theorem}

\begin{proof}
See \citet[Appendix H]{etesami:kiyavash:coleman:2016}.
\end{proof}

The proof of Theorem~\ref{thm:identifiability} proceeds by mathematical induction on the size of the observed node set $O$ and provides the constructive framework for the three-phase algorithm presented below.

\begin{definition}[Tree merger {\citealp[Def.~9]{etesami:kiyavash:coleman:2016}}]
\label{def:tree_merger}
A \emph{tree merger} is an operator that takes two directed trees $\vec{T}_1$, $\vec{T}_2$ and a given subtree of both of them, say $\vec{T}_3$, and merges them at $\vec{T}_3$. We denote this operation by
\[
\vec{T}_1 \circ \vec{T}_2 \big|_{\vec{T}_3}.
\]
\end{definition}
Figure~\ref{fig:tree_merger} demontrates one such tree merger.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[node distance=1.5cm, every node/.style={circle,draw,minimum size=0.6cm}]
  % Tree T1
  \begin{scope}[xshift=-4.5cm]
    \node (v) at (0,1.5) {$v$};
    \node (v1) at (-0.8,0.5) {$v_1$};
    \node (v2) at (0.8,0.5) {$v_2$};

    \draw[->] (v) -- (v1);
    \draw[->] (v) -- (v2);

    \node[below=0.3cm,draw=none] at (0,0) {$\vec{T}_1$};
  \end{scope}

  % Comma 1
  \node[draw=none] at (-2.8,0.8) {,};

  % Tree T2
  \begin{scope}[xshift=-1.5cm]
    \node (u) at (0,1.5) {$u$};
    \node (u1) at (-0.8,0.5) {$v_1$};
    \node (u3) at (0.8,0.5) {$v_3$};

    \draw[->] (u) -- (u1);
    \draw[->] (u) -- (u3);

    \node[below=0.3cm,draw=none] at (0,0) {$\vec{T}_2$};
  \end{scope}

  % Comma 2
  \node[draw=none] at (0.2,0.8) {,};

  % Shared subtree T3
  \begin{scope}[xshift=1.5cm]
    \node (w) at (0,1.5) {$w$};
    \node (w1) at (0,0.5) {$v_1$};

    \draw[->] (w) -- (w1);

    \node[below=0.3cm,draw=none] at (0,0) {$\vec{T}_3$};
  \end{scope}

  % Equals sign
  \node[draw=none] at (3.5,1) {$=$};

  % Merged tree - star graph
  \begin{scope}[xshift=6cm]
    \node (h) at (0,1.5) {$h$};
    \node (v1m) at (0,0.5) {$v_1$};
    \node (v2m) at (-1.2,0.5) {$v_2$};
    \node (v3m) at (1.2,0.5) {$v_3$};

    \draw[->] (h) -- (v1m);
    \draw[->] (h) -- (v2m);
    \draw[->] (h) -- (v3m);

    \node[below=0.3cm,draw=none] at (0,0) {$\vec{T}_1 \circ \vec{T}_2 \big|_{\vec{T}_3}$};
  \end{scope}
\end{tikzpicture}
\caption{Illustration of the tree merger operation. Two directed trees $\vec{T}_1$ and $\vec{T}_2$ sharing a common subtree $\vec{T}_3$ (node $v_1$ with its parent) are merged to form a single polytree. The merger introduces a latent node $h$ that becomes the common root with direct connections to all observed nodes.}
\label{fig:tree_merger}
\end{figure}

\subsection{Structure Recovery Algorithm}

The rationale of our algorithmic approach follows the three main steps of the proof of Theorem~\ref{thm:identifiability}:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Root discovery}: Discover the number of roots $|R|$ of the underlying polytree and all their descendants in the set of observed nodes $O$ given the discrepancy matrix $\Gamma_O$. This can be accomplished by fixing a node $v \in O$ and finding a maximal subset of $O$ containing $v$ in which every pair of nodes has non-negative discrepancy.

\item \textbf{Subtree recovery}: Recover the underlying tree for each root $r \in R$, based on the descendants of $r$ that were discovered in $O$ during the first step.

\item \textbf{Tree merging}: Merge the trees recovered in the previous step to reconstruct the underlying polytree. When two recovered trees are connected, their combined subgraph forms a tree, which can be learned using the Tree algorithm.
\end{enumerate}

The correctness of this approach relies on the key insight that if a polytree $\vec{T}$ and a directed tree $\vec{T}_i$ have a non-empty intersection, their union is guaranteed to be a single-rooted tree, enabling recursive reconstruction via the tree merger operation.


\subsection{Algorithmic Recovery from Cumulant Discrepancy}

We adapt the recovery pipeline of \citet{etesami:kiyavash:coleman:2016} to our cumulant-based discrepancy measure. The three-stage method first partitions nodes into sibling groups, then orients edges within groups, and finally inserts latent nodes to recover a minimal latent polytree.

\vspace{1ex}

\begin{algorithm}[H]
\caption{Separation($\Gamma_O$)}
\label{alg:separation}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Sibling groups $O_1, \ldots, O_{|\mathcal{R}|}$
    \State $M \leftarrow \varnothing$, $i \leftarrow 1$
    \While{$O \setminus M \ne \varnothing$}
        \State Choose $v \in O \setminus M$
        \State Find all $C \subseteq O$ such that $v \in C$ and
        \Statex \hspace{\algorithmicindent} for all $(u,w) \in C \times C$, $\gamma(u,w) \ge 0$
        \State $O_i \leftarrow$ maximal such $C$
        \State \Return $O_i$
        \State $M \leftarrow M \cup O_i$
        \State $i \leftarrow i + 1$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Tree($\Gamma_O$)}
\label{alg:tree}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Directed tree $\vec{T} = (V, \vec{E})$
    \ForAll{$v \in O$}
        \State $B_v \leftarrow \argmin_{u \in O \setminus \{v\}} \gamma(v,u)$
    \EndFor
    \If{$B_v = O \setminus \{v\}$ for all $v \in O$}
        \If{$\exists w \in O$ such that $\min_{u \in O \setminus \{w\}} \gamma(w,u) = 0$}
            \State $\vec{T}$ is a star graph with $w$ as the root in the center
        \Else
            \State $\vec{T}$ is a star graph with a hidden node as the root in the center
        \EndIf
    \Else
        \State Choose $w$ such that $B_w \ne O \setminus \{w\}$
        \State $\vec{T}' \leftarrow \textsc{Tree}(B_w \cup \{w\})$
        \State $\vec{T}'' \leftarrow \textsc{Tree}(O \setminus B_w)$
        \State Substitute $w$ in $\vec{T}''$ by another node, say $h$
        \State $\vec{T} \leftarrow \vec{T}' \oplus \vec{T}''(h)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Polytree($\Gamma_O$)}
\label{alg:polytree}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Minimal latent polytree $\vec{T} = (V, \vec{E})$
    \State $\{O_1, \ldots, O_{|\mathcal{R}|}\} \leftarrow \textsc{Separation}(\Gamma_O)$
    \State $\vec{T} \leftarrow \textsc{Tree}(O_1)$
    \State $S \leftarrow O_1$, \quad $I \leftarrow \{1\}$
    \While{$I \ne \{1,2,\ldots,|\mathcal{R}|\}$}
        \State Find $i \in \{1,\ldots,|\mathcal{R}|\} \setminus I$ such that $O_i \cap S \ne \varnothing$
        \State $\vec{T}_{\text{sub}} \leftarrow \textsc{Tree}(S \cap O_i)$
        \State $\vec{T}_i \leftarrow \textsc{Tree}(O_i)$
        \State $\vec{T} \leftarrow \vec{T} \circ \vec{T}_i \vert_{\vec{T}_{\text{sub}}}$
        \State $S \leftarrow S \cup O_i$
        \State $I \leftarrow I \cup \{i\}$
    \EndWhile
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:polytree} presents the main algorithm for learning the polytree $\vec{T}(V, \vec{E})$ with the root set $R$ given the discrepancy matrix $\Gamma_O$ on its observed nodes $O$. First, it calls the subroutine Separation($\Gamma_O$), which finds subsets $O_i$s, where $O = \cup_i O_i$ such that each subset corresponds to observed nodes in a directed tree with a single root. Each of these single rooted subtrees can be learned by Algorithm~\ref{alg:tree}. To complete the task, Algorithm~\ref{alg:polytree} must connect these subtrees to recover the original polytree. This is done by using the fact that if a polytree $\vec{T}$ and a directed tree $\vec{T}_i$ have an intersection, their common subgraph is also a tree; thus, it can be learned using Algorithm~\ref{alg:tree}.

\paragraph{Algorithm Description.}
The \textsc{Separation} algorithm operates on a given discrepancy matrix $\Gamma_O$ of the observed nodes. The aim of this partition is to obtain the set of vertices $O$ into subsets $O_1, O_2, \ldots, O_{|R|}$, where $|R|$ is the total number of subsets corresponding to different roots. Each subset satisfies a specific compatibility condition $\gamma(u, w) \geq 0$ for all pairs within the subset.

The algorithm starts with an empty set $M$ to track the processed vertices, and a counter $i$ set to 1 to keep track of the number of subsets generated. In the main loop, as long as there are unprocessed vertices in $O$, the algorithm picks an arbitrary vertex $v$ from the unprocessed set $O \setminus M$. It then identifies all possible subsets $C \subseteq O$ such that $v \in C$ and for all $(u,w) \in C \times C$, we have $\gamma(u,w) \geq 0$. The maximal such subset becomes $O_i$, ensuring that nodes sharing a common root ancestor are grouped together.

The \textsc{Tree} algorithm provides a method for constructing a directed tree from a set of nodes using the discrepancy measure. Initially, the algorithm computes for each vertex $v \in O$ the set $B_v$ of best neighbors, defined as those vertices $u \in O \setminus \{v\}$ that minimize $\gamma(v,u)$. The algorithm then checks if the tree can be simplified to a star graph structure. If all nodes satisfy $B_v = O \setminus \{v\}$, it determines whether there exists a node $w \in O$ such that $\min_{u \in O \setminus \{w\}} \gamma(w,u) = 0$. If such a node exists, the tree is constructed as a star graph with $w$ as the root. Otherwise, the tree is built as a star graph with a hidden root.

If the star graph condition is not met, the algorithm proceeds with recursive tree construction. It selects a node $w$ such that $B_w \neq O \setminus \{w\}$ and recursively constructs subtrees. The process involves substituting nodes and merging trees using the tree merger operation to ensure that the hierarchical relationships between nodes are properly captured.


\subsection{Adaptation to Cumulant Discrepancy}

In our setting, we adapt these algorithms to work with our cumulant-based discrepancy measure from Definition~\ref{def:discrepancy}. The key modification lies in verifying that our measure satisfies the four axioms of Definition~\ref{def:discrepancy_paper1}, which we established through our computational examples.

The theoretical guarantees of Theorem~\ref{thm:identifiability} transfer directly to our cumulant setting, provided that:
\begin{enumerate}[label=(\roman*)]
\item The cumulant estimates are sufficiently accurate,
\item The non-Gaussianity assumption ensures identifiability of the structural equations,
\item The minimal latent polytree structure satisfies the learnability condition.
\end{enumerate}

Under these conditions, the Separation-Tree-Merger pipeline provides a consistent estimator of the underlying minimal latent polytree structure.

\begin{remark}
The sample complexity of our approach depends on the accuracy of second and third cumulant estimation. For sub-Gaussian distributions, consistent estimation requires $n \gg \max\{p, \log p\}$ samples, where $p$ is the number of observed variables.
\end{remark}


\subsection{Sample Complexity}
Consistent estimation of $C^{(2)}$ and $C^{(3)}$ requires $n\gg \max\{p,\log p\}$ samples under sub--Gaussian tails.  Concentration bounds (\citealp{vershynin:2018}; \citealp[Cor.~4.1]{tramontano:monod:drton:2022}) yield rates matching those in the fully observed case.  Detailed finite--sample analysis is deferred to future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

We evaluate our cumulant-based discrepancy approach through comprehensive experiments on synthetic polytree data. Our experimental framework tests the theoretical predictions in controlled settings using population-level discrepancy matrices, providing insights into the fundamental performance of our method before considering finite-sample effects.

\subsection{Random Polytree Generation via Prüfer Sequences}

\subsubsection{Theoretical Foundation of Prüfer Sequences}

Our experimental design relies on \emph{Prüfer sequences} \citep{prufer}, a fundamental combinatorial tool that establishes a bijection between labeled trees and integer sequences.

\begin{definition}[Prüfer sequence]
A Prüfer sequence for a labeled tree with $n$ vertices is a sequence of length $n-2$ that uniquely encodes the tree structure. The encoding algorithm iteratively removes the leaf with the smallest label and records the label of its unique neighbor, continuing until only two vertices remain.
\end{definition}

Prüfer sequences provide several critical advantages for causal inference experiments:

\begin{enumerate}[label=(\roman*)]
\item \textbf{Uniform sampling}: There exists a bijection between labeled trees on $n$ vertices and sequences of length $n-2$ over the alphabet $\{1,\ldots,n\}$. This enables uniform random sampling from the space of all $n^{n-2}$ possible tree structures.

\item \textbf{Guaranteed validity}: The decoding process always produces a connected, acyclic graph, ensuring that every generated structure is a valid polytree.

\item \textbf{Computational efficiency}: Both encoding and decoding algorithms operate in $O(n)$ time using appropriate data structures, making large-scale experiments feasible.

\item \textbf{Parameter control}: By constraining the choice of root during orientation, we can ensure the presence of latent variables with specified out-degrees.
\end{enumerate}

\subsubsection{Implementation Details}

Our random minimal latent polytree generation follows Algorithm~\ref{alg:pruefer_pipeline}:

\begin{algorithm}[H]
\caption{Prüfer-Based Population Random Polytree Generation}
\label{alg:pruefer_pipeline}
\begin{algorithmic}[1]
    \Require Number of nodes $n$, random seed
    \Ensure Minimal latent polytree with population discrepancy matrix
    \State Generate random Prüfer sequence $S = (s_1, \ldots, s_{n-2})$ with $s_i \in \{1,\ldots,n\}$
    \State Decode $S$ to undirected tree $T = (V, E_{\text{undir}})$ using heap-based algorithm
    \State Choose root $r$ with undirected degree $\geq 2$ to ensure latent nodes exist
    \State Orient edges via breadth-first search from $r$: $E_{\text{dir}} = \{(u,v) : u \text{ is parent of } v\}$
    \State Assign edge weights $\lambda_{uv} \sim \text{Uniform}[-1,1]$ with $|\lambda_{uv}| \geq \eta$
    \State Set noise parameters: $\sigma_i^2 = 1$, $\kappa_i = 1$ for all $i \in V$
    \State Identify latent nodes: $L = \{v \in V : |\{u : (v,u) \in E_{\text{dir}}\}| \geq 2\}$
    \State Set observed nodes: $O = V \setminus L$
    \State Compute population discrepancy matrix $\Gamma_O$ via Definition~\ref{def:discrepancy}
\end{algorithmic}
\end{algorithm}

The key innovation in our approach is the systematic identification of latent variables based on out-degree. Any node with out-degree $\geq 2$ is designated as latent, ensuring that the resulting structure satisfies the minimality condition of Definition~\ref{def:minimal_polytree}.


\subsection{Evaluation Metrics and Methodology}

\subsubsection{Performance Measures}

We assess structural recovery using precision and recall metrics adapted to the latent variable setting:

\begin{definition}[Latent-aware precision and recall]
Let $\mathcal{E}_{\text{true}}$ be the set of true latent-to-observed edges and $\mathcal{E}_{\text{pred}}$ be the predicted edges. Define:
\begin{align}
\text{Precision} &= \frac{|\mathcal{E}_{\text{pred}} \cap \mathcal{E}_{\text{true}}|}{|\mathcal{E}_{\text{pred}}|}, \\
\text{Recall} &= \frac{|\mathcal{E}_{\text{pred}} \cap \mathcal{E}_{\text{true}}|}{|\mathcal{E}_{\text{true}}|}.
\end{align}
\end{definition}

Since latent variables can be recovered with different names, we employ a bipartite matching algorithm that maximizes Jaccard similarity between the children sets of true and predicted latent nodes:

\begin{algorithm}[H]
\caption{Latent Node Matching}
\label{alg:latent_matching}
\begin{algorithmic}[1]
    \Require True latent children $\{C_t^{\text{true}}\}$, predicted latent children $\{C_r^{\text{pred}}\}$
    \Ensure Optimal matching between latent nodes
    \State Compute Jaccard similarities: $J(C_t, C_r) = |C_t \cap C_r| / |C_t \cup C_r|$ for all pairs
    \State Sort all pairs $(t, r)$ by Jaccard score in descending order
    \State Greedily assign matches: for each pair in sorted order, if neither $t$ nor $r$ is already matched and $J(C_t, C_r) \geq 0.5$, create match $(t, r)$
    \State Return matched pairs
\end{algorithmic}
\end{algorithm}

\subsubsection{Experimental Protocol}

Our evaluation consists of the following steps:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Batch generation}: Generate $K = 20$ independent random latent polytrees using different seeds
\item \textbf{Structure recovery}: Apply our algorithm to each observed population discrepancy matrix
\item \textbf{Latent matching}: Match predicted latent nodes to ground truth using Algorithm~\ref{alg:latent_matching}
\item \textbf{Metric computation}: Calculate precision, recall, and F1-score for latent-to-observed edges
\item \textbf{Statistical analysis}: Report mean, standard deviation, and distribution statistics
\end{enumerate}



\subsection{Population-Level Experimental Design}

\subsubsection{Parameter Configuration}

Our experiments in this section operate in the \textbf{population regime}, examining the theoretical performance of our discrepancy-based algorithms without finite-sample noise. This approach allows us to isolate algorithmic performance from estimation uncertainty and focus on the fundamental scalability characteristics of the method.

\textbf{Core experimental parameters:}
\begin{itemize}
\item \textbf{Graph sizes}: $n \in \{30, 50, 100, 150, 200, 250, 300\}$ total nodes for systematic analysis, with large-scale validation extending to $n = 1500$
\item \textbf{Edge weights}: $\lambda_{ij} \sim \text{Uniform}[-1,1]$ subject to minimum threshold constraint $|\lambda_{ij}| \geq \eta$, where $\eta \in \{0.1, 0.3, 0.5, 0.8\}$
\item \textbf{Noise parameters}: Unit variance ($\sigma_i^2 = 1$) and unit third-order cumulants ($\kappa_i = 1$) for all nodes
\item \textbf{Latent identification}: Nodes with out-degree $\geq 2$ are designated as latent variables. For the population experiments, we designate \emph{all} candidate nodes (those with out-degree $\geq 2$) as latent, thereby creating the maximal latent configuration. This choice represents the most challenging scenario for structure recovery, as it maximizes the proportion of hidden variables while maintaining the minimality constraint. The theoretical upper bound for latent nodes in a minimal latent polytree with $n$ total nodes is approximately $\lfloor (n-1)/2 \rfloor$, derived from the requirement that each latent node must have at least two children and the tree must remain connected. Our maximal latent configuration therefore provides a conservative assessment of algorithmic performance under the most demanding structural conditions.
\end{itemize}

This configuration enables systematic investigation of the relationship between edge weight magnitudes and algorithmic performance, which forms the core contribution of our experimental analysis.

\subsubsection{Simplified Population Ground Truth}

Rather than computing complex population moments via multi-trek rules, our experimental framework employs a \textbf{simplified population approach} designed specifically for algorithmic validation:

\textbf{Ground truth construction process:}
\begin{enumerate}[label=(\arabic*)]
\item \textbf{Polytree generation}: Use Prüfer sequences to generate random minimal latent polytrees with specified parameters
\item \textbf{Parameter assignment}: Set unit variance and cumulant parameters ($\sigma_i^2 = 1$, $\kappa_i = 1$) for all nodes
\item \textbf{Direct discrepancy computation}: Apply Definition~\ref{def:discrepancy} directly to the structural parameters without intermediate moment calculations
\item \textbf{Population evaluation}: Test structure recovery algorithms on the resulting population discrepancy matrices
\end{enumerate}

This approach \textbf{isolates algorithmic performance} from moment estimation challenges, allowing us to focus on the fundamental question: \emph{Given perfect knowledge of the discrepancy measure, how well can the structure recovery algorithms perform?}

\textbf{Rationale for simplification}: The unit parameter assumption ensures that:
\begin{itemize}
\item All numerical variations arise from structural relationships rather than parameter heterogeneity
\item The discrepancy ratios reflect purely topological patterns
\item Computational focus remains on the structure learning algorithms rather than moment estimation
\end{itemize}

\subsubsection{Experimental Scope and Objectives}

Our experimental investigation addresses two primary research questions:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Algorithmic correctness}: Do the adapted algorithms from \citet{etesami:kiyavash:coleman:2016} correctly recover latent polytree structures when applied to our cumulant-based discrepancy measure?

\item \textbf{Scalability characteristics}: What factors determine the practical scalability limits of the approach, and how do parameter choices affect performance at different scales?
\end{enumerate}

The \textbf{Critical Edge Weight Threshold Phenomenon} (Section~\ref{subsec:threshold_phenomenon}) provides the definitive answer to both questions, revealing that numerical conditioning rather than algorithmic limitations determines scalability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Critical Edge Weight Threshold Phenomenon}
\label{subsec:threshold_phenomenon}

Our comprehensive experiments reveal a fundamental relationship between edge weight magnitudes and algorithmic performance that has not been previously documented in the polytree learning literature. This phenomenon represents a key practical constraint that bridges the gap between theoretical guarantees and computational implementation.

\subsubsection{Parameter Sensitivity Analysis}

We systematically investigate how the minimum absolute edge weight threshold $\eta$ affects structure recovery performance. Edge weights are parameterized as $\lambda_{ij} \sim \text{Uniform}[-1,1]$ subject to the constraint $|\lambda_{ij}| \geq \eta$ for threshold values $\eta \in \{0.1, 0.3, 0.5, 0.8\}$.

Our experiments span polytree sizes from $n = 30$ to $n = 300$ nodes, using the correct evaluation methodology that focuses on latent-to-observed edge recovery with Jaccard-based latent node matching. Each configuration is evaluated over 20 independently generated minimal latent polytrees to ensure statistical reliability.

\paragraph{Critical threshold discovery.} The results, depicted in Figure~\ref{fig:threshold_phenomenon}, reveal a dramatic performance stratification based on the minimum edge weight threshold:

\begin{itemize}
\item \textbf{$\eta = 0.1$ (weak threshold)}: Performance exhibits catastrophic degradation starting around $n = 100$ nodes, with F1 scores dropping from $\approx 1.0$ to $\approx 0.15$ by $n = 300$. This breakdown follows an approximately exponential decay pattern.

\item \textbf{$\eta = 0.3$ (moderate threshold)}: Shows improved stability with gradual performance decline. F1 scores remain above 0.8 until $n = 250$, demonstrating significantly better resilience than the weak threshold case.

\item \textbf{$\eta = 0.5$ and $\eta = 0.8$ (strong thresholds)}: Maintain excellent performance ($F_1 \approx 1.0$) across all tested scales, with minimal degradation even at $n = 300$ nodes.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{threshold_analysis.pdf}
\caption{Critical edge weight threshold phenomenon. The algorithm exhibits dramatically different scalability behavior depending on the minimum absolute edge weight $\eta$. Weak thresholds ($\eta = 0.1$) lead to performance collapse, while strong thresholds ($\eta \geq 0.5$) maintain excellent recovery across all tested scales. Error bars show standard deviations across 10 trials.}
\label{fig:threshold_phenomenon}
\end{figure}

\subsubsection{Numerical Conditioning Analysis}

The threshold phenomenon can be understood through the numerical conditioning of the cumulant discrepancy computation. Recall from Definition~\ref{def:discrepancy} that our measure involves the ratio:
\begin{equation}
\gamma(i,j) = \frac{\mathcal{C}^{(3)}_{i,j,j} \, \Sigma_{i,i}}{\mathcal{C}^{(3)}_{i,i,j} \, \Sigma_{i,j}}
\end{equation}

In polytree structures, both numerator and denominator terms contain products of edge weights along directed paths. When $|\lambda_{ij}| \to 0$ for any edge on these paths, the corresponding cumulant and covariance terms approach zero at potentially different rates, causing numerical instabilities in the ratio computation.


\paragraph{Condition number evidence.} To quantify this effect, we analyzed the condition numbers of discrepancy matrices across different threshold values.

\begin{definition}[Matrix condition number]
For a matrix $A \in \mathbb{R}^{m \times n}$, the condition number is defined as
\begin{equation}
\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)},
\end{equation}
where $\sigma_{\max}(A)$ and $\sigma_{\min}(A)$ are the largest and smallest singular values of $A$, respectively. For square matrices, this reduces to $\kappa(A) = \|A\| \|A^{-1}\|$ in any consistent matrix norm. A matrix is considered ill-conditioned when $\kappa(A) \gg 1$, indicating that small perturbations in the input can lead to large changes in the output.
\end{definition}

Our systematic analysis on polytrees with $n = 100$ nodes reveals dramatic differences in numerical conditioning:

\begin{itemize}
\item $\eta = 0.1$: Condition numbers reach $1.25 \times 10^{23}$ with rank deficiency (66/73 rank), indicating severe ill-conditioning that makes reliable computation impossible. Dynamic ranges exceed $10^8$, reflecting extreme value disparities in the discrepancy matrix.

\item $\eta = 0.3$: Condition numbers improve to $1.12 \times 10^9$ with full rank recovery (73/73), representing a $10^{14}$-fold improvement. Despite this substantial improvement, the dynamic range of $3.53 \times 10^8$ still indicates potential numerical challenges.

\item $\eta = 0.5$ and $\eta = 0.8$: Condition numbers further decrease to $5.78 \times 10^5$ and $3.93 \times 10^3$ respectively, both maintaining full rank. The dynamic range stabilizes below $4 \times 10^5$, ensuring robust numerical computation.
\end{itemize}

Significantly, perfect structure recovery ($F_1 = 1.0$) is achieved only when condition numbers remain below $10^6$, establishing this as a practical threshold for reliable computation. This numerical analysis confirms that the performance degradation is fundamentally linked to matrix conditioning rather than algorithmic limitations.

The relationship between minimum edge weights and matrix conditioning can be understood through the discrepancy computation mechanism. When $|\lambda_{ij}| < 0.3$, products of edge weights along directed paths approach zero faster than individual terms, leading to near-singular denominator matrices in the ratio computation. This creates the observed rank deficiency and numerical instability.

Furthermore, our analysis reveals that computational breakdown occurs precisely when eigenvalue ratios exceed $10^{10}$, indicating extreme spectral conditioning. The transition from $\eta = 0.1$ (infinite eigenvalue ratio due to near-zero minimum eigenvalues) to $\eta = 0.8$ (finite, well-conditioned eigenvalue spectrum) demonstrates the critical nature of the minimum weight threshold for ensuring algorithmic reliability.

\subsubsection{Large-Scale Validation and Runtime Analysis}

Beyond the systematic threshold analysis up to $n = 300$, we conducted large-scale validation experiments to establish the true scalability limits of the method under optimal parameterization.

\paragraph{Extreme-scale validation.} Under strong threshold conditions ($\eta = 0.8$), we successfully scaled the algorithm to unprecedented sizes:

\begin{table}[h]
\centering
\begin{tabular}{cccc}
\toprule
Nodes ($n$) & Edge Threshold ($\eta$) & F1-Score & Runtime (seconds) \\
\midrule
1000 & 0.7 & $0.99 \pm 0.01$ & $\approx 1800$ \\
1500 & 0.8 & $1.000 \pm 0.000$ & $3769.6$ \\
\bottomrule
\end{tabular}
\caption{Large-scale validation results demonstrating perfect recovery at unprecedented scales.}
\label{tab:large_scale}
\end{table}

These results represent the largest successful latent polytree structure learning experiments reported in the literature to date. The perfect F1 scores confirm that the method maintains theoretical guarantees even at scales orders of magnitude larger than previous demonstrations.

\paragraph{Runtime characteristics.} The computational complexity is dominated by the $O(p^2 \log p)$ cost of computing the Chow-Liu tree using Kruskal's algorithm \citep{cormen:introduction}, where $p$ is the number of observed nodes. The subsequent structure learning algorithms (Separation, Tree, and Polytree) have worst-case complexity $O(p^2)$ for star-like structures, but typically exhibit better performance on balanced polytrees. Overall, the method scales polynomially with the number of nodes, making it practically feasible for large-scale applications. For $n = 1500$ nodes, the total runtime of approximately 63 minutes demonstrates practical feasibility for large-scale structure learning.

\paragraph{Scale-dependent breakdown under weak thresholds.} In contrast, weak threshold conditions ($\eta = 0.1$) exhibit clear breakdown points around $n = 100-120$ nodes, where F1 scores drop below 0.5. This breakdown is characterized by:
\begin{itemize}
\item Rapid precision degradation (from 1.0 to $\approx 0.1$)
\item Stable recall maintenance ($\approx 0.95$ across all scales)
\item Increasing variance in performance across trials
\end{itemize}

The dramatic contrast between failure at $n \approx 100$ under weak thresholds and perfect recovery at $n = 1500$ under strong thresholds conclusively demonstrates that numerical conditioning, rather than algorithmic limitations, has been the primary scalability barrier.

\subsubsection{Practical Guidelines}

These findings establish concrete guidelines for practitioners applying cumulant-based polytree learning:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Edge weight regularization}: In real applications, estimated structural coefficients below $|\lambda_{ij}| < 0.5$ should be subject to regularization or robust estimation techniques.

\item \textbf{Condition number monitoring}: Discrepancy matrix condition numbers provide early warning signals for numerical instability. Condition numbers exceeding $10^{12}$ indicate potential breakdown.

\item \textbf{Scale-appropriate thresholds}: For large-scale applications ($n > 100$), minimum edge weight thresholds should be set to $\eta \geq 0.5$ to ensure stable performance.
\end{enumerate}

\paragraph{Theoretical implications.} The threshold phenomenon reveals an important gap between theoretical identifiability guarantees and computational implementation. While our cumulant discrepancy measure is mathematically well-defined for any non-zero edge weights, practical computation requires careful attention to numerical conditioning that is not captured in existing theoretical frameworks.

\subsubsection{Comparison with Related Work}

This work provides the first systematic documentation of numerical conditioning effects in discrepancy-based latent polytree learning. Previous theoretical analyses \citep{etesami:kiyavash:coleman:2016} focus on asymptotic consistency without addressing finite-precision arithmetic constraints. Our empirical findings complement theoretical guarantees by establishing practical parameter ranges for reliable computation.

The threshold phenomenon also connects to broader numerical stability issues in higher-order moment estimation \citep{comon:jutten:handbook}, where small denominators in ratio-based statistics can lead to computational breakdown. However, the specific manifestation in polytree structure learning—where performance remains excellent under appropriate parameterization—has not been previously characterized.

\subsubsection{Novel Methodological Contribution}

The discovery of the critical edge weight threshold represents a novel methodological contribution with immediate practical value. The clear stratification of performance across threshold values provides actionable guidance for parameter selection, transforming a method that appeared to have limited scalability (under default weak thresholds) into an approach capable of handling large-scale problems when properly configured.

The transition from failure at $n \approx 100$ under weak thresholds to excellent performance at $n = 1500$ under strong thresholds demonstrates that numerical conditioning, rather than algorithmic limitations, has been the primary barrier to large-scale latent polytree learning with cumulant-based methods.

\subsubsection{Future Research Directions}

The threshold phenomenon opens several avenues for future investigation:

\begin{itemize}
\item \textbf{Adaptive thresholds}: Development of data-driven threshold selection methods based on condition number monitoring
\item \textbf{Regularization techniques}: Investigation of alternative regularization approaches for handling near-zero edge weights
\item \textbf{Alternative discrepancy formulations}: Exploration of numerically stable variants of the cumulant discrepancy ratio
\item \textbf{Finite-sample analysis}: Extension to sample-based cumulant estimation with threshold-adaptive confidence intervals
\end{itemize}

These findings establish a foundation for developing more robust cumulant-based structure learning methods that can reliably scale to large polytree systems while maintaining theoretical guarantees.

\subsection{Finite-Sample Validation Experiments}
\label{subsec:finite_sample}

Having established the correctness and scalability of our approach in the population regime, we now validate the method under realistic finite-sample conditions. These experiments bridge the gap between theoretical guarantees and practical implementation by introducing estimation uncertainty through finite-sample moment computation.

\subsubsection{Experimental Progression}

Our finite-sample validation follows a systematic progression that mirrors the population-level experimental design:

\paragraph{Phase 1: Validation on known example.} We begin with the four-node polytree from Section~\ref{sec:discrepancy} to establish baseline performance and validate our finite-sample implementation against known analytical results. This provides a controlled environment where theoretical predictions can be directly verified.

\paragraph{Phase 2: Extension to random polytrees.} Following successful validation on the known example, we extend the finite-sample analysis to randomly generated polytrees using the same Prüfer sequence methodology established in the population experiments. This progression enables assessment of finite-sample robustness across diverse structural configurations while maintaining the strong edge weight thresholds ($\eta \geq 0.8$) that ensure numerical stability.

The systematic progression from known analytical cases to random structures provides comprehensive validation of our finite-sample methodology while building directly on the insights from both the theoretical analysis and population-level experiments.

\paragraph{Test polytree configuration.} We begin our finite-sample validation with the same four-node polytree structure introduced in the example of Section~\ref{sec:discrepancy}, but with modified parameters for enhanced numerical stability:
\begin{itemize}
\item \textbf{Structure}: 4-node polytree with edges $\{(v_1, v_2), (v_1, v_3), (v_3, v_4)\}$ as depicted in Figure~\ref{fig:example-polytree}
\item \textbf{Edge weights}: Strong coefficients $\lambda_{1,2} = -0.95$, $\lambda_{1,3} = -0.95$, $\lambda_{3,4} = 0.95$ (compared to $\{2, 3, 4\}$ in the theoretical example)
\item \textbf{Node configuration}: Following the minimal latent polytree definition, we set $v_1$ as a latent variable (out-degree = 2), while $v_2, v_3, v_4$ serve as observed variables. Only the observed discrepancy matrix is provided to the structure recovery algorithm, as required by the theoretical framework.
\end{itemize}

This configuration leverages our established theoretical understanding while using edge weights that satisfy the strong threshold condition ($|\lambda_{ij}| \geq 0.8$) identified in the population experiments. The choice ensures robust numerical conditioning while maintaining the interpretable structure from our theoretical analysis.

\paragraph{Gamma noise specification.} To ensure realistic non-Gaussian conditions while maintaining comparability with our theoretical analysis, we employ heterogeneous Gamma-distributed noise terms:
\begin{align}
\varepsilon_{v_1} &\sim \Gamma(3.0, 1.2), \quad \varepsilon_{v_2} \sim \Gamma(2.5, 0.8), \\
\varepsilon_{v_3} &\sim \Gamma(2.8, 1.0), \quad \varepsilon_{v_4} \sim \Gamma(3.5, 0.9)
\end{align}
where $\Gamma(\alpha, \beta)$ denotes the Gamma distribution with shape parameter $\alpha$ and scale parameter $\beta$. This parameterization ensures:
\begin{itemize}
\item Sufficient asymmetry for third-order cumulant identifiability
\item Moderate heterogeneity across nodes without extreme outliers
\item Realistic noise characteristics commonly encountered in empirical applications
\item Compatibility with our theoretical framework while introducing realistic estimation challenges
\end{itemize}

\subsubsection{Data Generation Pipeline}

Our experimental pipeline follows a systematic four-step process that mirrors realistic causal discovery scenarios:

\paragraph{Step 1: Centered noise generation.} For each node $i \in \{1,2,3,4\}$, we generate $n = 150{,}000$ i.i.d. samples from the specified Gamma distribution and apply analytic centering:
\begin{equation}
\tilde{\varepsilon}_i^{(t)} = \varepsilon_i^{(t)} - \mathbb{E}[\varepsilon_i] = \varepsilon_i^{(t)} - \alpha_i \beta_i
\end{equation}
where $\alpha_i, \beta_i$ are the shape and scale parameters for node $i$. The large sample size ($n = 150{,}000$) ensures high-precision moment estimation while remaining computationally feasible.

\paragraph{Step 2: Linear structural equation model (LSEM) transformation.} Given the edge weight matrix $\Lambda$ encoding the polytree structure, we apply the standard LSEM transformation:
\begin{equation}
\mathbf{X} = (I - \Lambda)^{-1} \tilde{\boldsymbol{\varepsilon}}
\end{equation}
where $\Lambda_{ji} = \lambda_{ij}$ for each directed edge $i \to j$. This generates the observed data matrix $\mathbf{X} \in \mathbb{R}^{n \times 4}$ with the desired causal dependencies.

\paragraph{Step 3: Finite-sample moment estimation.} From the generated samples $\mathbf{X}$, we compute empirical estimates of the required second and third-order moments:
\begin{align}
\hat{\Sigma}_{ij} &= \frac{1}{n} \sum_{t=1}^n (X_i^{(t)} - \bar{X}_i)(X_j^{(t)} - \bar{X}_j), \\
\hat{\mathcal{C}}^{(3)}_{i,i,j} &= \frac{1}{n} \sum_{t=1}^n (X_i^{(t)} - \bar{X}_i)^2 (X_j^{(t)} - \bar{X}_j), \\
\hat{\mathcal{C}}^{(3)}_{i,j,j} &= \frac{1}{n} \sum_{t=1}^n (X_i^{(t)} - \bar{X}_i) (X_j^{(t)} - \bar{X}_j)^2
\end{align}
These empirical moments serve as inputs to our finite-sample discrepancy computation, introducing realistic estimation uncertainty.

\paragraph{Step 4: Finite-sample discrepancy matrix computation.} We apply our robust discrepancy estimation algorithm (Algorithm~\ref{alg:finite_sample_discrepancy}) to the empirical moments, yielding the finite-sample discrepancy matrix $\hat{\Gamma}$.

\subsubsection{Population Benchmark Computation}

To enable precise comparison, we compute the corresponding population discrepancy matrix $\Gamma^*$ using the known structural parameters and analytic Gamma distribution moments:

\paragraph{Population moment computation.} For Gamma distributions $\Gamma(\alpha, \beta)$, the population moments are:
\begin{align}
\sigma^2 &= \alpha \beta^2 \quad \text{(variance)}, \\
\kappa^{(3)} &= 2\alpha \beta^3 \quad \text{(third-order cumulant)}
\end{align}

\paragraph{Population discrepancy evaluation.} Using these analytic moments and the known edge weights, we compute the population discrepancy matrix $\Gamma^*$ via Definition~\ref{def:discrepancy}, providing the ground truth benchmark for comparison.

\subsubsection{Performance Metrics and Analysis}

Our evaluation focuses on two key performance dimensions:

\paragraph{Approximation accuracy.} We measure the \textbf{maximum absolute difference} between finite-sample and population discrepancy matrices:
\begin{equation}
\Delta_{\max} = \max_{i,j} |\hat{\Gamma}_{ij} - \Gamma^*_{ij}|
\end{equation}
This metric quantifies the estimation precision achieved under finite-sample conditions.

\paragraph{Structural pattern preservation.} We verify that the finite-sample discrepancy matrix preserves the key structural patterns required for accurate structure recovery:
\begin{enumerate}[label=(\roman*)]
\item \textbf{Latent node signature}: Row corresponding to latent variable $v_1$ should exhibit near-zero entries in the observed-only discrepancy submatrix
\item \textbf{Sibling consistency}: Observed nodes $v_2, v_3$ with common latent parent $v_1$ should display consistent discrepancy patterns
\item \textbf{Path length sensitivity}: Discrepancy values should correctly reflect graph-theoretic distances between observed nodes
\item \textbf{Structure recovery}: The finite-sample observed discrepancy matrix should yield correct recovery of the observed polytree structure $(v_3 \to v_4)$
\end{enumerate}

\subsubsection{Implementation Details}

Our finite-sample implementation incorporates several numerical stability enhancements:

\begin{itemize}
\item \textbf{Adaptive thresholds}: Tolerance parameters scale with $n^{-1/2}$ to account for estimation uncertainty
\item \textbf{Correlation-based filtering}: Fisher's uncorrelatedness test with Bonferroni correction identifies genuinely independent node pairs
\item \textbf{Robust ratio computation}: Small denominator detection prevents numerical instabilities in discrepancy ratio calculation
\item \textbf{Topological ordering}: Edge weight assignments respect DAG constraints to ensure valid causal interpretations
\end{itemize}

\subsubsection{Expected Outcomes and Validation Criteria}

Success criteria for the finite-sample validation include:
\begin{enumerate}[label=(\arabic*)]
\item $\Delta_{\max} < 0.01$ (high approximation accuracy)
\item Perfect preservation of structural zero patterns
\item Identical structure recovery results between finite-sample and population cases
\item Robust performance across multiple random seeds
\end{enumerate}

These experiments serve as a crucial bridge between our population-level theoretical analysis and the practical applicability of our method to real-world causal discovery problems.

\subsubsection{Comprehensive Convergence Analysis Results}

We conducted comprehensive finite-sample validation experiments across sample sizes ranging from $n = 100$ to $n = 10{,}000{,}000$, with 20 independent trials per sample size to ensure statistical reliability. The results demonstrate robust convergence behavior across all evaluated metrics.

\paragraph{Convergence analysis.} Figure~\ref{fig:finite_sample_convergence} presents the convergence analysis for the four-node polytree example. All three performance metrics exhibit clear convergence patterns:
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{finite_sample_convergence.png}
\caption{Finite-sample convergence analysis for the four-node polytree example. All metrics exhibit clear $n^{-1/2}$ convergence behavior with variance estimation errors (top left), third cumulant estimation errors (top right), and discrepancy matrix errors (bottom left) decreasing systematically with sample size. The convergence rate analysis (bottom right) confirms optimal statistical efficiency. Error bars represent standard deviations across 20 independent trials.}
\label{fig:finite_sample_convergence}
\end{figure}


\begin{enumerate}[label=(\roman*)]
\item \textbf{Variance estimation error}: Decreases from $0.67 \pm 0.14$ at $n = 1{,}000$ to $0.003 \pm 0.001$ at $n = 10{,}000{,}000$, representing a 200-fold improvement
\item \textbf{Third cumulant estimation error}: Reduces from $4.58 \pm 1.67$ to $0.018 \pm 0.008$, achieving a 250-fold error reduction
\item \textbf{Discrepancy matrix error}: Exhibits the most dramatic improvement, decreasing from $2.49 \pm 0.56$ to $0.003 \pm 0.001$, representing an 800-fold reduction in maximum absolute error
\end{enumerate}

\paragraph{Theoretical convergence validation.} The convergence rate analysis (Figure~\ref{fig:finite_sample_convergence}, bottom right) reveals that the observed error reduction closely follows the theoretical $n^{-1/2}$ convergence rate expected for moment estimation. The efficiency ratio of observed-to-theoretical improvement approaches unity for large sample sizes, confirming that our finite-sample implementation achieves optimal statistical efficiency.

\paragraph{Numerical stability assessment.} The consistently small standard deviations across trials (particularly evident at large sample sizes) demonstrate the numerical stability of our discrepancy computation algorithm. At $n = 10{,}000{,}000$, the coefficient of variation for discrepancy errors is approximately 30\%, indicating reliable performance across different random realizations.

\paragraph{Practical implications.} The results establish clear sample size guidelines for practical applications:
\begin{itemize}
\item For high-precision applications requiring discrepancy errors below 0.01, sample sizes of $n \geq 1{,}000{,}000$ are recommended
\item For moderate-precision exploratory analysis, $n = 100{,}000$ provides discrepancy errors around 0.6, which may be sufficient for structure recovery
\item The dramatic error reduction between $n = 100{,}000$ and $n = 1{,}000{,}000$ (50-fold improvement) suggests this range as a critical transition region
\end{itemize}

\paragraph{Validation of theoretical framework.} The convergence behavior validates our theoretical framework linking moment estimation accuracy to discrepancy matrix precision. The fact that all three metrics converge at similar rates confirms that the bottleneck in finite-sample performance is the statistical estimation of second and third-order moments, rather than the algorithmic computation of discrepancy ratios.

These results provide strong empirical validation that our cumulant-based discrepancy approach maintains its theoretical guarantees under realistic finite-sample conditions, with error rates that decrease predictably according to standard statistical theory. The finite-sample validation establishes a solid foundation for extending the methodology to larger, randomly generated polytree structures using the Prüfer sequence framework.

\subsubsection{Structure Recovery Performance Analysis}

Beyond moment estimation accuracy, we evaluated the finite-sample structure recovery performance using our adapted Separation-Tree-Merger algorithm. The structure recovery analysis provides crucial insights into the practical applicability of our method for real-world causal discovery tasks.

\paragraph{Success rate progression.} Figure~\ref{fig:finite_sample_structure_recovery} presents comprehensive structure recovery results across the full range of sample sizes. The structure recovery success rate shows a clear transition pattern with three distinct phases:

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{finite_sample_structure_recovery.png}
\caption{Structure recovery performance analysis for the four-node polytree example. The left panel shows structure recovery success rates across sample sizes, with clear transition from poor performance at small samples to perfect recovery at large samples. The right panel demonstrates the inverse relationship between discrepancy matrix errors and recovery success rates.}
\label{fig:finite_sample_structure_recovery}
\end{figure}

\begin{itemize}
\item \textbf{Low sample sizes} ($n \leq 1{,}000$): Success rates between 15-35\%, indicating insufficient statistical power for reliable structure recovery. The high variance in performance across trials reflects the dominance of sampling noise over structural signal.

\item \textbf{Moderate sample sizes} ($n = 10{,}000$ to $100{,}000$): Rapid improvement from 30\% to 70\% success rate, demonstrating the critical transition region where moment estimation accuracy becomes sufficient for structural inference.

\item \textbf{Large sample sizes} ($n \geq 1{,}000{,}000$): Very good recovery (90-100\% success rate) achieved consistently across all trials, confirming the asymptotic consistency of our approach.
\end{itemize}

\paragraph{Critical transition point.} The analysis reveals that reliable structure recovery ($\geq 90\%$ success rate) requires approximately $n \geq 1{,}000{,}000$ samples for this four-node configuration. This establishes a practical sample size recommendation for high-precision applications requiring guaranteed structure recovery.

The sharp transition observed between $n = 500{,}000$ (70\% success) and $n = 10{,}000{,}000$ (100\% success) demonstrates the existence of a critical threshold where moment estimation precision becomes sufficient to overcome the numerical challenges inherent in discrepancy-based structure learning.

\paragraph{Relationship to discrepancy accuracy.} The combined analysis (Figure~\ref{fig:finite_sample_structure_recovery}, right panel) demonstrates a clear inverse relationship between discrepancy matrix errors and structure recovery success rates. The dual-axis visualization reveals several key insights:

\begin{enumerate}[label=(\arabic*)]
\item When discrepancy errors exceed 1.0, structure recovery performance remains poor ($<40\%$ success rate)
\item The transition region ($0.1 \leq \text{error} \leq 1.0$) corresponds to rapidly improving but still unreliable recovery
\item Once discrepancy errors drop below approximately 0.1, structure recovery reliability increases dramatically, confirming the importance of accurate moment estimation for successful structure learning
\item Perfect recovery is achieved only when discrepancy errors fall below 0.01, establishing this as the practical precision threshold
\end{enumerate}

\paragraph{Variance analysis across trials.} The error bars in Figure~\ref{fig:finite_sample_structure_recovery} reveal important patterns in the reliability of structure recovery:

\begin{itemize}
\item At small sample sizes, high variance in success rates indicates that performance is dominated by random sampling effects
\item In the transition region, decreasing variance reflects the emergence of consistent structural signal over noise
\item At large sample sizes, zero variance confirms deterministic perfect recovery, validating the theoretical consistency guarantees
\end{itemize}

\paragraph{Methodological validation.} The structure recovery results provide strong validation of our overall methodological approach:

\begin{itemize}
\item The clear relationship between moment estimation accuracy and structure recovery success confirms that our cumulant-based discrepancy measure correctly captures the structural information needed for minimal latent polytree learning
\item The existence of a sharp transition threshold demonstrates that the method exhibits predictable scaling behavior rather than gradual degradation
\item The achievement of perfect recovery at large sample sizes validates the theoretical guarantees established in our population-level analysis
\end{itemize}

\subsubsection{Comprehensive Practical Guidelines}

The finite-sample validation establishes clear, evidence-based guidelines for practitioners applying cumulant-based polytree learning in real-world scenarios:

\paragraph{Sample size recommendations.} Based on the comprehensive convergence and structure recovery analysis:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{High-precision applications}: For applications requiring discrepancy errors below 0.01 and guaranteed perfect structure recovery, sample sizes of $n \geq 1{,}000{,}000$ are strongly recommended. This threshold ensures both numerical precision and structural reliability.

\item \textbf{Moderate-precision exploratory analysis}: Sample sizes of $n = 100{,}000$ provide discrepancy errors around 0.6 and structure recovery success rates around 40\%, which may be sufficient for initial structure exploration and hypothesis generation in early-stage research.

\item \textbf{Critical transition region}: The range $n = 100{,}000$ to $n = 1{,}000{,}000$ represents a critical transition where dramatic error reduction occurs (50-fold improvement in discrepancy precision). This region should be targeted for applications requiring a balance between computational cost and structural reliability.

\item \textbf{Minimal viability threshold}: Below $n = 10{,}000$, the method exhibits poor and unreliable performance, suggesting this as a practical lower bound for meaningful application.
\end{enumerate}

\paragraph{Quality assessment criteria.} Practitioners can use the following indicators to assess the reliability of their finite-sample results:

\begin{itemize}
\item \textbf{Discrepancy precision monitoring}: Maximum absolute discrepancy errors should be below 0.1 for reliable structure recovery
\item \textbf{Moment estimation quality}: Variance estimation errors below 0.1 and third cumulant errors below 0.5 indicate sufficient precision for structural inference
\item \textbf{Cross-validation stability}: Results should be consistent across multiple random subsamples of the data
\end{itemize}

\paragraph{Computational considerations.} The finite-sample validation provides guidance for computational resource allocation:

\begin{itemize}
\item Memory requirements scale linearly with sample size, making large-sample analysis computationally feasible
\item The dramatic improvement in precision between $n = 100{,}000$ and $n = 1{,}000{,}000$ suggests that investing in larger sample sizes provides excellent returns in terms of structural reliability
\item For applications where perfect recovery is not essential, the moderate sample size range offers a reasonable compromise between computational cost and performance
\end{itemize}

\subsection{Extension to Random Polytrees}
\label{subsec:random_polytree_extension}

Following successful validation on the known four-node example, we extend the finite-sample analysis to randomly generated polytrees using the Prüfer sequence framework. This extension proceeds in two phases: first, we establish baseline performance on unstructured random polytrees without topological constraints (Section~\ref{subsubsec:unstructured_random}), providing a general characterization of algorithm performance across diverse structures. Subsequently, we investigate how specific structural topologies systematically affect recovery difficulty (Section~\ref{subsubsec:topology_framework}), revealing the fundamental determinants of finite-sample requirements.

\subsubsection{Unstructured Random Polytrees: Baseline Characterization}
\label{subsubsec:unstructured_random}

We begin with the most general experimental setting: randomly generated polytrees with minimal structural constraints. This approach mirrors the four-node validation methodology but scales to larger systems, providing baseline performance characterization across varying polytree sizes and sample sizes without imposing topological restrictions.

\paragraph{Experimental methodology.}

Our experimental methodology operates at two complementary levels to enable comprehensive performance analysis:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Pure finite-sample evaluation (primary approach)}: For assessing structure recovery under realistic conditions, we sample non-Gaussian noise from gamma distributions, apply the LSEM transformation to generate observed data, estimate second and third cumulants from samples, construct the finite-sample observed discrepancy matrix $\hat{\Gamma}_O$, and apply the Separation-Tree-Merger algorithm. This represents the complete pipeline a practitioner would follow, requiring no knowledge of the true population parameters beyond the non-Gaussianity assumption.

\item \textbf{Population-referenced evaluation (for diagnostic analysis)}: To decompose performance and separately quantify moment estimation error versus algorithmic recovery capability, we additionally compute the population discrepancy matrix $\Gamma_O$ using the true structural parameters. This enables measurement of discrepancy error $\|\hat{\Gamma}_O - \Gamma_O\|$ and attribution of recovery failures to either (i) insufficient moment estimation precision or (ii) fundamental algorithmic limitations given the topology.
\end{enumerate}

The population-referenced approach provides crucial diagnostic information but requires knowledge of ground truth parameters. The pure finite-sample approach, by contrast, mirrors real-world application where only the non-Gaussianity assumption is available.

\paragraph{Noise generation protocol.} For all experiments, we generate centered non-Gaussian noise from gamma distributions with heterogeneous parameters:

\begin{enumerate}[label=(\roman*)]
\item \textbf{Shape parameter sampling}: For each node $i$, draw shape parameter $k_i \sim \text{Uniform}[1.2, 9.0]$, ensuring sufficient non-Gaussianity while avoiding extreme tail behavior

\item \textbf{Scale parameter standardization}: Set scale parameter $\theta_i = 1/\sqrt{k_i}$ to standardize variance to unity, yielding $\text{Var}(\varepsilon_i) = k_i \theta_i^2 = 1$

\item \textbf{Third cumulant determination}: The standardization yields third cumulants $\kappa_i = 2k_i\theta_i^3 = 2/\sqrt{k_i}$, providing heterogeneous asymmetry across nodes with $\kappa_i \in [0.67, 1.83]$

\item \textbf{Centering}: Generate raw samples from $\Gamma(k_i, \theta_i)$ and center by subtracting the population mean $\mu_i = k_i\theta_i$, ensuring zero-mean noise: $\tilde{\varepsilon}_i = \varepsilon_i - \mu_i$
\end{enumerate}

This protocol ensures that all nodes have unit variance but distinct third-order characteristics, satisfying the non-Gaussianity requirement for identifiability while maintaining numerical stability.

\paragraph{Data generation and moment estimation.} The complete experimental pipeline proceeds as follows:

\begin{enumerate}[label=\textbf{Step \arabic*:}]
\item \textbf{Structural parameter generation}: Generate random minimal latent polytree with $n$ nodes using Prüfer sequences, assign edge weights $\lambda_{ij} \sim \text{Uniform}[-1,1]$ with $|\lambda_{ij}| \geq \eta$, and sample gamma noise parameters $(k_i, \theta_i)$ as described above

\item \textbf{Population reference computation (optional)}: If performing population-referenced evaluation, compute the population observed discrepancy matrix $\Gamma_O$ using the true parameters $((\lambda_{ij}), (k_i, \theta_i))$ via Definition~\ref{def:discrepancy}, restricted to observed nodes

\item \textbf{Efficient finite-sample data generation}: For computational efficiency and consistency across sample sizes, we employ a nested sampling strategy. Generate $n_{\max} = \max(\mathcal{S})$ centered noise samples $\tilde{\varepsilon}_i^{(t)} \sim \Gamma(k_i, \theta_i) - k_i\theta_i$ for $t = 1, \ldots, n_{\max}$, where $\mathcal{S}$ is the set of sample sizes to be evaluated. Apply the LSEM transformation $\mathbf{X}_{\max} = (I - \Lambda)^{-1}\tilde{\boldsymbol{\varepsilon}}$ to obtain the maximum sample size dataset. For each smaller sample size $n_s \in \mathcal{S}$ with $n_s < n_{\max}$, extract the first $n_s$ samples: $\mathbf{X}_{n_s} = \mathbf{X}_{\max}[1:n_s, :]$. This ensures that smaller datasets are proper subsets of larger ones, enabling consistent comparison across sample sizes while avoiding redundant data generation and ensuring that performance differences are attributable solely to sample size rather than sampling variability.

\item \textbf{Moment estimation}: For each sample size $n_s \in \mathcal{S}$, compute sample covariance matrix $\hat{\Sigma} = \frac{1}{n_s}\mathbf{X}_{n_s}^\top\mathbf{X}_{n_s}$ and sample third cumulants $\hat{\mathcal{C}}^{(3)}_{i,j,k}$ from the centered data

\item \textbf{Finite-sample discrepancy construction}: Construct finite-sample observed discrepancy matrix $\hat{\Gamma}_O$ using the ratio-based formula from Definition~\ref{def:discrepancy}, applied to the estimated moments, restricted to observed nodes

\item \textbf{Structure recovery}: Apply the Separation-Tree-Merger algorithm to $\hat{\Gamma}_O$ to recover the polytree structure

\item \textbf{Performance evaluation}: Compute structure recovery metrics (precision, recall, F1-score, perfect recovery rate). If population reference is available, additionally compute discrepancy error $\max_{i,j \in O}|\hat{\Gamma}_O(i,j) - \Gamma_O(i,j)|$
\end{enumerate}

\paragraph{Experimental configuration.} For the unstructured random polytree baseline:

\begin{itemize}
\item \textbf{Polytree sizes}: $n \in \{4, 5, 6, 7, 8, 9, 10\}$ total nodes
\item \textbf{Latent structure}: Exactly $k = 1$ latent node per polytree, randomly selected from candidate nodes with out-degree $\geq 2$. This constraint simplifies interpretation by isolating the effects of system size and observed topology on recovery difficulty, while still capturing the essential challenges of latent variable learning.
\item \textbf{Sample sizes}: $n_{\text{samples}} \in \{10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 2 \times 10^7\}$ to characterize high-precision regime
\item \textbf{Edge weight threshold}: $\eta = 0.8$ for strong numerical conditioning
\item \textbf{Trials per configuration}: $N = 10$ independent replications for statistical reliability
\end{itemize}
This configuration provides a comprehensive baseline characterization of algorithm performance on general polytree structures before introducing topology-specific constraints.

\paragraph{Rationale for nested sampling strategy.} The nested sampling approach (Step 3) provides several critical advantages:

\begin{itemize}
\item \textbf{Consistency}: By ensuring smaller datasets are subsets of larger ones, performance differences across sample sizes reflect only estimation precision, not sampling variability
\item \textbf{Computational efficiency}: Generating data once at the maximum sample size eliminates redundant noise generation and LSEM transformations
\item \textbf{Fair comparison}: The nested structure enables direct assessment of how additional samples improve performance on the identical underlying random polytree realization
\end{itemize}

This methodology, inherited from best practices in finite-sample convergence analysis, ensures that our empirical results provide clean characterization of sample size requirements unconfounded by trial-to-trial structural variation.

\paragraph{Expected outcomes.} Based on the four-node validation results and preliminary experiments, we anticipate:

\begin{itemize}
\item High F1-scores ($> 0.9$) for small polytrees ($n \leq 6$) at sample sizes $n_{\text{samples}} \geq 10^7$
\item Graceful degradation of performance as polytree size increases, requiring larger samples to maintain recovery quality
\item Clear relationship between discrepancy error and structure recovery success, enabling prediction of sample requirements for specific accuracy targets
\end{itemize}

\paragraph{Experimental results and convergence analysis.}

We conducted comprehensive finite-sample experiments on randomly generated polytrees with sizes $n \in \{4,5,6,7,8,9,10\}$ across sample sizes ranging from $10^2$ to $2 \times 10^7$. For each configuration, we generated 10 independent random polytrees using Prüfer sequences, employed the nested sampling strategy described above, and evaluated performance across all sample sizes. Figure~\ref{fig:random_polytree_convergence} presents the aggregated convergence results across all three key metrics: discrepancy error, structure recovery F1-score, and empirical validation of the theoretical $O(n^{-1/2})$ convergence rate.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{random_polytree_analysis.png}
  \caption{Comprehensive convergence analysis for unstructured random polytrees. \textbf{Left:} Maximum discrepancy error $\|\hat{\Gamma}_O - \Gamma_O\|_{\max}$ converges to zero with increasing sample size, with smaller polytrees achieving faster convergence. \textbf{Middle:} Structure recovery F1-scores demonstrate clear improvement with sample size, reaching near-perfect recovery ($> 0.9$) for smaller polytrees at $n_{\text{samples}} \geq 10^7$. \textbf{Right:} Discrepancy errors exhibit empirical $O(n^{-1/2})$ convergence rates (dashed reference lines), confirming the theoretical finite-sample behavior predicted by the central limit theorem for moment estimators.}
  \label{fig:random_polytree_convergence}
\end{figure*}

\subparagraph{Discrepancy error convergence (left panel).} The maximum discrepancy error $\max_{i,j \in O} |\hat{\Gamma}_O(i,j) - \Gamma_O(i,j)|$ exhibits systematic convergence patterns that validate our finite-sample methodology. Across all polytree sizes, we observe monotonic improvement with increasing sample size, confirming that moment estimation error constitutes the primary bottleneck in the moderate sample size regime.

For small polytrees ($n=4$), discrepancy errors decrease from $12.95 \pm 10.16$ at $n_{\text{samples}} = 100$ to $0.14 \pm 0.10$ at $n_{\text{samples}} = 2 \times 10^7$, representing an approximately 90-fold improvement. Medium-sized polytrees ($n=6$) achieve similar convergence patterns, reducing errors from $23.47 \pm 29.75$ to values below $1.0$ at the largest sample sizes. For larger polytrees ($n=10$), even at maximum sample sizes, discrepancy errors remain elevated ($1.70 \pm 2.28$), indicating that topological complexity introduces fundamental statistical challenges beyond mere system size.

\subparagraph{Structure recovery performance (middle panel).} The F1-score analysis reveals clear size-stratified performance with sharp transitions between failure and success regimes. Table~\ref{tab:random_polytree_summary} presents detailed performance metrics across all configurations.

\begin{table}[h]
\centering
\caption{Structure recovery F1-scores for random polytrees, averaged over 10 independent trials per configuration. Standard deviations reflect variability across different randomly generated polytree topologies.}
\label{tab:random_polytree_summary}
\small
\begin{tabular}{ccc|ccc}
\toprule
\multicolumn{3}{c}{Small Polytrees} & \multicolumn{3}{c}{Large Polytrees} \\
\midrule
$n$ & $n_{\text{samples}}$ & F1 Score & $n$ & $n_{\text{samples}}$ & F1 Score \\
\midrule
\multirow{3}{*}{4} & $10^6$ & $0.93 \pm 0.13$ & \multirow{3}{*}{7} & $10^6$ & $0.37 \pm 0.26$ \\
 & $10^7$ & $0.97 \pm 0.10$ &  & $10^7$ & $0.58 \pm 0.27$ \\
 & $2 \times 10^7$ & $0.97 \pm 0.10$ &  & $2 \times 10^7$ & $0.60 \pm 0.29$ \\
\midrule
\multirow{3}{*}{5} & $10^6$ & $0.64 \pm 0.33$ & \multirow{3}{*}{8} & $10^6$ & $0.49 \pm 0.24$ \\
 & $10^7$ & $0.89 \pm 0.21$ &  & $10^7$ & $0.49 \pm 0.21$ \\
 & $2 \times 10^7$ & $0.94 \pm 0.17$ &  & $2 \times 10^7$ & $0.53 \pm 0.27$ \\
\midrule
\multirow{3}{*}{6} & $10^6$ & $0.79 \pm 0.30$ & \multirow{3}{*}{9} & $10^6$ & $0.36 \pm 0.17$ \\
 & $10^7$ & $0.79 \pm 0.30$ &  & $10^7$ & $0.46 \pm 0.17$ \\
 & $2 \times 10^7$ & $0.85 \pm 0.24$ &  & $2 \times 10^7$ & $0.46 \pm 0.15$ \\
\bottomrule
\end{tabular}
\end{table}

Several critical patterns emerge from this analysis:

\begin{itemize}
\item \textbf{Sharp performance transitions}: For $n \leq 5$, a clear transition occurs between $10^6$ and $10^7$ samples. The F1-score for $n=4$ improves modestly from $0.93 \pm 0.13$ to $0.97 \pm 0.10$, while $n=5$ exhibits a dramatic jump from $0.64 \pm 0.33$ to $0.89 \pm 0.21$. This identifies $10^7$ as the critical sample size threshold for reliable structure recovery in small polytree systems.

\item \textbf{Size-dependent sample requirements}: Medium polytrees ($n=6$) achieve F1 $= 0.85 \pm 0.24$ at $2 \times 10^7$ samples, indicating that each additional node increases sample requirements by approximately one order of magnitude to maintain comparable performance levels.

\item \textbf{Performance ceiling for large polytrees}: For $n \geq 7$, even maximum evaluated sample sizes fail to achieve high F1-scores ($\leq 0.6$). Notably, polytrees with $n=8$ show minimal improvement between $10^6$ and $10^7$ samples (F1 $= 0.49$ for both), suggesting that fundamental algorithmic or topological barriers, rather than mere statistical precision, limit performance in this regime.

\item \textbf{High variance in intermediate regimes}: The large standard deviations for medium polytrees at moderate sample sizes (e.g., $n=5$ at $10^6$ samples: F1 $= 0.64 \pm 0.33$) indicate substantial heterogeneity in recovery difficulty across different random topologies, even at fixed system size.
\end{itemize}

\subparagraph{Theoretical convergence rate validation (right panel).} The comparison between empirical discrepancy errors and the theoretical $O(n_{\text{samples}}^{-1/2})$ convergence rate (dashed reference lines) provides crucial validation of our finite-sample statistical theory.

Across all polytree sizes, the log-log plot reveals that empirical error trajectories closely parallel the $n^{-1/2}$ reference lines, confirming that sample cumulant estimators follow the expected central limit theorem behavior. The vertical offset between curves for different polytree sizes reflects the constant factors in the asymptotic variance bounds provided by concentration inequalities for sample cumulants under log-concave distributions \citep[Cor.~4.1]{tramontano:monod:drton:2022}. Larger polytrees exhibit systematically higher variance due to: (i) increased path complexity accumulating trek-based variance contributions, and (ii) a combinatorially larger number of discrepancy matrix entries to estimate, each subject to independent sampling variability.

Critically, we observe no anomalous deviations from the theoretical rate across five orders of magnitude in sample size ($10^2$ to $2 \times 10^7$), indicating that our ratio-based discrepancy construction avoids pathological numerical instabilities and inherits the favorable concentration properties of polynomial moment estimators.

\paragraph{Failure mode analysis and diagnostic insights.}

The detailed trial-by-trial diagnostics reveal consistent patterns in structure recovery failures that inform both practical application and theoretical understanding. Analysis of imperfect recovery cases across all experiments identifies a recurring failure signature:

\begin{itemize}
\item \textbf{Structural zero violations}: In essentially all failures examined, finite-sample discrepancy matrices exhibit spurious non-zero entries $\hat{\Gamma}_O(i,j) \gg 0$ where the population matrix satisfies $\Gamma_O(i,j) = 0$. For example, in Trial 1 of $n=4$ experiments at $n_{\text{samples}} = 10^6$, the entry $\hat{\Gamma}_O(\text{v4}, \text{v3}) = 0.994$ compared to the true value $\Gamma_O(\text{v4}, \text{v3}) = 0$, directly causing the algorithm to miss the critical edge $(\text{v4}, \text{v3})$.

\item \textbf{Systematic misrecovery patterns}: When structural zeros are corrupted by sampling noise, the Separation-Tree-Merger algorithm systematically produces characteristic errors: missing edges from the true structure and spurious edges connecting nodes through the latent root. In the example above, the missing edge $(\text{v4}, \text{v3})$ is replaced by an incorrect direct connection $(\text{h1}, \text{v3})$ from the latent node.

\item \textbf{Error magnitude thresholds}: Across all successful recoveries ($n \leq 6$, $n_{\text{samples}} \geq 10^6$), maximum discrepancy errors remain below approximately $0.5$. Conversely, failures consistently exhibit errors exceeding $1.0$, suggesting a practical diagnostic criterion: $\|\hat{\Gamma}_O - \Gamma_O\|_{\max} < 0.5$ reliably correlates with successful structure recovery, though this requires knowledge of the population matrix for validation.
\end{itemize}

These failure patterns confirm that \emph{precise preservation of structural zeros}, rather than overall matrix approximation accuracy, constitutes the critical requirement for successful polytree learning.

\paragraph{Implications for topology-stratified analysis.}

The observed performance degradation for $n \geq 7$, even at maximum evaluated sample sizes ($2 \times 10^7$), cannot be fully explained by statistical convergence rates alone. Several lines of evidence suggest that topological structure fundamentally determines recovery difficulty:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Variance heterogeneity across polytree sizes}: While all sizes exhibit the same asymptotic $O(n^{-1/2})$ convergence rate, the constant factors differ dramatically. The vertical spread in the right panel of Figure~\ref{fig:random_polytree_convergence} reveals that larger polytrees incur inherently higher variance at any fixed sample size, but this alone cannot explain the complete stagnation observed for $n=8$ where F1-scores remain at $0.49$ across an order of magnitude increase in sample size.

\item \textbf{Structural zero sensitivity depends on graph topology}: The critical failure mode---corruption of structural zeros---is fundamentally determined by the polytree's path structure. Certain topologies (such as star configurations where multiple observed nodes share a single latent parent) create nearly indistinguishable discrepancy patterns that remain difficult to resolve even under high statistical precision.

\item \textbf{Performance variance suggests topology-dependent difficulty}: The large standard deviations in F1-scores for intermediate sizes (e.g., $n=6$ at $10^7$ samples: F1 $= 0.79 \pm 0.30$) indicate that different randomly generated polytrees within each size class have fundamentally different recovery difficulty profiles. This suggests that specific structural properties (beyond node count) determine whether recovery succeeds or fails.

\item \textbf{Latent node selection may introduce additional variability}: In our experiments, the single latent node per polytree is randomly selected from candidates with out-degree $\geq 2$. This selection strategy may interact with polytree topology in complex ways---for instance, selecting a latent node at the center of a long chain versus at a branch point could yield different discrepancy patterns and recovery difficulty. Alternative approaches for identifying latent structure, such as the rank constraint methods of \citet{cai:etal:2024:rank}, may offer complementary strategies for handling such variability.
\end{enumerate}

These observations motivate the topology-stratified framework developed in Section~\ref{subsubsec:topology_framework}, where we systematically investigate how specific structural patterns (chains, balanced trees, stars) determine finite-sample requirements independent of overall system size.


\subsubsection{Structural Topology Framework: Difficulty Stratification}
\label{subsubsec:topology_framework}

A critical insight from preliminary experimentation on unstructured random polytrees is that \emph{polytree topology fundamentally determines recovery difficulty}. Rather than treating all random polytrees as equivalent, we now investigate how specific structural patterns systematically affect finite-sample requirements. This topology-stratified analysis provides practitioners with actionable guidance for estimating data requirements based on their domain's likely causal structure.

\paragraph{Topology-driven difficulty hypothesis.} Our experimental observations suggest a clear hierarchy of recovery difficulty:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Chain structures} (easiest): Linear directed paths create monotonic discrepancy orderings that remain distinguishable even under moderate finite-sample noise

\item \textbf{Balanced branching structures} (intermediate): Multiple latent nodes with distributed out-degrees provide rich information but introduce ambiguity requiring more precise moment estimates

\item \textbf{Star structures} (hardest): Symmetric arrangements with a single high-degree latent root create nearly indistinguishable discrepancy patterns, demanding extreme moment estimation precision
\end{enumerate}

Figure~\ref{fig:topology_examples} illustrates these three canonical topologies. Within each topological class, we systematically vary the proportion of latent nodes to assess how latent configuration affects recovery performance under finite-sample conditions.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    observed/.style={circle,draw,minimum size=0.6cm},
    latent/.style={circle,draw,minimum size=0.6cm,fill=gray!20}]

  % Chain structure
  \begin{scope}[xshift=-5cm]
    \node[latent] (h1) at (0,1.5) {$h_1$};
    \node[observed] (v1) at (-1.2,0.5) {$v_1$};
    \node[observed] (v2) at (1.2,0.5) {$v_2$};
    \node[observed] (v3) at (1.2,-0.7) {$v_3$};
    \node[observed] (v4) at (1.2,-1.9) {$v_4$};
    \node[observed,draw=none] (vdots) at (1.2,-2.8) {$\vdots$};

    \draw[->] (h1) -- (v1);
    \draw[->] (h1) -- (v2);
    \draw[->] (v2) -- (v3);
    \draw[->] (v3) -- (v4);
    \draw[->] (v4) -- (vdots);

    \node[below=1.2cm,draw=none] at (0,-2.8) {\textbf{(a) Chain}};
  \end{scope}

  % Balanced branching structure
  \begin{scope}[xshift=0cm]
    \node[latent] (h1) at (0,2.5) {$h_1$};
    \node[observed] (v1) at (-1.5,1.3) {$v_1$};
    \node[latent] (h2) at (0,1.3) {$h_2$};
    \node[observed] (v2) at (1.5,1.3) {$v_2$};
    \node[observed] (v3) at (-0.7,0.1) {$v_3$};
    \node[observed] (v4) at (0.7,0.1) {$v_4$};

    \draw[->] (h1) -- (v1);
    \draw[->] (h1) -- (h2);
    \draw[->] (h1) -- (v2);
    \draw[->] (h2) -- (v3);
    \draw[->] (h2) -- (v4);

    \node[below=0.4cm,draw=none] at (0,-0.3) {\textbf{(b) Balanced}};
  \end{scope}

  % Star structure - more star-like arrangement
  \begin{scope}[xshift=5.5cm]
    \node[latent] (h) at (0,0) {$h_1$};
    \node[observed] (v1) at (0,1.5) {$v_1$};
    \node[observed] (v2) at (1.3,0.75) {$v_2$};
    \node[observed] (v3) at (1.3,-0.75) {$v_3$};
    \node[observed] (v4) at (0,-1.5) {$v_4$};
    \node[observed] (v5) at (-1.3,-0.75) {$v_5$};
    \node[observed] (v6) at (-1.3,0.75) {$v_6$};

    \draw[->] (h) -- (v1);
    \draw[->] (h) -- (v2);
    \draw[->] (h) -- (v3);
    \draw[->] (h) -- (v4);
    \draw[->] (h) -- (v5);
    \draw[->] (h) -- (v6);

    \node[below=0.4cm,draw=none] at (0,-1.9) {\textbf{(c) Star}};
  \end{scope}
\end{tikzpicture}
\caption{Canonical polytree topologies ordered by recovery difficulty. (a) \textbf{Chain structure}: Natural extension of the four-node validation example, featuring a latent root $h_1$ with one immediate child $v_1$ and one continuing chain $v_2 \to v_3 \to v_4 \to \cdots$. Easiest to recover due to clear monotonic discrepancy patterns. (b) \textbf{Balanced branching structure}: Multiple latent nodes ($h_1$, $h_2$) with moderate out-degrees create hierarchical dependencies. Intermediate recovery difficulty. (c) \textbf{Star structure}: Single latent root with six observed children arranged radially, exhibiting symmetric discrepancy patterns. Hardest to recover due to limited discriminative information. Latent nodes are shaded gray; observed nodes are white.}
\label{fig:topology_examples}
\end{figure}

\subsubsection{Chain Structures: Extension of the Four-Node Example}

Chain polytrees naturally extend the validated four-node example and represent the easiest recovery scenario. The canonical chain configuration features a single latent root $h_1$ with two children, where one branch terminates immediately at an observed leaf $v_1$ while the other continues as a directed path: $v_1 \leftarrow h_1 \to v_2 \to v_3 \to v_4 \to \cdots$.

\paragraph{Structural characteristics.} Chain structures exhibit predominantly degree-one or degree-two nodes forming linear paths. The directed paths create clear hierarchical relationships that manifest as monotonic orderings in the discrepancy matrix. For any two observed nodes $v_i, v_j$ on the main chain with $i < j$, the discrepancy ratio $\gamma(v_j, v_i)$ increases predictably with path distance, providing strong discriminative signals for structure recovery.

\paragraph{Recovery advantage.} The monotonic discrepancy patterns along chains enable reliable structure identification even under moderate sample sizes. Finite-sample noise affects all discrepancy entries similarly, but the clear ordering relationships are preserved, allowing the Separation-Tree-Merger algorithm to correctly orient edges and identify the latent root position.

\paragraph{Experimental design for chains.} Chain polytree experiments follow this protocol:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Chain generation}: Generate random polytrees with $n \in \{6, 8, 10, 15, 20\}$ total nodes, constraining the Prüfer sequence to favor linear structures with minimal branching

\item \textbf{Latent proportion variation}: For each chain size, vary the number of latent nodes $k \in \{1, 2, 3\}$, selecting from nodes with out-degree $\geq 2$, subject to $k \leq \lfloor (n-1)/2 \rfloor$

\item \textbf{Parameter assignment and data generation}: Apply the standardized protocol described in Section~\ref{subsec:random_polytree_extension} for edge weights, noise parameters, and sample generation

\item \textbf{Sample size progression}: For each configuration, generate data with $n_{\text{samples}} \in \{10^4, 10^5, 10^6, 10^7\}$ to characterize convergence behavior

\item \textbf{Performance evaluation}: Compute precision, recall, F1-score, perfect recovery rate, and (for population-referenced analysis) discrepancy errors across $N = 20$ independent trials per configuration
\end{enumerate}

\paragraph{Expected performance.} Based on the four-node validation results, chain structures with $n = 10$ nodes and $k = 2$ latent variables should achieve reliable recovery (F1 $> 0.9$) at sample sizes $n_{\text{samples}} \geq 10^6$. Longer chains may require proportionally larger samples to maintain comparable precision, scaling approximately as $O(\sqrt{n_{\text{nodes}}})$ due to increased moment estimation requirements.


\subsubsection{Balanced Branching Structures: Hierarchical Dependencies}

Balanced branching polytrees feature multiple latent nodes with moderate out-degrees (typically 2--3) distributed across the hierarchy, representing intermediate recovery difficulty. These structures create richer dependency patterns than chains while avoiding the extreme symmetry of star configurations.

\paragraph{Structural characteristics.} A canonical balanced structure with $n = 10$ nodes might have two or three latent nodes, each with 2--3 children, forming a shallow hierarchy. For example: latent root $h_1$ with children $\{v_1, h_2, v_2\}$, where secondary latent node $h_2$ further branches to $\{v_3, v_4\}$. This creates multiple "zones" in the discrepancy matrix corresponding to different latent subtrees.

\paragraph{Recovery challenges and advantages.} The hierarchical structure provides richer information than chains—multiple latent nodes create diverse discrepancy patterns that help distinguish structural features. However, the increased complexity introduces ambiguity: observed nodes may have similar discrepancy relationships to multiple latent ancestors, requiring more precise moment estimates to resolve correctly. The balance between information richness and structural complexity places these polytrees at intermediate difficulty.

\paragraph{Experimental design for balanced structures.} Balanced polytree experiments use controlled generation:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Balanced generation}: Generate polytrees with $n \in \{8, 10, 15, 20\}$ nodes by constraining Prüfer sequences to produce trees where no single node has out-degree exceeding 4, ensuring distributed branching

\item \textbf{Latent proportion variation}: Vary $k \in \{2, 3, 4, 5\}$ latent nodes (where feasible given $k \leq \lfloor (n-1)/2 \rfloor$), selecting from candidates with out-degree $\geq 2$

\item \textbf{Parameter assignment and data generation}: Apply the standardized protocol described in Section~\ref{subsec:random_polytree_extension} for edge weights, noise parameters, and sample generation

\item \textbf{Structural balance verification}: After generation, verify that no single latent node has more than 60\% of total latent-to-observed edges, ensuring distributed branching
\end{enumerate}

\paragraph{Expected performance.} Balanced structures with $n = 10$ nodes and $k = 3$ latent variables likely require sample sizes $n_{\text{samples}} \in [10^6, 5 \times 10^6]$ to achieve F1-scores comparable to chain structures at $n_{\text{samples}} = 10^6$. The increased structural complexity demands better moment estimation precision to resolve hierarchical relationships correctly.


\subsubsection{Star Structures: The Algorithmic Frontier}

Star polytrees, featuring a single latent root with many observed children arranged radially (Figure~\ref{fig:topology_examples}c), represent the most challenging recovery scenario. The extreme symmetry of star structures places maximal demands on the algorithm's ability to distinguish subtle differences in discrepancy ratios.

\paragraph{Structural characteristics.} A star with latent root $h_1$ and observed children $\{v_1, \ldots, v_m\}$ exhibits perfect symmetry: all observed nodes have identical structural relationships to the latent root. The discrepancy matrix reflects this symmetry—for all pairs $(v_i, v_j)$ with $i \neq j$, the discrepancy values $\gamma(v_i, v_j)$ are equal, providing minimal discriminative information for structure recovery.

\paragraph{Recovery challenges.} The symmetric discrepancy patterns create fundamental identification difficulties:

\begin{itemize}
\item \textbf{Limited discriminative power}: All observed nodes exhibit nearly identical discrepancy relationships, making it difficult for the algorithm to distinguish the true star structure from alternative configurations

\item \textbf{Sensitivity to estimation error}: Small finite-sample deviations from perfect symmetry can mislead the algorithm into inferring spurious hierarchical relationships among observed nodes

\item \textbf{Increased moment precision requirements}: Resolving the correct star structure requires extremely accurate estimation of second and third cumulants to detect the subtle signatures of the shared latent parent
\end{itemize}

\paragraph{Experimental design for star structures.} Star polytree experiments employ generation constraints and extended sample sizes:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Star generation}: Generate polytrees with $n \in \{6, 8, 10, 15\}$ nodes by selecting roots with maximum feasible degree and minimal secondary branching. For pure stars, the single latent root has $n-1$ observed children.

\item \textbf{Latent proportion}: Focus on $k = 1$ (pure star) and $k = 2$ (star with one secondary latent node) to isolate the effects of extreme symmetry

\item \textbf{Extended sample sizes}: Use $n_{\text{samples}} \in \{10^5, 10^6, 5 \times 10^6, 10^7, 5 \times 10^7\}$ to characterize the substantially larger samples needed for reliable recovery

\item \textbf{Parameter assignment and data generation}: Apply the standardized protocol described in Section~\ref{subsec:random_polytree_extension} for edge weights, noise parameters, and sample generation
\end{enumerate}

\paragraph{Expected performance.} Preliminary analysis suggests star structures require sample sizes one to two orders of magnitude larger than chain structures for comparable recovery performance. A star with $n = 10$ nodes ($k = 1$ latent, $m = 9$ observed children) may require $n_{\text{samples}} \geq 10^7$ to achieve F1-scores exceeding 0.8, compared to $n_{\text{samples}} \approx 10^6$ for equivalent chain structures. This dramatic increase reflects the fundamental information-theoretic challenge posed by symmetric structures.

\paragraph{Implications for practice.} Star structures represent the worst-case scenario for cumulant-based polytree learning. Real-world causal systems rarely exhibit perfect star symmetry, but systems with highly central latent variables (e.g., a single unobserved confounder affecting many observed variables) may approach star-like behavior. Understanding performance on star structures provides conservative bounds for sample size requirements in practical applications.


\subsection{Unified Experimental Framework and Future Directions}

\paragraph{Configuration space for systematic exploration.} The topology-stratified framework enables comprehensive characterization:

\begin{itemize}
\item \textbf{Polytree sizes}: $n \in \{6, 8, 10, 15, 20\}$ total nodes across all topologies
\item \textbf{Structural topologies}: Chain (easiest), balanced branching (intermediate), star (hardest)
\item \textbf{Latent proportions}: Within each topology, $k/n \in \{0.1, 0.2, 0.3, 0.4\}$ where feasible
\item \textbf{Sample sizes}: Topology-dependent ranges from $10^4$ (chains) to $5 \times 10^7$ (stars)
\item \textbf{Edge weight thresholds}: $\eta \in \{0.5, 0.8, 0.9\}$ to assess numerical conditioning effects
\end{itemize}

\paragraph{Scaling predictions.} The observed $n^{-1/2}$ convergence for moment estimation, combined with topology-specific difficulty levels, enables informed experimental design. For a target F1-score of 0.9:

\begin{itemize}
\item \textbf{Chain structures}: $n_{\text{samples}} \approx 10^6 \cdot (n_{\text{nodes}}/10)$
\item \textbf{Balanced structures}: $n_{\text{samples}} \approx 3 \times 10^6 \cdot (n_{\text{nodes}}/10)$
\item \textbf{Star structures}: $n_{\text{samples}} \approx 10^7 \cdot (n_{\text{nodes}}/10)^{1.5}$
\end{itemize}

These scaling predictions, derived from the four-node validation and preliminary experiments, provide actionable guidelines for practitioners assessing data requirements for real-world applications.

\paragraph{Robustness assessment.} The topology-stratified framework provides a template for assessing robustness to various modeling assumptions:

\begin{itemize}
\item Alternative noise distributions (exponential, Laplace) beyond gamma
\item Heterogeneous edge weight configurations with varying ranges
\item Violation of minimality through redundant latent nodes
\item Partial Gaussianity where some noise terms are Gaussian
\end{itemize}

These comprehensive validation results establish our cumulant-based discrepancy approach as a practical method for latent polytree structure learning, with clear performance characteristics stratified by structural topology. The progression from chain structures (natural extensions of the validated four-node example) to star structures (the algorithmic frontier) provides rigorous finite-sample characterization across the full spectrum of polytree configurations.

\subsection{Implementation and Reproducibility}

Our experimental framework is implemented in Python with the following key components:

\begin{itemize}
\item \textbf{Prüfer sequence generation}: \texttt{random\_polytrees\_pruefer.py} implements uniform random tree sampling
\item \textbf{Discrepancy computation}: \texttt{polytree\_discrepancy.py} computes population discrepancy matrices
\item \textbf{Structure recovery}: \texttt{latent\_polytree\_truepoly.py} implements the three-phase algorithm
\item \textbf{Evaluation pipeline}: \texttt{eval\_runner\_pruefer.py} orchestrates batch experiments and metric computation
\end{itemize}

All experiments use fixed random seeds to ensure reproducibility, and the codebase is available in the \texttt{causalLatentPolytree} repository.


\section{Conclusion and Outlook}
\label{sec:conclusion}

% TODO: Summarise findings, limitations, and prospective extensions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}

\end{document}
