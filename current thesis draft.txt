%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis Draft -- TU München
% Title  : LiNGAM Models on Minimal Latent Polytrees
% Author : <Your Name>
% Supervisor : Prof. Dr. Mathias Drton
% Mentor : Daniele Tramontano
% Date   : \today
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

%%%%%% Packages %%%%%%
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
    \bibliographystyle{plainnat}
\usepackage{lmodern}
\usepackage{geometry}
  \geometry{margin=3cm}
\usepackage{setspace}
  \onehalfspacing
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning}
\usepackage[overload,ntheorem]{empheq}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{subfig}
\usepackage{comment}
\usepackage{booktabs} % commands to create good-looking tables
\usetikzlibrary{arrows.meta}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand{\indep}{\perp \!\!\! \perp}  % Independence symbol
\numberwithin{equation}{section}

%%%%%% Theorem Environments %%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}

\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\an}{an}
\DeclareMathOperator{\de}{de}
\DeclareMathOperator{\cum}{cum}
\DeclareMathOperator{\ttop}{top}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\thr}{thr}


%%%%%% Title %%%%%%
\title{LiNGAM Models on Minimal Latent Polytrees:\\A Cumulant--Based Discrepancy Approach}
\author{Sang Hyeon Lee\\
  \small Supervisor: Prof.~Dr.~Mathias Drton\\
  \small Mentor: Daniele Tramontano\\[1ex]
  \small Department of Mathematics, Technical University of Munich}
\date{\today}


\begin{document}

\maketitle

\begin{abstract}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % TODO: Write a concise abstract (\approx 200 words) summarising
  % the research question, methodology, and expected contributions.
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  This thesis studies linear, non--Gaussian, acyclic models (LiNGAM) whose underlying directed graph is a \emph{minimal latent polytree}.  Building on the moment identities of \citet{shimizu:hoyer:2006} and \citet{tramontano:monod:drton:2022}, we propose a novel discrepancy matrix based on second and third cumulants of the observed variables.  When plugged into the axiomatic framework of \citet{etesami:kiyavash:coleman:2016}, the new discrepancy enables provably consistent recovery of both observed and latent portions of the polytree.  We outline theoretical guarantees and an empirical evaluation on synthetic data sets.
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Causal discovery from observational data is a central challenge in modern statistics and machine learning.  The \emph{Linear Non--Gaussian Acyclic Model} (LiNGAM) \citep{shimizu:hoyer:2006} demonstrates that non--Gaussianity of disturbances suffices to orient edges in a directed acyclic graph (DAG) that is fully observed.  However, many practical systems contain latent variables whose omission can distort causal inferences.  This thesis investigates LiNGAMs whose causal structure forms a \emph{minimal latent polytree}, following the terminology of \citet{etesami:kiyavash:coleman:2016}.

The work of \citet{tramontano:monod:drton:2022} shows that certain rank conditions on matrices built from second and third cumulants characterize edge orientations, yet its focus is on fully observed graphs.  Conversely, \citep{etesami:kiyavash:coleman:2016} develops a four--axiom \emph{discrepancy matrix} that suffices to learn a latent polytree, but instantiates the matrix via directed--information estimators suited to time series.  In this work we synthesise the two lines: we design a cumulant--based discrepancy that obeys the axioms on a single--time--slice latent polytree.

\paragraph{Contributions.} The main contributions are:
\begin{enumerate}[label=(C\arabic*)]
  \item Definition of the \emph{Latent--LiNGAM Polytree Model} (Section~\ref{sec:model}), combining non--Gaussian noise with minimal latent structures.
  \item Construction of a \emph{cumulant discrepancy matrix} (Section~\ref{sec:discrepancy}) and proof that it satisfies the axioms of \citet{etesami:kiyavash:coleman:2016}.
  \item Adaptation of the Separation--Tree--Merger algorithm to the new discrepancy and consistency analysis (Section~\ref{sec:algorithm}).
  \item Empirical study evaluating orientation accuracy and sample complexity under varying latent proportions (Section~\ref{sec:experiments}).
\end{enumerate}

\paragraph{Organisation.} Section~\ref{sec:lingam_background} reviews the requisite graph--theoretic and statistical preliminaries.  The latent--LiNGAM model is formalised in Section~\ref{sec:model}.  Section~\ref{sec:discrepancy} introduces the cumulant discrepancy matrix and establishes its properties.  Section~\ref{sec:algorithm} outlines a learning algorithm and sketches consistency proofs.  Section~\ref{sec:experiments} presents numerical experiments, and Section~\ref{sec:conclusion} concludes with directions for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Non-Gaussian Structural Causal Models}
\label{sec:lingam_background}

A directed graph (digraph) is a pair $G = (V, E)$, where $V$ is the set of vertices and $E \subset V \times V$ is the set of directed edges. We let $V = [p] := \{1, \ldots, p\}$. An element $(i, j) \in E$ may also be denoted by $i \to j$. A digraph $G$ is acyclic (i.e., a DAG) if it does not contain any directed cycle: there is no sequence of vertices $i_0, \ldots, i_k$ with $i_j \to i_{j+1} \in E$ for $j = 0, \ldots, k - 1$ and $i_0 = i_k$.

A path in $G$ is a sequence of vertices $i_0, \ldots, i_k$ such that $i_j \to i_{j+1} \in E$ or $i_{j+1} \to i_j \in E$ for all $j$. It is directed if all the arrows point in the same direction. A \emph{polytree} is a DAG in which there is a unique path between any two vertices.

If $i \to j \in E$, then $i$ is a parent of $j$, and $j$ is a child of $i$. If $G$ contains a directed path from $i$ to $j$, then $i$ is an ancestor of $j$ and $j$ is a descendant of $i$. The sets of parents, children, ancestors, and descendants of $i \in V$ are denoted by $\pa(i)$, $\ch(i)$, $\an(i)$, $\de(i)$, respectively.

\begin{definition}[Conditional independence]
\label{def:conditional_independence}
Two random variables $X_i$ and $X_j$ are \emph{conditionally independent} given a set of variables $X_C$ if
\[
P(X_i = x_i, X_j = x_j \mid X_C = x_C) = P(X_i = x_i \mid X_C = x_C) P(X_j = x_j \mid X_C = x_C)
\]
for all values $x_i, x_j, x_C$ such that $P(X_C = x_C) > 0$. We denote this relationship by $X_i \indep X_j \mid X_C$.

More generally, for disjoint subsets $A, B, C \subset [p]$, the random vectors $X_A$ and $X_B$ are conditionally independent given $X_C$, written $A \indep B \mid C$, if
\[
P(X_A = x_A, X_B = x_B \mid X_C = x_C) = P(X_A = x_A \mid X_C = x_C) P(X_B = x_B \mid X_C = x_C)
\]
for all values $x_A, x_B, x_C$ such that $P(X_C = x_C) > 0$.
\end{definition}


The joint distribution of $X$ satisfies the \emph{local Markov property} with respect to $G$ if
\[
\{i\} \indep [p] \setminus (\pa(i) \cup \de(i)) \mid \pa(i) \quad \forall i \in [p].
\]

% Replace the existing paragraph about Markov equivalence class with this expanded version:

The \emph{Markov equivalence class} of $G$ is the set of all DAGs that encode the same conditional independence relations, i.e., for which the set of distributions satisfying the local Markov property is the same. See \citet[Chap.~1]{handbook} for further details.

Two fundamental challenges arise when learning causal structure from observational data. First, multiple DAGs can be statistically indistinguishable because they encode the same set of conditional independence constraints. For instance, the two DAGs with two nodes and one edge (so, $1 \to 2$ and $1 \leftarrow 2$) are in the same Markov equivalence class, and cannot be distinguished empirically without imposing further assumptions on the model. This motivates the need for additional identifying assumptions, such as the non-Gaussianity constraints exploited in LiNGAM models.

Methods for causal discovery thus aim to either infer the Markov equivalence class or infer the DAG itself in a model class that renders the graph identifiable. The former approach focuses on recovering the \emph{completed partially directed acyclic graph} (CPDAG), a mixed graph that encodes the causal information common to all members of a Markov equivalence class \citep{meek:1995}. The latter scenario, which is the focus of this work, postulates that the considered models exhibit special properties that permit identification of the full graph \citep{shimizu:hoyer:2006}.

The \emph{skeleton} of a DAG is the undirected graph obtained by replacing each directed edge by an undirected edge. Here, edges are denoted by $\{i,j\} \subseteq E$.


% Replace the existing structural equation paragraph with this expanded version:

\subsection{Structural Equations}
\label{subsec:equations}

A structural equation model hypothesizes that every random variable in $X$ is functionally related to its parent variables:
\[
X_i = f_i(X_{\pa(i)}, \varepsilon_i), \quad i \in V,
\]
where the $\varepsilon_i$ are independent noise terms and the $f_i$ are measurable functions.

This framework provides a principled approach to modeling causation by encoding the fundamental insight that effects are generated by their causes \citep{Peters:Elements:2017}. The structural equations make explicit the \emph{causal mechanism} by which each variable is generated: $X_i$ is determined as a function of its direct causes $X_{\pa(i)}$ and an independent random disturbance $\varepsilon_i$. The independence of the noise terms $\varepsilon_i$ reflects the assumption that, conditional on the direct causes, there are no confounding variables affecting multiple outcomes simultaneously.

The key conceptual advantage of this formulation is that it distinguishes between \emph{seeing} and \emph{doing} \citep{pearl:causality:2009}. While observational distributions $P(X)$ capture statistical associations, the structural equations enable us to answer counterfactual questions of the form ``What would happen to $X_j$ if we set $X_i = x_i$?'' Such interventional reasoning is essential for causal inference and policy evaluation.

If the $f_i$ are linear, then we obtain a \emph{linear structural equation model} (LSEM). An LSEM can be written in matrix form as
\begin{equation}
\label{eq:lsem}
X = (I - \Lambda)^{-\top} \varepsilon,
\end{equation}
where $\Lambda = (\lambda_{ij})$ with $\lambda_{ij} \neq 0$ only if $i \to j \in E$.

The linearity assumption, while restrictive, offers significant computational and theoretical advantages. Linear models admit closed-form solutions for interventional distributions and enable the use of powerful algebraic tools for structure learning. Moreover, in many applications, linear approximations provide reasonable first-order descriptions of complex causal relationships.

An LSEM constrains the dependence structure on the coordinates of $X$, but not the mean. Hence, when working with the LSEM, we may assume without loss of generality that $\mathbb{E}[\varepsilon_i] = 0$, which implies $\mathbb{E}[X_i] = 0$ for all $i \in V$.

Let $\varepsilon^{(2)} = (\mathbb{E}[\varepsilon_i \varepsilon_j])_{ij}$ be the covariance matrix of $\varepsilon$, which is a diagonal matrix by independence, and write $\varepsilon^{(2)}_i := \mathbb{E}[\varepsilon_i^2] > 0$ for its $i$th diagonal entry. The covariance matrix of $X$ is then the positive definite matrix
\begin{equation}
\label{eq:Sigma}
\Sigma = (I - \Lambda)^{-\top} \varepsilon^{(2)} (I - \Lambda)^{-1}.
\end{equation}

This relationship between the structural parameters $(\Lambda, \varepsilon^{(2)})$ and the observed covariance matrix $\Sigma$ is fundamental to the identifiability analysis that follows. When the noise is Gaussian, equation~\eqref{eq:Sigma} captures all distributional information, leading to the Markov equivalence problem discussed above. The non-Gaussian setting, which we explore next, breaks this equivalence and enables full structural recovery.

\subsection{Cumulants in Gaussian and Non-Gaussian Models}
\label{subsec:non-gaussian}

Cumulants are alternative representations of moments of a distribution. Here, we formalize the definition in higher order settings and discuss their implications under Gaussian and non-Gaussian errors.

\begin{definition}[Cumulant tensor]
\label{def:cumulant_tensor}
The $k$th cumulant tensor of a random vector $(X_1, \ldots, X_p)$ is the $k$-way tensor in $\mathbb{R}^{p \times \cdots \times p} \equiv (\mathbb{R}^p)^{\otimes k}$ whose entry in position $(i_1, \ldots, i_k)$ is the joint cumulant
\[
\cum(X_{i_1}, \ldots, X_{i_k}) := \sum_{(A_1, \ldots, A_L)} (-1)^{L-1} (L-1)! \mathbb{E}\left[\prod_{j \in A_1} X_j\right] \cdots \mathbb{E}\left[\prod_{j \in A_L} X_j\right],
\]
where the sum is taken over all partitions $(A_1, \ldots, A_L)$ of the multiset $\{i_1, \ldots, i_k\}$.
\end{definition}

In our context, the variables have mean 0, so
\begin{align}
\cum(X_i) &= \mathbb{E}[X_i] = 0, \\
\cum(X_{i_1}, X_{i_2}) &= \text{Cov}[X_{i_1}, X_{i_2}] = \mathbb{E}[X_{i_1} X_{i_2}].
\end{align}

More generally, the sum can be restricted to the partitions in which all blocks $A_i$ have at least two elements. In particular,
\begin{align}
\cum(X_{i_1}, X_{i_2}, X_{i_3}) &= \mathbb{E}[X_{i_1} X_{i_2} X_{i_3}], \\
\cum(X_{i_1}, X_{i_2}, X_{i_3}, X_{i_4}) &= \mathbb{E}[X_{i_1} X_{i_2} X_{i_3} X_{i_4}] - \mathbb{E}[X_{i_1} X_{i_2}] \mathbb{E}[X_{i_3} X_{i_4}] \\
&\quad - \mathbb{E}[X_{i_1} X_{i_3}] \mathbb{E}[X_{i_2} X_{i_4}] - \mathbb{E}[X_{i_1} X_{i_4}] \mathbb{E}[X_{i_2} X_{i_3}].
\end{align}

The following powerful result dictates a simple condition that characterizes the Gaussianity of $X$.

\begin{theorem}[Marcinkiewicz's theorem {\citealp{marcinkiewicz:1939}}]
\label{thm:marcinkiewicz}
If there exists $k$ such that $\cum(X_{i_1}, \ldots, X_{i_j}) = 0$ for all $j \geq k$, then $k = 3$ and $X$ has a multivariate Gaussian distribution.
\end{theorem}

This theorem establishes that non-Gaussian distributions necessarily have non-zero cumulants of order three or higher, making these higher-order moments essential for distinguishing between Gaussian and non-Gaussian models.


\begin{lemma}
\label{lem:indep_cum}
If the variables $ \varepsilon_1,\dots,\varepsilon_n$ are jointly independent, then $\cum(\varepsilon_{i_1},\dots,\varepsilon_{i_k})=0$ unless $i_1=\dots=i_k$.
\end{lemma}

\begin{lemma}
\label{lem:tucker}
Let the random vector $X$ follow the LSEM from \eqref{eq:lsem} with noise vector $\varepsilon$.  Let
$\mathcal{C}^{(k)}$ and $\varepsilon^{(k)}$ be the $k$th order cumulant tensors of $X$ and $\varepsilon$, respectively.  Then
\begin{align*}
    \mathcal{C}^{(k)}&= \varepsilon^{(k)}\bullet \big[(I-\Lambda)^{-1} \big]_{j=1}^k\\
    &=\varepsilon^{(k)}\bullet(I-\Lambda)^{-1}\bullet \dots \bullet (I-\Lambda)^{-1}
\end{align*}
is the Tucker product of $\varepsilon^{(k)}$ and $k$ copies of $(I-\Lambda)^{-1}$.
\end{lemma}
Notice here that $\mathcal{C}^{(k)}$ reduces to \eqref{eq:Sigma} when $k=2$.

See \citet{comon:jutten:handbook} and references therein for proofs of Theorem \ref{thm:marcinkiewicz} and Lemmas \ref{lem:indep_cum} and \ref{lem:tucker}.

The next definition introduces the cumulant model obtained from the LSEM \eqref{eq:lsem}.  %Recall that tensor $\varepsilon^{(k)}$ is diagonal if only entries in positions $(i,\dots,i)$, $i\in[p]$, are nonzero.

\begin{definition}
Let $G=(V,E)$ be a DAG, and let $K\geq2$ be an integer.  The $K$th cumulant model of $G$ is the set of $K$-way tensors
\begin{multline*}
    \mathcal{M}^{(K)}(G)=
    \{\varepsilon^{(K)}\bullet \big[(I-\Lambda)^{-1} \big]_{j=1}^K\;:
    \Lambda\in\mathbb{R}^E,\; \varepsilon^{(K)}\in(\mathbb{R}^{p})^K \ \text{diagonal}\}.
\end{multline*}
Here, $\mathbb{R}^E$ is the set of $p\times p$ matrices with support $E$.
Further, the cumulants up to order K defined by G are modeled by
\begin{equation}
    \mathcal{M}^{(\leq K)}(G)=\mathcal{M}^{(2)}(G)\times\dots\times\mathcal{M}^{(K)}(G).
\end{equation}
%to be the model of cumulants up to order $K$ defined by $G$.
\end{definition}

By Theorem~\ref{thm:marcinkiewicz}, all multivariate Gaussian vectors $X$ correspond to the zero element of $\mathcal{M}^{(K)}(G)$ for $k\geq3$.
% From now on, we assume the variables in $\varepsilon$ to be non-Gaussian.

When the errors in an LSEM are Gaussian, all distributional information is captured by the covariance matrix and equivalence issues arise that hinder identifiability of the full graph.  %However, the situation is different when the errors are non-Gaussian.
It then becomes necessary to consider non-Gaussian settings.  Relaxing the constraint of Gaussianity gives rise to the class of LiNGAMs where the underlying graph now becomes identifiable \citep{shimizu:hoyer:2006,shimizu:2011}.  We will exploit this property algorithmically and use the signal provided by higher cumulants; we do this by way of {\em treks}.

\begin{definition}[Multi-Trek]
A $k$-trek between vertices $i_1,\dots,i_k\in V$ of a DAG $G=(V,E)$ is a collection of directed paths $T=(P_1,\dots,P_k)$ in $G$ that share the same source and have $i_j$ as the sink of $P_j$ for all $j$. The common source node is the top of the trek $\ttop(T)$.  A trek is simple if the top node is the unique node on all the paths.
\end{definition}
We denote the set of $k$-treks between $i_1,\dots,i_k$ by $\mathcal{T}(i_1,\dots,i_k)$ and the set of simple treks by $\mathcal{S}(i_1,\dots,i_k)$. See Figure~\ref{fig:trek} for an example.
\begin{figure}[ht]
\centering
\begin{tikzpicture}[scale=0.5]
\begin{scope}[every node/.style={circle,thick,draw}]
    \node (1) at (0,0) {t};
    \node (2) at (-6,-4) {1};
    \node (3) at (-2,-4) {2};
    \node (4) at (2,-4) {3};
    \node (5) at (6,-4) {4};
\end{scope}

\begin{scope}[>={Stealth[black]},
              every edge/.style={draw=black,thick}]
    \path [->] (1) edge node{} (2);
    \path [->] (1) edge node{} (3);
    \path [->] (1) edge node{} (4);
    \path [->] (1) edge node{} (5);
\end{scope}
\end{tikzpicture}
\caption{Example of a 4-trek.}
\label{fig:trek}
\end{figure}

If $P$ is a directed path in the DAG $G=(V,E)$ and $\Lambda=(\lambda_{ij})\in\mathbb{R}^E$, then $\lambda^P=\prod_{(i,j)\in P}\lambda_{ij}$ is a path monomial.
%the path monomial given by the product of $\lambda_{ij}$ with $i\to j$ an edge in $P$.
For a $k$-trek $T=(P_1,\dots,P_k)$, set $\lambda^T:=\lambda^{P_1}\cdots\lambda^{P_k}$.

\begin{proposition}[Multi-Trek Rule]
\label{prop:multi:trek}
The $k$th order cumulant tensor $\mathcal{C}^{(k)}(G)$ of $X$ can be expressed as
\begin{equation}
\label{eq:trek}
    \mathcal{C}^{(k)}_{i_1,\dots,i_k}(G)=\sum\varepsilon^{(k)}_{\ttop(T)}\lambda^T,
\end{equation}
where the sum is over all the treks $T$ in $\mathcal{T}(i_1,\dots,i_k)$ and $\varepsilon^{(k)}_{\ttop(T)}$ denotes the $\ttop(T)$ diagonal entry of $\varepsilon^{(k)}$.
\end{proposition}

Proposition \ref{prop:multi:trek} follows from Lemma~\ref{lem:tucker} and expanding the entries of $(I-\Lambda)^{-1}$ into sums of path monomials as in the usual trek rule for covariances \citep{robeva:2021}.

\begin{corollary}[Simple Multi-Trek Rule]
\label{cor:simple-trek-rule}
The $k$th order cumulant tensor $\mathcal{C}^{(k)}(G)$ of $X$ can be expressed as
\begin{equation}
    \mathcal{C}^{(k)}_{i_1,\dots,i_k}(G)=\sum \mathcal{C}^{(k)}_{\ttop(S)}(G)\lambda^{S},
\end{equation}
where the sum is extended to all the simple treks $S$ in $\mathcal{S}(i_1,\dots,i_k)$.
%, and the new term $m^{(k)}_{i}$.
\end{corollary}

\begin{corollary}
\label{cor:simple_trek_2}
The $i$th diagonal entry of $\mathcal{C}^{(k)}$ is
\begin{equation*}
     \mathcal{C}^{(k)}_{i}(G)=\displaystyle\sum_{p_1,\dots,p_k\in \pa(i)}\lambda_{p_1, i}\cdots\lambda_{p_k,i}\mathcal{C}^{(k)}_{p_1,\dots,p_k}(G)+\varepsilon^{(k)}_i.
\end{equation*}
\end{corollary}

\subsection{Polytree Models}
\label{subsec:polytree-models}
For general graphs, the algebraic relations among the cumulants may be far more complicated than the bivariate case (which is discussed in Example~\ref{ex:two:vert}) and have not yet been fully characterized. However, there exists a generalization of rank-one constraints for polytrees, which we now discuss.

By consequence of there being at most one directed path between any two nodes of a polytree $G$, there is at most one simple trek between any set of nodes $i_1,\dots,i_k$. The simple multi-trek rule then reduces to $C^{(k)}_{i_1,\dots,i_k}(G)=\lambda^{S}\mathcal{C}^{(k)}_{\ttop(S)}$ for a trek between nodes with $S$ being the unique simple trek; denote the top of the simple trek between $i_1,\dots,i_k$, if it exists by $\ttop(i_1,\dots,i_k)$. Also, $C^{(k)}_{i_1,\dots,i_k}(G)=0$ if there is no $k$-trek between the nodes.

For any two vertices $i\not=j$, let $c^{(i,j),k}_m$ denote the $k$th order cumulant $\mathcal{C}^{(k)}_{{i\dots i},{j\dots j}}(G)$, where the first $m$ indices are equal to $i$ and the remaining $k-m$ equal $j$.
% ---------- Section 4 : matrix definition (exact Prop. 2.10) ----------
% For i ≠ j observed and an integer K ≥ 3
\newcommand{\cumedge}[3]{c_{#1,#2#3}}  % usage: \cumedge{e}{k}{m}
\begin{proposition}[{\citealp[Prop.~2.10]{tramontano:monod:drton:2022}}]
\label{prop:rank}
Let $e:i\!\to\!j$ be an edge of a polytree $G$ and fix an integer $K\ge 3$.
Then the matrix is of rank one
\begin{align}
\label{eq:matrix}
  A^{e,K}
  \;=\;
  \Bigl[
        \begin{matrix}
          c^{e,k}_{m} \\[2pt]
          c^{e,k}_{m-1}
        \end{matrix}
        \,\Bigm\vert\,
        2\le m\le k\le K
  \Bigr].
\end{align}
The first column of $A^{e,K}$ contains $\mathbb{E}[X_i^2]>0$. Moreover, for every distribution induced by non-Gaussian errors, there exists $k$ such that $\mathcal{C}^{(k)}_i\neq0$. Hence, at least one minor of $A^{e,K}$ gives us an equation that is satisfied if $i\to j$ is in $G$, and is not satisfied in general for the graph with the edge reversed. This observation will provide the foundation for our discrepancy measure, which we present in Section~\ref{sec:discrepancy}.
\end{proposition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Latent--LiNGAM Polytree Model}
\label{sec:model}

We now specialize the general LiNGAM framework to the polytree setting with latent variables. This restriction provides significant computational advantages while still capturing important classes of causal relationships.

\begin{definition}[Minimal latent polytree]\label{def:minimal_polytree}
Let $G=(V,E)$ be a DAG whose underlying undirected graph is a tree. Partition $V=O\cup L$ into observed and latent vertices. The pair $(G,O)$ is a \emph{minimal latent polytree} if every $\ell\in L$ has out-degree at least~$2$.
\end{definition}

The minimality condition ensures that no latent variable is redundant—removing any latent variable would disconnect the observed variables or change the conditional independence structure among them. This constraint is essential for identifiability, as latent nodes with out-degree 1 cannot be distinguished from direct edges between observed variables.

\begin{definition}[Latent--LiNGAM polytree model]\label{def:latent_lingam_model}
A random vector $X\in\mathbb R^{|V|}$ follows the latent--LiNGAM polytree model on $(G,O)$ if:
\begin{enumerate}[label=(\roman*)]
  \item The distribution of $X$ satisfies the structural equation~\eqref{eq:lsem} with coefficient matrix~$\Lambda$ compatible with $G$.
  \item The set $\varepsilon=(\varepsilon_i)_{i\in V}$ has independent, non--Gaussian entries with finite third moments.
  \item Only $(X_i)_{i\in O}$ are observed.
\end{enumerate}
\end{definition}

This model combines the identifiability advantages of non-Gaussian noise with the computational tractability of polytree structures. The restriction to polytrees ensures that there is a unique undirected path between any two nodes, which simplifies both the theoretical analysis and algorithmic development.

\subsection{Key Properties of the Model}

Under the latent-LiNGAM polytree assumptions, several important properties hold:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Path uniqueness}: The unique directed path between any $i,j\in O$ factors through their lowest common ancestor (LCA). This property will underpin our discrepancy construction.

\item \textbf{Moment identifiability}: The non-Gaussian noise assumption ensures that higher-order cumulants provide sufficient information to identify both the structure and parameters of the model, breaking the equivalence classes that arise under Gaussianity.

\item \textbf{Computational tractability}: The polytree constraint reduces the complexity of structure learning algorithms from exponential (in general DAGs) to polynomial time.
\end{enumerate}

The combination of these properties makes the latent-LiNGAM polytree model particularly well-suited for developing efficient structure learning algorithms based on cumulant information, as we will demonstrate in the subsequent sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cumulant--Based Discrepancy Matrix}
\label{sec:discrepancy}
%--------------------------------------------------------------------
%  Definition 7 from Etesami–Kiyavash–Coleman (Neural Computation 2016)
%--------------------------------------------------------------------
\begin{definition}[Discrepancy on a polytree {\citealp[Def.~7]{etesami:kiyavash:coleman:2016}}]
\label{def:discrepancy_paper1}
Given a polytree $\vec T=(V,\vec E)$ with root set $R$, every function
$\gamma:V\times V\to\mathbb R$ that satisfies the following four criteria
is called a \emph{discrepancy} on $\vec T$:
\begin{enumerate}[label=(\arabic*)]
  \item $\gamma(v_1,v_2)=0$ $\iff$ either $v_1$ is an ancestor of $v_2$ or $v_1=v_2$.
  \item If $\operatorname{LCA}(v_1,v_2)=\operatorname{LCA}(v_1,v_3)$, then $\gamma(v_1,v_2)=\gamma(v_1,v_3)$.
  \item If $\operatorname{LCA}(v_1,v_2)$ lies on the path from $\operatorname{LCA}(v_1,v_3)$ to $v_1$, then $\gamma(v_1,v_2) < \gamma(v_1,v_3)$.
  \item $\gamma(v_1,v_2)<0$ $\iff$ $v_1$ and $v_2$ have no common ancestor.
\end{enumerate}

The image of such functions can be presented by the discrepancy matrix:
\[
  \Gamma_V := \left[ \gamma(v_i, v_j) \right], \quad v_i, v_j \in V.
\]

Note that for a given tree, the discrepancy matrix is not unique. Any function that satisfies the conditions in Definition~\ref{def:discrepancy_paper1} is a valid discrepancy measure.

\end{definition}


\begin{definition}[Cumulant Discrepancy Measure]\label{def:discrepancy}
Let $X = (X_1, \dots, X_p)$ be observed variables from a linear non-Gaussian model. For any $i,j \in \{1,\dots,p\}$, define the cumulant discrepancy measure $\gamma(i,j)$ as
\begin{equation}\label{eq:gamma}
\gamma(i,j) =
\begin{cases}
-1, & \text{if } \rho_{i,j} = 0, \\[0.5ex]
0, & \text{if } (\Sigma_{i,i} \mathcal{C}^{(3)}_{i,i,j} - \Sigma_{i,j} \mathcal{C}^{(3)}_{i,i,i}) = 0 \text{ or } i = j, \\[0.5ex]
\dfrac{\mathcal{C}^{(3)}_{i,j,j} \, \Sigma_{i,i}}{\mathcal{C}^{(3)}_{i,i,j} \, \Sigma_{i,j}}, & \text{otherwise}.
\end{cases}
\end{equation}
\noindent
where $\Sigma$ is the covariance matrix, and $\mathcal{C}^{(3)}$ is the third-order cumulant tensor.

\end{definition}

\begin{lemma}[Wright's Formula {\citealp{wright:1960}}]
\label{lem:wright}
In a LiNGAM polytree model, the correlation $\rho_{i,j} = \text{Corr}[X_i,X_j]$ satisfies
\begin{equation}
  |\rho_{i,j}| =
  \begin{cases}
    \prod |\rho_e|, & \text{if } \mathcal{T}(i,j) \ne \emptyset, \\[0.5ex]
    0, & \text{otherwise},
  \end{cases}
\end{equation}
where the product is taken over the edges $e$ of the unique trek connecting $i$ and $j$, and $\rho_e$ denotes the correlation between the variables at the endpoints of $e$.
\end{lemma}

\begin{remark}
This result implies that $\rho_{i,j} = 0$ if and only if there exists no trek connecting $i$ and $j$ in the polytree. This is particularly useful in interpreting the case $\gamma(i,j) = -1$ in Definition~\ref{def:discrepancy}.
\end{remark}

\begin{proposition}[Axioms]\label{prop:axioms}
The map $\gamma:O\times O\to\mathbb R$ defined in Definition~\ref{def:discrepancy} satisfies the four axioms of Definition~\ref{def:discrepancy_paper1} on the latent--LiNGAM polytree model, provided the cumulants exist.
\end{proposition}

\begin{proof}
We verify that the cumulant discrepancy measure $\gamma: O \times O \to \mathbb{R}$ defined in Definition~\ref{def:discrepancy} satisfies the four axioms of Definition~\ref{def:discrepancy_paper1}, assuming all required cumulants exist.

\begin{enumerate}[label=(\arabic*)]
  \item
    From Definition~\ref{def:discrepancy}, $\gamma(i,i) = 0$. Assume $i \ne j$ and that $i$ is an ancestor of $j$. Then we know that there is a simple trek between $i$ and $j$. From the simple trek rule we know that $\Sigma_{i,j} = \lambda^{S}\Sigma_{i,i}$ and $\mathcal{C}^{(3)}_{i,i,j} = \lambda^{S}\mathcal{C}^{(3)}_{i}$, where $S$ is the simple trek between $i$ and $j$. Hence we have $(\Sigma_{i,i} \mathcal{C}^{(3)}_{i,i,j} - \Sigma_{i,j} \mathcal{C}^{(3)}_{i,i,i}) = 0$.

  \item Let $v_i$ be the lowest common ancestor of two observed variables $v_j$ and $v_k$. Suppose also that $v_i$ is the lowest common ancestor of $v_j$ and $v_\ell$. We will show that
  \[
  \gamma(j,k) = \gamma(j,\ell).
  \]
  By Definition~\ref{def:discrepancy}, for distinct observed vertices $u,v$ one has
    \[
      \gamma(u,v)
      =
      \frac{\mathcal{C}^{(3)}_{u,v,v}\,\Sigma_{u,i}}{\mathcal{C}^{(3)}_{u,u,v}\,\Sigma_{u,v}},
    \]
    where $\Sigma$ is the covariance matrix and $\mathcal{C}^{(3)}$ is the third-order cumulant tensor. We now express each entry in this ratio using the simple trek rule (Corollary~\ref{cor:simple-trek-rule}).

    \begin{enumerate}
      \item Because $v_i$ is the lowest common ancestor of $v_j$ and $v_k$, every simple trek from $v_j$ to $v_k$ factors through $v_i$. The simple trek rule implies
      \[
        \mathcal{C}^{(3)}_{j,j,k}
        =
        \bigl(\lambda^{P(i,j)}\bigr)^2\,\lambda^{P(i,k)}\,\mathcal{C}^{(3)}_{i},\qquad
        \Sigma_{j,k}
        =
        \lambda^{P(i,j)}\,\lambda^{P(i,k)}\,\Sigma_{i},
      \]
      where $\lambda^{P(a,b)}$ denotes the product of structural coefficients along the unique directed path $P(a,b)$ in the polytree, and $\mathcal{C}^{(3)}_{i}$, $\Sigma_i$ are the third-order cumulant and variance of $X_{v_i}$, respectively.

      \item Similarly,
      \[
        \mathcal{C}^{(3)}_{j,k,k}
        =
        \lambda^{P(i,j)}\,\bigl(\lambda^{P(i,k)}\bigr)^2\,\mathcal{C}^{(3)}_{i},\qquad
        \Sigma_{j,j}
        =
        \,\bigl(\lambda^{P(i,j)}\bigr)^2\,\Sigma_{i}.
      \]
    \end{enumerate}

    Substituting these identities into the definition of $\gamma$ gives
    \begin{align*}
      \gamma(j,k)
      &= \frac{\mathcal{C}^{(3)}_{j,k,k}}{\mathcal{C}^{(3)}_{j,j,k}}
         \cdot\frac{\Sigma_{j,j}}{\Sigma_{j,k}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(i,j)}\,(\lambda^{P(i,k)})^2\,\mathcal{C}^{(3)}_{i}\bigr]
        \cdot
        \bigl[\bigl(\lambda^{P(i,j)}\bigr)^2\,\Sigma_{i}\bigr]
      }{
        \bigl[\bigl(\lambda^{P(i,j)}\bigr)^2\,\lambda^{P(i,k)}\,\mathcal{C}^{(3)}_{i}\bigr]
        \cdot
        \bigl[\lambda^{P(i,j)}\,\lambda^{P(i,k)}\,\Sigma_{i}\bigr]
      } \\[1ex]
      &= 1.
    \end{align*}

    An identical computation replacing $v_k$ by $v_\ell$ yields
    \[
      \gamma(j,\ell)
      = 1.
    \]

    This completes the verification of Axiom~(2).

  \item Suppose $d:=\mathrm{LCA}(i,j)$ lies strictly below $c:=\mathrm{LCA}(i,k)$ on the unique path from \(c\) to \(i\).  Write \(\lambda^{P(u,v)}\) for the product of structural coefficients on the directed path \(P(u,v)\) in the polytree.  Using the simple–trek rule again, we obtain the following identities:
  \[
    \begin{aligned}
    \mathcal{C}^{(3)}_{i,j,j}&=\lambda^{P(d,i)}\,\bigl(\lambda^{P(d,j)}\bigr)^{2}\,\mathcal{C}^{(3)}_{d},&
    \mathcal{C}^{(3)}_{i,i,j}&=\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\lambda^{P(d,j)}\,\mathcal{C}^{(3)}_{d},\\[0.5ex]
    \mathcal{C}^{(3)}_{i,k,k}&=\lambda^{P(c,i)}\,\bigl(\lambda^{P(c,k)}\bigr)^{2}\,\mathcal{C}^{(3)}_{c},&
    \mathcal{C}^{(3)}_{i,i,k}&=\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\lambda^{P(c,k)}\,\mathcal{C}^{(3)}_{c},\\[0.5ex]
    \Sigma_{i,j}&=\lambda^{P(d,i)}\,\lambda^{P(d,j)}\,\Sigma_{d,d},&
    \Sigma_{i,k}&=\lambda^{P(c,i)}\,\lambda^{P(c,k)}\,\Sigma_{c,c}.&
    \end{aligned}
  \]
  Substituting into~\eqref{eq:gamma}, the path monomials and third–order cumulants cancel, and we find
  \begin{align*}
      \gamma(i,j)
      &= \frac{\mathcal{C}^{(3)}_{i,j,j}}{\mathcal{C}^{(3)}_{i,i,j}}
         \cdot\frac{\Sigma_{i,i}}{\Sigma_{i,j}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(d,i)}\,(\lambda^{P(d,j)})^2\,\mathcal{C}^{(3)}_{d}\bigr]
        \cdot
        \Sigma_{i,i}
      }{
        \bigl[\bigl(\lambda^{P(d,i)}\bigr)^2\,\lambda^{P(d,j)}\,\mathcal{C}^{(3)}_{d}\bigr]
        \cdot
        \bigl[\lambda^{P(d,i)}\,\lambda^{P(d,j)}\,\Sigma_{d,d}\bigr]
      } \\[1ex]
      &= \frac{\Sigma_{i,i}}{\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\Sigma_{d,d}},
    \end{align*}

  \begin{align*}
      \gamma(i,k)
      &= \frac{\mathcal{C}^{(3)}_{i,k,k}}{\mathcal{C}^{(3)}_{i,i,k}}
         \cdot\frac{\Sigma_{i,i}}{\Sigma_{i,k}}  \\[1ex]
      &=
      \frac{
        \bigl[\lambda^{P(c,i)}\,(\lambda^{P(c,k)})^2\,\mathcal{C}^{(3)}_{c}\bigr]
        \cdot
        \Sigma_{i,i}
      }{
        \bigl[\bigl(\lambda^{P(c,i)}\bigr)^2\,\lambda^{P(c,k)}\,\mathcal{C}^{(3)}_{c}\bigr]
        \cdot
        \bigl[\lambda^{P(c,i)}\,\lambda^{P(c,k)}\,\Sigma_{c,c}\bigr]
      } \\[1ex]
      &= \frac{\Sigma_{i,i}}{\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\Sigma_{c,c}}.
    \end{align*}

  Taking the ratio gives
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\bigl(\lambda^{P(c,i)}\bigr)^{2}\,\Sigma_{c,c}}{\bigl(\lambda^{P(d,i)}\bigr)^{2}\,\Sigma_{d,d}}.
  \]
  Because \(c\) is an ancestor of \(d\), the path from \(c\) to \(i\) factors through \(d\).  Hence
  \(\lambda^{P(c,i)}=\lambda_{c,d}\,\lambda^{P(d,i)}\) and
  \(\bigl(\lambda^{P(c,i)}\bigr)^{2}=\lambda_{c,d}^{2}\,\bigl(\lambda^{P(d,i)}\bigr)^{2}\).  Substituting yields
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\lambda_{c,d}^{2}\,\Sigma_{c,c}}{\Sigma_{d,d}}.
  \]
  Finally, using the Corollary~\ref{cor:simple_trek_2} and that in a polytree there can be at most one simple trek between $c$ and $d$, we have that
  \(\Sigma_{d,d}=\lambda_{c,d}^{2}\,\Sigma_{c,c}+\omega_{d}^{2}\), where \(\omega_{d}^{2}=\var(\varepsilon_{d})>0\) is the variance of the disturbance at \(d\).  Consequently,
  \[
    \frac{\gamma(i,j)}{\gamma(i,k)}
    =
    \frac{\lambda_{c,d}^{2}\,\Sigma_{c,c}}{\lambda_{c,d}^{2}\,\Sigma_{c,c}+\omega_{d}^{2}}
    < 1,
  \]
  proving that \(\gamma(i,j)<\gamma(i,k)\) whenever \(\mathrm{LCA}(i,j)\) is strictly below \(\mathrm{LCA}(i,k)\).

  \item
  If $i$ and $j$ have no common ancestor, then there exists no trek between them. Hence from Lemma~\ref{lem:wright} we have $\rho_{i,j} = 0$, and thus $\gamma(i,j) = -1 < 0$.
\end{enumerate}
\end{proof}

\subsection{Example: Discrepancy Matrix on a Four‐Node Polytree}

We illustrate the cumulant‐based discrepancy measure on a simple polytree with four observed nodes.  Let $V=\{v_1,v_2,v_3,v_4\}$ and consider the directed edges
\[
  v_1 \longrightarrow v_2,\qquad
  v_1 \longrightarrow v_3,\qquad
  v_3 \longrightarrow v_4,
\]
depicted in Figure~\ref{fig:example-polytree}.  In this example every vertex is observed; there are no latent variables.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[scale=1.1,node distance=1.5cm and 2cm]
  \node[circle,draw](v1) {$v_1$};
  \node[circle,draw,below left of=v1](v2) {$v_2$};
  \node[circle,draw,below right of=v1](v3) {$v_3$};
  \node[circle,draw,below right of=v3](v4) {$v_4$};
  \draw[->] (v1) -- (v2);
  \draw[->] (v1) -- (v3);
  \draw[->] (v3) -- (v4);
\end{tikzpicture}
\caption{A four‐node polytree with edges $v_1\to v_2$, $v_1\to v_3$ and $v_3\to v_4$.}
\label{fig:example-polytree}
\end{figure}

\paragraph{Structural parameters.}
Assign structural coefficients $\lambda_{1,2}=2$, $\lambda_{1,3}=3$ and $\lambda_{3,4}=4$.  Let the noise variables $\varepsilon_i$ be independent with variances $\sigma_i^2 = 1$ and third cumulants $\kappa_i = 1$ for $i=1,2,3,4$.  The structural equations are then
\[
  X_1 = \varepsilon_1,\quad
  X_2 = 2\,X_1 + \varepsilon_2,\quad
  X_3 = 3\,X_1 + \varepsilon_3,\quad
  X_4 = 4\,X_3 + \varepsilon_4 = 12\,X_1 + 4\,\varepsilon_3 + \varepsilon_4.
\]

\paragraph{Covariances and third cumulants.}
From these recursions one computes the covariance matrix
\[
\Sigma =
\begin{pmatrix}
 1 &  2 &  3 & 12\\
 2 &  5 &  6 & 24\\
 3 &  6 & 10 & 40\\
12 & 24 & 40 & 161
\end{pmatrix}.
\]
The third‐order cumulant tensor $\mathcal{C}^{(3)}$ has diagonal entries
\[
 \mathcal{C}^{(3)}_{1,1,1} = 1,\quad
 \mathcal{C}^{(3)}_{2,2,2} = 2^3 + 1 = 9,\quad
 \mathcal{C}^{(3)}_{3,3,3} = 3^3 + 1 = 28,\quad
 \mathcal{C}^{(3)}_{4,4,4} = (12)^3 + (4)^3 + 1 = 1793,
\]
using the simple trek rule, where for $\mathcal{C}^{(3)}_{4,4,4}$, it can also be shown differently with Corollary~\ref{cor:simple_trek_2} :
\[
 \mathcal{C}^{(3)}_{4,4,4} = \lambda_{3,4}^3\cdot\mathcal{C}^{(3)}_{3,3,3} + \kappa_4 = 4^3\cdot28+1=1793.
\]
Moreover, with a polytree, the only non–vanishing mixed entries are of the form
\[
\mathcal{C}^{(3)}_{i,j,k} \;=\; \lambda^{P(h,i)}\lambda^{P(h,j)}\lambda^{P(h,k)}\kappa_h,
\]
where $\lambda^{P(h,i)}$ denotes the path monomial from $h$ to $i$, with $h$ being the unique common ancestor of $i$,$j$ and $k$. For instance, $\mathcal{C}^{(3)}_{2,3,3}=2\cdot 3^2\cdot1=18$ and $\mathcal{C}^{(3)}_{2,3,4}=2\cdot3\cdot12\cdot1=72$.

\paragraph{Discrepancy matrix.}
Applying Definition~\ref{def:discrepancy} yields the following cumulant discrepancy matrix~$\Gamma=[\gamma(v_i,v_j)]$:
\[
\Gamma
=
\begin{pmatrix}
 0 & 0 & 0 & 0\\[0.2ex]
 \frac{5}{4} & 0 & \frac{5}{4} & \frac{5}{4}\\[0.4ex]
 \frac{10}{9} & \frac{10}{9} & 0 & 0\\[0.4ex]
 \frac{161}{144} & \frac{161}{144} & \frac{161}{160} & 0
\end{pmatrix}.
\]
Here, each non-zero entry was computed using the ratio in~\eqref{eq:gamma} or, equivalently, by using the simplified formula derived in the proof of Proposition~\ref{prop:axioms}.  For example,
\[
  \gamma(v_2,v_3)
  = \frac{\Sigma_{2,2}\,\mathcal{C}^{(3)}_{2,3,3}}
         {\mathcal{C}^{(3)}_{2,2,3}\,\Sigma_{2,3}}
  = \frac{5 \cdot 18}{12\cdot 6} = \frac{5}{4},
  \quad
  \gamma(v_4,v_3)
  = \frac{\Sigma_{4,4}}{\lambda_{3,4}^2\,\Sigma_{3,3}}
  = \frac{161}{4^2\cdot 10} = \frac{161}{160}\approx 1.006.
\]

\paragraph{Interpretation.}
The matrix $\Gamma$ respects all four axioms of Definition~\ref{def:discrepancy_paper1}.  Zero entries arise whenever the first argument is an ancestor of the second; equal values appear when pairs share the same lowest common ancestor, and nested ancestry leads to increasing values, e.g.\ $\gamma(v_4,v_3)=161/160<\gamma(v_4,v_2)=161/144$ since $\mathrm{LCA}(v_4,v_3)=v_3$ lies below $\mathrm{LCA}(v_4,v_2)=v_1$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recovery of Latent Trees}
\label{sec:algorithm}

\subsection{Theoretical Foundation}

Before presenting the algorithmic framework, we establish the key theoretical concepts from \citet{etesami:kiyavash:coleman:2016} that underpin our approach.

\begin{definition}[Learnable subset {\citealp[Def.~8]{etesami:kiyavash:coleman:2016}}]
\label{def:learnable_subset}
In a polytree $\vec{T} = (V, \vec{E})$, we call a subset $L \subset V$ \emph{learnable} if every node $v \in L$ has at least two outgoing arrows. We call $O := V \setminus L$ the set of observed nodes.
\end{definition}

\begin{remark}
From Definition~\ref{def:learnable_subset}, if $L$ is a learnable subset of a polytree, then all the leaves belong to $O = V \setminus L$. This ensures that latent variables with insufficient connectivity (out-degree less than 2) cannot be distinguished from observed variables based solely on the discrepancy patterns.
\end{remark}

The following theorem provides the theoretical guarantee for the recovery algorithm:

\begin{theorem}[Structure identifiability {\citealp[Thm.~4]{etesami:kiyavash:coleman:2016}}]
\label{thm:identifiability}
Let $\vec{T} = (V, \vec{E})$ be a polytree with root set $R$, and let $L \subseteq V$ be a learnable subset. Then the existence of a discrepancy matrix $\Gamma_O$ for $O = V \setminus L$ suffices for learning $\vec{T}$.
\end{theorem}

\begin{proof}
See \citet[Appendix H]{etesami:kiyavash:coleman:2016}.
\end{proof}

The proof of Theorem~\ref{thm:identifiability} proceeds by mathematical induction on the size of the observed node set $O$ and provides the constructive framework for the three-phase algorithm presented below.

\begin{definition}[Tree merger {\citealp[Def.~9]{etesami:kiyavash:coleman:2016}}]
\label{def:tree_merger}
A \emph{tree merger} is an operator that takes two directed trees $\vec{T}_1$, $\vec{T}_2$ and a given subtree of both of them, say $\vec{T}_3$, and merges them at $\vec{T}_3$. We denote this operation by
\[
\vec{T}_1 \circ \vec{T}_2 \big|_{\vec{T}_3}.
\]
\end{definition}

\subsection{Structure Recovery Algorithm}

The rationale of our algorithmic approach follows the three main steps of the proof of Theorem~\ref{thm:identifiability}:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Root discovery}: Discover the number of roots $|R|$ of the underlying polytree and all their descendants in the set of observed nodes $O$ given the discrepancy matrix $\Gamma_O$. This can be accomplished by fixing a node $v \in O$ and finding a maximal subset of $O$ containing $v$ in which every pair of nodes has non-negative discrepancy.

\item \textbf{Subtree recovery}: Recover the underlying tree for each root $r \in R$, based on the descendants of $r$ that were discovered in $O$ during the first step.

\item \textbf{Tree merging}: Merge the trees recovered in the previous step to reconstruct the underlying polytree. When two recovered trees are connected, their combined subgraph forms a tree, which can be learned using the Tree algorithm.
\end{enumerate}

The correctness of this approach relies on the key insight that if a polytree $\vec{T}$ and a directed tree $\vec{T}_i$ have a non-empty intersection, their union is guaranteed to be a single-rooted tree, enabling recursive reconstruction via the tree merger operation.


\subsection{Algorithmic Recovery from Cumulant Discrepancy}

We adapt the recovery pipeline of \citet{etesami:kiyavash:coleman:2016} to our cumulant-based discrepancy measure. The three-stage method first partitions nodes into sibling groups, then orients edges within groups, and finally inserts latent nodes to recover a minimal latent polytree.

\vspace{1ex}

\begin{algorithm}[H]
\caption{Separation($\Gamma_O$)}
\label{alg:separation}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Sibling groups $O_1, \ldots, O_{|\mathcal{R}|}$
    \State $M \leftarrow \varnothing$, $i \leftarrow 1$
    \While{$O \setminus M \ne \varnothing$}
        \State Choose $v \in O \setminus M$
        \State Find all $C \subseteq O$ such that $v \in C$ and
        \Statex \hspace{\algorithmicindent} for all $(u,w) \in C \times C$, $\gamma(u,w) \ge 0$
        \State $O_i \leftarrow$ maximal such $C$
        \State \Return $O_i$
        \State $M \leftarrow M \cup O_i$
        \State $i \leftarrow i + 1$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Tree($\Gamma_O$)}
\label{alg:tree}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Directed tree $\vec{T} = (V, \vec{E})$
    \ForAll{$v \in O$}
        \State $B_v \leftarrow \argmin_{u \in O \setminus \{v\}} \gamma(v,u)$
    \EndFor
    \If{$B_v = O \setminus \{v\}$ for all $v \in O$}
        \If{$\exists w \in O$ such that $\min_{u \in O \setminus \{w\}} \gamma(w,u) = 0$}
            \State $\vec{T}$ is a star graph with $w$ as the root in the center
        \Else
            \State $\vec{T}$ is a star graph with a hidden node as the root in the center
        \EndIf
    \Else
        \State Choose $w$ such that $B_w \ne O \setminus \{w\}$
        \State $\vec{T}' \leftarrow \textsc{Tree}(B_w \cup \{w\})$
        \State $\vec{T}'' \leftarrow \textsc{Tree}(O \setminus B_w)$
        \State Substitute $w$ in $\vec{T}''$ by another node, say $h$
        \State $\vec{T} \leftarrow \vec{T}' \oplus \vec{T}''(h)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Polytree($\Gamma_O$)}
\label{alg:polytree}
\begin{algorithmic}[1]
    \Require Discrepancy matrix $\Gamma_O$
    \Ensure Minimal latent polytree $\vec{T} = (V, \vec{E})$
    \State $\{O_1, \ldots, O_{|\mathcal{R}|}\} \leftarrow \textsc{Separation}(\Gamma_O)$
    \State $\vec{T} \leftarrow \textsc{Tree}(O_1)$
    \State $S \leftarrow O_1$, \quad $I \leftarrow \{1\}$
    \While{$I \ne \{1,2,\ldots,|\mathcal{R}|\}$}
        \State Find $i \in \{1,\ldots,|\mathcal{R}|\} \setminus I$ such that $O_i \cap S \ne \varnothing$
        \State $\vec{T}_{\text{sub}} \leftarrow \textsc{Tree}(S \cap O_i)$
        \State $\vec{T}_i \leftarrow \textsc{Tree}(O_i)$
        \State $\vec{T} \leftarrow \vec{T} \circ \vec{T}_i \vert_{\vec{T}_{\text{sub}}}$
        \State $S \leftarrow S \cup O_i$
        \State $I \leftarrow I \cup \{i\}$
    \EndWhile
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:polytree} presents the main algorithm for learning the polytree $\vec{T}(V, \vec{E})$ with the root set $R$ given the discrepancy matrix $\Gamma_O$ on its observed nodes $O$. First, it calls the subroutine Separation($\Gamma_O$), which finds subsets $O_i$s, where $O = \cup_i O_i$ such that each subset corresponds to observed nodes in a directed tree with a single root. Each of these single rooted subtrees can be learned by Algorithm~\ref{alg:tree}. To complete the task, Algorithm~\ref{alg:polytree} must connect these subtrees to recover the original polytree. This is done by using the fact that if a polytree $\vec{T}$ and a directed tree $\vec{T}_i$ have an intersection, their common subgraph is also a tree; thus, it can be learned using Algorithm~\ref{alg:tree}.

\paragraph{Algorithm Description.}
The \textsc{Separation} algorithm operates on a given discrepancy matrix $\Gamma_O$ of the observed nodes. The aim of this partition is to obtain the set of vertices $O$ into subsets $O_1, O_2, \ldots, O_{|R|}$, where $|R|$ is the total number of subsets corresponding to different roots. Each subset satisfies a specific compatibility condition $\gamma(u, w) \geq 0$ for all pairs within the subset.

The algorithm starts with an empty set $M$ to track the processed vertices, and a counter $i$ set to 1 to keep track of the number of subsets generated. In the main loop, as long as there are unprocessed vertices in $O$, the algorithm picks an arbitrary vertex $v$ from the unprocessed set $O \setminus M$. It then identifies all possible subsets $C \subseteq O$ such that $v \in C$ and for all $(u,w) \in C \times C$, we have $\gamma(u,w) \geq 0$. The maximal such subset becomes $O_i$, ensuring that nodes sharing a common root ancestor are grouped together.

The \textsc{Tree} algorithm provides a method for constructing a directed tree from a set of nodes using the discrepancy measure. Initially, the algorithm computes for each vertex $v \in O$ the set $B_v$ of best neighbors, defined as those vertices $u \in O \setminus \{v\}$ that minimize $\gamma(v,u)$. The algorithm then checks if the tree can be simplified to a star graph structure. If all nodes satisfy $B_v = O \setminus \{v\}$, it determines whether there exists a node $w \in O$ such that $\min_{u \in O \setminus \{w\}} \gamma(w,u) = 0$. If such a node exists, the tree is constructed as a star graph with $w$ as the root. Otherwise, the tree is built as a star graph with a hidden root.

If the star graph condition is not met, the algorithm proceeds with recursive tree construction. It selects a node $w$ such that $B_w \neq O \setminus \{w\}$ and recursively constructs subtrees. The process involves substituting nodes and merging trees using the tree merger operation to ensure that the hierarchical relationships between nodes are properly captured.


\subsection{Adaptation to Cumulant Discrepancy}

In our setting, we adapt these algorithms to work with our cumulant-based discrepancy measure from Definition~\ref{def:discrepancy}. The key modification lies in verifying that our measure satisfies the four axioms of Definition~\ref{def:discrepancy_paper1}, which we established through our computational examples.

The theoretical guarantees of Theorem~\ref{thm:identifiability} transfer directly to our cumulant setting, provided that:
\begin{enumerate}[label=(\roman*)]
\item The cumulant estimates are sufficiently accurate,
\item The non-Gaussianity assumption ensures identifiability of the structural equations,
\item The minimal latent polytree structure satisfies the learnability condition.
\end{enumerate}

Under these conditions, the Separation-Tree-Merger pipeline provides a consistent estimator of the underlying minimal latent polytree structure.

\begin{remark}
The sample complexity of our approach depends on the accuracy of second and third cumulant estimation. For sub-Gaussian distributions, consistent estimation requires $n \gg \max\{p, \log p\}$ samples, where $p$ is the number of observed variables.
\end{remark}


\subsection{Sample Complexity}
Consistent estimation of $C^{(2)}$ and $C^{(3)}$ requires $n\gg \max\{p,\log p\}$ samples under sub--Gaussian tails.  Concentration bounds (\citealp{vershynin:2018}; \citealp[Cor.~4.1]{tramontano:monod:drton:2022}) yield rates matching those in the fully observed case.  Detailed finite--sample analysis is deferred to future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

We evaluate our cumulant-based discrepancy approach through comprehensive experiments on synthetic polytree data. Our experimental framework tests the theoretical predictions in controlled settings using population-level discrepancy matrices, providing insights into the fundamental performance of our method before considering finite-sample effects.

\subsection{Random Polytree Generation via Prüfer Sequences}

\subsubsection{Theoretical Foundation of Prüfer Sequences}

Our experimental design relies on \emph{Prüfer sequences} \citep{prufer}, a fundamental combinatorial tool that establishes a bijection between labeled trees and integer sequences.

\begin{definition}[Prüfer sequence]
A Prüfer sequence for a labeled tree with $n$ vertices is a sequence of length $n-2$ that uniquely encodes the tree structure. The encoding algorithm iteratively removes the leaf with the smallest label and records the label of its unique neighbor, continuing until only two vertices remain.
\end{definition}

Prüfer sequences provide several critical advantages for causal inference experiments:

\begin{enumerate}[label=(\roman*)]
\item \textbf{Uniform sampling}: There exists a bijection between labeled trees on $n$ vertices and sequences of length $n-2$ over the alphabet $\{1,\ldots,n\}$. This enables uniform random sampling from the space of all $n^{n-2}$ possible tree structures.

\item \textbf{Guaranteed validity}: The decoding process always produces a connected, acyclic graph, ensuring that every generated structure is a valid polytree.

\item \textbf{Computational efficiency}: Both encoding and decoding algorithms operate in $O(n)$ time using appropriate data structures, making large-scale experiments feasible.

\item \textbf{Parameter control}: By constraining the choice of root during orientation, we can ensure the presence of latent variables with specified out-degrees.
\end{enumerate}

\subsubsection{Implementation Details}

Our random polytree generation follows Algorithm~\ref{alg:pruefer_pipeline}:

\begin{algorithm}[H]
\caption{Prüfer-Based Random Polytree Generation}
\label{alg:pruefer_pipeline}
\begin{algorithmic}[1]
    \Require Number of nodes $n$, random seed
    \Ensure Minimal latent polytree with population discrepancy matrix
    \State Generate random Prüfer sequence $S = (s_1, \ldots, s_{n-2})$ with $s_i \in \{1,\ldots,n\}$
    \State Decode $S$ to undirected tree $T = (V, E_{\text{undir}})$ using heap-based algorithm
    \State Choose root $r$ with undirected degree $\geq 2$ to ensure latent nodes exist
    \State Orient edges via breadth-first search from $r$: $E_{\text{dir}} = \{(u,v) : u \text{ is parent of } v\}$
    \State Assign edge weights $\lambda_{uv} \sim \text{Uniform}[-1,1]$ with $|\lambda_{uv}| \geq 0.1$
    \State Set noise parameters: $\sigma_i^2 = 1$, $\kappa_i = 1$ for all $i \in V$
    \State Identify latent nodes: $L = \{v \in V : |\{u : (v,u) \in E_{\text{dir}}\}| \geq 2\}$
    \State Set observed nodes: $O = V \setminus L$
    \State Compute population discrepancy matrix $\Gamma_O$ via Definition~\ref{def:discrepancy}
\end{algorithmic}
\end{algorithm}

The key innovation in our approach is the systematic identification of latent variables based on out-degree. Any node with out-degree $\geq 2$ is designated as latent, ensuring that the resulting structure satisfies the minimality condition of Definition~\ref{def:minimal_polytree}.

\subsection{Population-Level Experimental Design}

\subsubsection{Parameter Configuration}

Our current experiments operate in the population regime, examining the theoretical performance of our discrepancy-based algorithms without finite-sample noise. The experimental parameters are:

\begin{itemize}
\item \textbf{Graph size}: $n = 8$ total nodes, yielding approximately 3-4 observed variables after latent node removal
\item \textbf{Edge weights}: $\lambda_{ij} \sim \text{Uniform}[-1,1]$ with $|\lambda_{ij}| \geq 0.1$ to avoid near-singular configurations
\item \textbf{Noise variances}: $\sigma_i^2 = 1$ for all nodes (unit variance assumption)
\item \textbf{Third-order cumulants}: $\kappa_i = 1$ for all nodes (unit skewness assumption)
\item \textbf{Latent identification rule}: Out-degree $\geq 2$ (branching-based rule)
\end{itemize}

This configuration represents a challenging but tractable test case where the signal-to-noise ratio in cumulant space is moderate, requiring the algorithm to distinguish subtle higher-order dependencies.

\subsubsection{Ground Truth Construction}

For each generated polytree, we construct the ground truth through the following process:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Structural equations}: Given the oriented polytree $G = (V, E)$ with weights $\Lambda$, construct the linear system $X = (I - \Lambda)^{-\top}\varepsilon$ where $\varepsilon$ has the specified variance and cumulant structure.

\item \textbf{Population moments}: Compute the population covariance matrix $\Sigma$ and third-order cumulant tensor $\mathcal{C}^{(3)}$ using the multi-trek rules from Proposition~\ref{prop:multi:trek}.

\item \textbf{Observed restriction}: Extract the submatrices corresponding to observed variables $O$, yielding $\Sigma_O$ and $\mathcal{C}^{(3)}_O$.

\item \textbf{Discrepancy computation}: Apply Definition~\ref{def:discrepancy} to construct the population discrepancy matrix $\Gamma_O$.
\end{enumerate}

This approach ensures that our evaluation focuses purely on the algorithmic aspects of structure recovery, isolating the performance from estimation uncertainty.

\subsection{Structure Recovery Pipeline}

Our implementation follows the three-phase approach outlined in Section~\ref{sec:algorithm}:

\begin{algorithm}[H]
\caption{Population-Level Structure Recovery}
\label{alg:population_recovery}
\begin{algorithmic}[1]
    \Require Population discrepancy matrix $\Gamma_O$
    \Ensure Recovered minimal latent polytree
    \State \textbf{Separation Phase}: Apply Algorithm~\ref{alg:separation} to partition observed nodes into sibling groups
    \State \textbf{Tree Recovery}: For each group, apply Algorithm~\ref{alg:tree} to recover directed tree structure
    \State \textbf{Merger Phase}: Apply Algorithm~\ref{alg:polytree} to merge trees via common subtrees
    \State \textbf{Latent Insertion}: Add latent nodes with appropriate connectivity to satisfy minimality
\end{algorithmic}
\end{algorithm}

The key challenge in our experimental setup is the accurate recovery of latent variable connectivity. Since latent nodes are not directly observed, the algorithm must infer their presence and relationships purely from the discrepancy patterns among observed variables.

\subsection{Evaluation Metrics and Methodology}

\subsubsection{Performance Measures}

We assess structural recovery using precision and recall metrics adapted to the latent variable setting:

\begin{definition}[Latent-aware precision and recall]
Let $\mathcal{E}_{\text{true}}$ be the set of true latent-to-observed edges and $\mathcal{E}_{\text{pred}}$ be the predicted edges. Define:
\begin{align}
\text{Precision} &= \frac{|\mathcal{E}_{\text{pred}} \cap \mathcal{E}_{\text{true}}|}{|\mathcal{E}_{\text{pred}}|}, \\
\text{Recall} &= \frac{|\mathcal{E}_{\text{pred}} \cap \mathcal{E}_{\text{true}}|}{|\mathcal{E}_{\text{true}}|}.
\end{align}
\end{definition}

Since latent variables can be recovered with different names, we employ a bipartite matching algorithm that maximizes Jaccard similarity between the children sets of true and predicted latent nodes:

\begin{algorithm}[H]
\caption{Latent Node Matching}
\label{alg:latent_matching}
\begin{algorithmic}[1]
    \Require True latent children $\{C_t^{\text{true}}\}$, predicted latent children $\{C_r^{\text{pred}}\}$
    \Ensure Optimal matching between latent nodes
    \State Compute Jaccard similarities: $J(C_t, C_r) = |C_t \cap C_r| / |C_t \cup C_r|$
    \State Solve maximum weight bipartite matching with threshold $\tau = 0.5$
    \State Return matched pairs $(t, r)$ with $J(C_t, C_r) \geq \tau$
\end{algorithmic}
\end{algorithm}

\subsubsection{Experimental Protocol}

Our evaluation consists of the following steps:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Batch generation}: Generate $K = 100$ independent random polytrees using different seeds
\item \textbf{Structure recovery}: Apply our algorithm to each population discrepancy matrix
\item \textbf{Latent matching}: Match predicted latent nodes to ground truth using Algorithm~\ref{alg:latent_matching}
\item \textbf{Metric computation}: Calculate precision, recall, and F1-score for latent-to-observed edges
\item \textbf{Statistical analysis}: Report mean, standard deviation, and distribution statistics
\end{enumerate}

\subsection{Preliminary Results}

Our experiments on polytrees with varying sizes reveal the scalability limitations of the cumulant-based approach under the current parameter configuration.

\subsubsection{Large-Scale Performance Analysis}

We conducted experiments on $n = 100$ node polytrees with unit variance and cumulant parameters, yielding approximately 74 observed variables and 26 latent variables. The results averaged over 50 trials reveal significant challenges:

\begin{itemize}
\item \textbf{Precision}: $0.29 \pm 0.30$ for latent-to-observed edge recovery
\item \textbf{Recall}: $0.999 \pm 0.007$ for latent-to-observed edge recovery
\item \textbf{F1-score}: $0.40 \pm 0.27$ indicating poor precision-recall balance
\item \textbf{Over-prediction}: $120 \pm 76$ recovered latent nodes vs $26$ true latent nodes
\end{itemize}

These results indicate that while the algorithm successfully identifies most true edges (high recall), it suffers from severe over-prediction of latent variables (low precision), leading to poor overall performance at this scale.

\subsubsection{Algorithm Robustness}

The distribution of performance metrics reveals several robustness characteristics:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Consistent separation}: The separation phase (Algorithm~\ref{alg:separation}) successfully partitions observed variables in $98\%$ of cases.

\item \textbf{Tree recovery stability}: The tree recovery phase achieves high accuracy for observed-to-observed edges, with precision $>0.9$ in most cases.

\item \textbf{Merger complexity}: The primary source of errors occurs during the tree merger phase when reconstructing latent variable connectivity.
\end{enumerate}

\subsubsection{Failure Mode Analysis}

Our analysis identifies three primary failure modes:

\begin{itemize}
\item \textbf{Ambiguous latent connectivity}: When multiple latent nodes have overlapping children sets, the greedy matching algorithm may produce suboptimal assignments.

\item \textbf{Near-singular discrepancy}: Configurations where edge weights nearly cancel in the discrepancy computation lead to numerical instabilities.

\item \textbf{Complex merger topologies}: Cases requiring multiple merger operations with intricate latent hierarchies challenge the current algorithm.
\end{itemize}

\subsection{Implementation and Reproducibility}

Our experimental framework is implemented in Python with the following key components:

\begin{itemize}
\item \textbf{Prüfer sequence generation}: \texttt{random\_polytrees\_pruefer.py} implements uniform random tree sampling
\item \textbf{Discrepancy computation}: \texttt{polytree\_discrepancy.py} computes population discrepancy matrices
\item \textbf{Structure recovery}: \texttt{latent\_polytree\_truepoly.py} implements the three-phase algorithm
\item \textbf{Evaluation pipeline}: \texttt{eval\_runner\_pruefer.py} orchestrates batch experiments and metric computation
\end{itemize}

All experiments use fixed random seeds to ensure reproducibility, and the codebase is available in the \texttt{causalLatentPolytree} repository.

\subsection{Future Experimental Directions}

Our current population-level experiments establish a baseline for algorithmic performance. Future work will extend this framework in several directions:

\begin{enumerate}[label=(\arabic*)]
\item \textbf{Finite-sample effects}: Incorporate sample-based cumulant estimation with varying sample sizes $n \in \{100, 1000, 10000\}$.

\item \textbf{Noise distribution robustness}: Test performance across different non-Gaussian distributions (Gamma, Laplace, Lognormal) as suggested in the related thesis work.

\item \textbf{Scalability analysis}: Evaluate performance on larger polytrees with $n \in \{10, 15, 20\}$ nodes.

\item \textbf{Parameter sensitivity}: Study the impact of edge weight distributions and cumulant magnitudes on recovery accuracy.
\end{enumerate}

These extensions will provide a comprehensive evaluation of our method's practical applicability and identify the regimes where cumulant-based latent polytree recovery is most effective.
\section{Conclusion and Outlook}
\label{sec:conclusion}

% TODO: Summarise findings, limitations, and prospective extensions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}

\end{document}
